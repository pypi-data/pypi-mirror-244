Metadata-Version: 2.1
Name: privacera_shield
Version: 1.0.4
Summary: Privacera AI Governance (PAIG) Shield Plugin Library
Project-URL: Homepage, https://github.com/pypa/privacera_shield
Project-URL: Bug Tracker, https://github.com/pypa/privacera_shield/issues
Author-email: PAIG Shield <paig@privacera.com>
License-File: LICENSE
Classifier: License :: Other/Proprietary License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Requires-Python: >=3.7
Requires-Dist: cryptography>=41.0.4
Requires-Dist: urllib3>=2.0.6
Description-Content-Type: text/markdown

# Privacera AI Goverance Shield Library

The `privacera_shield` library allows you to seamlessly integrate Privacera AI Governance into your Langchain projects. 

This README provides a step-by-step guide on how to set up and use the library.

## Installation

You can install the `privacera_shield` library using `pip`:

```shell
pip3 install privacera_shield
```

## Initialization
Register an account with Privacera AI Governance at https://privacera.ai. Register your AI application and 
download the Privacera Shield Configuration file.  

```python
# Import privacera_shield
import privacera_shield.client

# Setup Privacera Shield
privacera_shield.client.setup(frameworks=["langchain"])
```

## Setting User Context
Once you have completed the setup of privacera_shield, you can set the user in the context 
for Privacera Shield to use.

### Using context manager

```python
import privacera_shield.client

privacera_shield.client.setup(frameworks=["langchain"])

# Set the current user_name in the context
try:
    with privacera_shield.client.create_shield_context(username="user"):
        response = llm_chain.run(prompt_text)
except privacera_shield.exception.AccessControlException as e:
    # If access is denied, then this exception will be thrown. You can handle it accordingly.
    print(f"AccessControlException: {e}")
```

# Full Example with OpenAI

```python
import os

import privacera_shield.client
import privacera_shield.exception

from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

api_key=os.getenv("OPENAI_API_KEY") # 

# Initialize Privacera Shield
privacera_shield.client.setup(frameworks=["langchain"])

llm = OpenAI(openai_api_key=api_key)
template = """Question: {question}

Answer: Let's think step by step."""
prompt = PromptTemplate(template=template, input_variables=["question"])

# Let's assume the user is "testuser"
user = "testuser"
prompt_text = "Who is first President of USA and where did they live?"
llm_chain = LLMChain(prompt=prompt, llm=llm)
try:
   with privacera_shield.client.create_shield_context(username=user):
      response = llm_chain.run(prompt_text)
      print(f"LLM Response: {response}")
except privacera_shield.exception.AccessControlException as e:
   # If access is denied, then this exception will be thrown. You can handle it accordingly.
   print(f"AccessControlException: {e}")

```
# This file was auto-generated by Fern from our API Definition.

import typing
import urllib.parse
from json.decoder import JSONDecodeError

from ...core.api_error import ApiError
from ...core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ...core.jsonable_encoder import jsonable_encoder
from ..models.types.command_model import CommandModel
from ..models.types.embed_model import EmbedModel
from ..models.types.embed_request_truncate import EmbedRequestTruncate
from ..models.types.embed_response import EmbedResponse
from ..models.types.generate_request_return_likelihoods import GenerateRequestReturnLikelihoods
from ..models.types.generate_request_truncate import GenerateRequestTruncate
from ..models.types.generation import Generation

try:
    import pydantic.v1 as pydantic  # type: ignore
except ImportError:
    import pydantic  # type: ignore

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class BedrockClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._client_wrapper = client_wrapper

    def embed(
        self,
        model_id: EmbedModel,
        *,
        texts: typing.List[str],
        model: typing.Optional[str] = OMIT,
        input_type: typing.Optional[str] = OMIT,
        truncate: typing.Optional[EmbedRequestTruncate] = OMIT,
    ) -> EmbedResponse:
        """
        Parameters:
            - model_id: EmbedModel.

            - texts: typing.List[str].

            - model: typing.Optional[str].

            - input_type: typing.Optional[str].

            - truncate: typing.Optional[EmbedRequestTruncate].
        """
        _request: typing.Dict[str, typing.Any] = {"texts": texts}
        if model is not OMIT:
            _request["model"] = model
        if input_type is not OMIT:
            _request["input_type"] = input_type
        if truncate is not OMIT:
            _request["truncate"] = truncate
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_environment().bedrock}/", f"model/{model_id}/invoke"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(EmbedResponse, _response.json())  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    def generate(
        self,
        model_id: CommandModel,
        *,
        prompt: str,
        model: typing.Optional[str] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        truncate: typing.Optional[GenerateRequestTruncate] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        preset: typing.Optional[str] = OMIT,
        end_sequences: typing.Optional[typing.List[str]] = OMIT,
        stop_sequences: typing.Optional[typing.List[str]] = OMIT,
        k: typing.Optional[int] = OMIT,
        p: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        return_likelihoods: typing.Optional[GenerateRequestReturnLikelihoods] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, float]] = OMIT,
    ) -> Generation:
        """
        Parameters:
            - model_id: CommandModel.

            - prompt: str.

            - model: typing.Optional[str].

            - num_generations: typing.Optional[int].

            - stream: typing.Optional[bool].

            - max_tokens: typing.Optional[int].

            - truncate: typing.Optional[GenerateRequestTruncate].

            - temperature: typing.Optional[float].

            - preset: typing.Optional[str].

            - end_sequences: typing.Optional[typing.List[str]].

            - stop_sequences: typing.Optional[typing.List[str]].

            - k: typing.Optional[int].

            - p: typing.Optional[float].

            - frequency_penalty: typing.Optional[float].

            - presence_penalty: typing.Optional[float].

            - return_likelihoods: typing.Optional[GenerateRequestReturnLikelihoods].

            - logit_bias: typing.Optional[typing.Dict[str, float]].
        """
        _request: typing.Dict[str, typing.Any] = {"prompt": prompt}
        if model is not OMIT:
            _request["model"] = model
        if num_generations is not OMIT:
            _request["num_generations"] = num_generations
        if stream is not OMIT:
            _request["stream"] = stream
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if truncate is not OMIT:
            _request["truncate"] = truncate
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if preset is not OMIT:
            _request["preset"] = preset
        if end_sequences is not OMIT:
            _request["end_sequences"] = end_sequences
        if stop_sequences is not OMIT:
            _request["stop_sequences"] = stop_sequences
        if k is not OMIT:
            _request["k"] = k
        if p is not OMIT:
            _request["p"] = p
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if return_likelihoods is not OMIT:
            _request["return_likelihoods"] = return_likelihoods
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        _response = self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_environment().bedrock}/", f"model/{model_id}/invoke"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Generation, _response.json())  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)


class AsyncBedrockClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._client_wrapper = client_wrapper

    async def embed(
        self,
        model_id: EmbedModel,
        *,
        texts: typing.List[str],
        model: typing.Optional[str] = OMIT,
        input_type: typing.Optional[str] = OMIT,
        truncate: typing.Optional[EmbedRequestTruncate] = OMIT,
    ) -> EmbedResponse:
        """
        Parameters:
            - model_id: EmbedModel.

            - texts: typing.List[str].

            - model: typing.Optional[str].

            - input_type: typing.Optional[str].

            - truncate: typing.Optional[EmbedRequestTruncate].
        """
        _request: typing.Dict[str, typing.Any] = {"texts": texts}
        if model is not OMIT:
            _request["model"] = model
        if input_type is not OMIT:
            _request["input_type"] = input_type
        if truncate is not OMIT:
            _request["truncate"] = truncate
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_environment().bedrock}/", f"model/{model_id}/invoke"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(EmbedResponse, _response.json())  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

    async def generate(
        self,
        model_id: CommandModel,
        *,
        prompt: str,
        model: typing.Optional[str] = OMIT,
        num_generations: typing.Optional[int] = OMIT,
        stream: typing.Optional[bool] = OMIT,
        max_tokens: typing.Optional[int] = OMIT,
        truncate: typing.Optional[GenerateRequestTruncate] = OMIT,
        temperature: typing.Optional[float] = OMIT,
        preset: typing.Optional[str] = OMIT,
        end_sequences: typing.Optional[typing.List[str]] = OMIT,
        stop_sequences: typing.Optional[typing.List[str]] = OMIT,
        k: typing.Optional[int] = OMIT,
        p: typing.Optional[float] = OMIT,
        frequency_penalty: typing.Optional[float] = OMIT,
        presence_penalty: typing.Optional[float] = OMIT,
        return_likelihoods: typing.Optional[GenerateRequestReturnLikelihoods] = OMIT,
        logit_bias: typing.Optional[typing.Dict[str, float]] = OMIT,
    ) -> Generation:
        """
        Parameters:
            - model_id: CommandModel.

            - prompt: str.

            - model: typing.Optional[str].

            - num_generations: typing.Optional[int].

            - stream: typing.Optional[bool].

            - max_tokens: typing.Optional[int].

            - truncate: typing.Optional[GenerateRequestTruncate].

            - temperature: typing.Optional[float].

            - preset: typing.Optional[str].

            - end_sequences: typing.Optional[typing.List[str]].

            - stop_sequences: typing.Optional[typing.List[str]].

            - k: typing.Optional[int].

            - p: typing.Optional[float].

            - frequency_penalty: typing.Optional[float].

            - presence_penalty: typing.Optional[float].

            - return_likelihoods: typing.Optional[GenerateRequestReturnLikelihoods].

            - logit_bias: typing.Optional[typing.Dict[str, float]].
        """
        _request: typing.Dict[str, typing.Any] = {"prompt": prompt}
        if model is not OMIT:
            _request["model"] = model
        if num_generations is not OMIT:
            _request["num_generations"] = num_generations
        if stream is not OMIT:
            _request["stream"] = stream
        if max_tokens is not OMIT:
            _request["max_tokens"] = max_tokens
        if truncate is not OMIT:
            _request["truncate"] = truncate
        if temperature is not OMIT:
            _request["temperature"] = temperature
        if preset is not OMIT:
            _request["preset"] = preset
        if end_sequences is not OMIT:
            _request["end_sequences"] = end_sequences
        if stop_sequences is not OMIT:
            _request["stop_sequences"] = stop_sequences
        if k is not OMIT:
            _request["k"] = k
        if p is not OMIT:
            _request["p"] = p
        if frequency_penalty is not OMIT:
            _request["frequency_penalty"] = frequency_penalty
        if presence_penalty is not OMIT:
            _request["presence_penalty"] = presence_penalty
        if return_likelihoods is not OMIT:
            _request["return_likelihoods"] = return_likelihoods
        if logit_bias is not OMIT:
            _request["logit_bias"] = logit_bias
        _response = await self._client_wrapper.httpx_client.request(
            "POST",
            urllib.parse.urljoin(f"{self._client_wrapper.get_environment().bedrock}/", f"model/{model_id}/invoke"),
            json=jsonable_encoder(_request),
            headers=self._client_wrapper.get_headers(),
            timeout=60,
        )
        if 200 <= _response.status_code < 300:
            return pydantic.parse_obj_as(Generation, _response.json())  # type: ignore
        try:
            _response_json = _response.json()
        except JSONDecodeError:
            raise ApiError(status_code=_response.status_code, body=_response.text)
        raise ApiError(status_code=_response.status_code, body=_response_json)

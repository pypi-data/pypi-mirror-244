import{j as e,I as F,r as c,R as p,E as v,J as I,ac as K,H as w,a2 as L,$ as R,a3 as V,a4 as E,a1 as A,a0 as H,ad as $}from"./__uno-f8f92817.js";const z=({type:t})=>e.jsx("div",{style:{paddingRight:"20px"},children:e.jsx(F,{type:t,size:18})}),D=({role:t,content:n})=>e.jsx("div",{className:`flex flex-col ${t==="bot"?"items-start":"items-end"}`,children:e.jsxs("div",{className:`flex items-center ${t==="bot"?"bg-neutral-200 text-neutral-900":"bg-blue-200"} rounded-xl px-3 py-2 max-w-[80%]`,style:{overflowWrap:"anywhere"},children:[e.jsx(z,{type:t==="bot"?"blog":"user"}),n]})}),Q=({history:t})=>e.jsx("div",{className:"mx-auto space-y-5 md:space-y-10 px-3 pt-5 md:pt-12 w-[100%]",children:e.jsx("div",{className:"flex flex-col space-y-5",children:t.map(n=>e.jsx(D,{...n},n.content))})}),B=({onSubmit:t})=>{const n=c.useRef(null),r=s=>{s.key==="Enter"&&(s.preventDefault(),t(n.current?.value??""),n.current.value="")};return e.jsx("div",{className:"relative",children:e.jsx("textarea",{ref:n,className:"min-h-[44px] rounded-lg pl-4 pr-12 py-2 w-full focus:outline-none focus:ring-1 focus:ring-neutral-300 border-2 border-neutral-200",style:{resize:"none"},placeholder:"Type a message...",rows:1,onKeyUp:r})})},g=({label:t,description:n,defaultValue:r=0,step:s=.1,max:m=1,min:o=0,onChange:x})=>{const[h,j]=c.useState(r),y=i=>{const d=parseFloat(i.target.value);j(d),x(d)},_=c.useCallback(i=>{const f=s.toString().split(".")[1],b=f?f.length:0;return Number.isInteger(i)?i:i.toFixed(b)},[s]);return e.jsxs("div",{className:"flex flex-col w-100% mb-8",children:[e.jsx("label",{className:"mb-2 text-left text-neutral-700 dark:text-neutral-400",children:t}),e.jsx("span",{className:"text-[12px] text-black/50 dark:text-white/50 text-sm",children:n}),e.jsx("span",{className:"mt-2 mb-1 text-center text-neutral-900 dark:text-neutral-100",children:_(h)}),e.jsx("input",{className:"cursor-pointer",type:"range",min:o,max:m,step:s,value:h,onChange:y})]})},U=({onChangeTemperature:t})=>{const n={label:"Temperature",description:"Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic.",defaultValue:.5,onChange:t};return e.jsx(g,{...n})},W=({onChange:t})=>e.jsx(g,{label:"Top K",onChange:t,description:"The top-k parameter limits the model’s predictions to the top k most probable tokens at each step of generation",defaultValue:1,max:100,min:0,step:1}),J=({onChange:t})=>e.jsx(g,{label:"Top P",onChange:t,description:"The top-p parameter limits the model’s predictions such that the cumulative probability of the tokens generated is always less than p",defaultValue:.8,max:1,min:0,step:.01}),q=({onChange:t})=>e.jsx(g,{label:"Max new tokens",onChange:t,description:"The maximum number of tokens to generate. The higher this value, the longer the text that will be generated.",defaultValue:256,max:1024,min:0,step:1});function l(t,n){return!!n.find(r=>r.name===t)}function G({api:t}){const[n,r]=p.useState([]),[s,m]=p.useState(.5),[o,x]=p.useState(1),[h,j]=p.useState(.8),[y,_]=p.useState(256),i=async a=>{const u={user_input:a,history:n};return l("temperature",t.components_hint)&&(u.temperature=s),l("top_k",t.components_hint)&&(u.top_k=o),l("top_p",t.components_hint)&&(u.top_p=h),l("max_new_tokens",t.components_hint)&&(u.max_new_tokens=y),v.post(`/api/${t.uri}`,u)},d=async a=>{const u=await i(a);r(u.data)},f=async a=>{m(a)},b=c.useMemo(()=>l("temperature",t.components_hint)?e.jsx(U,{onChangeTemperature:f}):e.jsx(e.Fragment,{}),[t.components_hint]),k=async a=>{x(a)},C=c.useMemo(()=>l("top_k",t.components_hint)?e.jsx(W,{onChange:k}):e.jsx(e.Fragment,{}),[t.components_hint]),S=async a=>{j(a)},N=c.useMemo(()=>l("top_p",t.components_hint)?e.jsx(J,{onChange:S}):e.jsx(e.Fragment,{}),[t.components_hint]),M=async a=>{_(a)},P=c.useMemo(()=>l("max_new_tokens",t.components_hint)?e.jsx(q,{onChange:M}):e.jsx(e.Fragment,{}),[t.components_hint]);return e.jsxs("div",{className:"max-w-[80%] flex flex-col mx-auto items-center justify-center md:px-6 md:py-8 xl:max-w-4xl",children:[n.length===0&&e.jsxs(e.Fragment,{children:[b,C,N,P]}),e.jsx(Q,{history:n}),e.jsx("div",{className:"pointer-events-none absolute inset-x-0 bottom-0 z-0 mx-auto flex w-full max-w-3xl flex-col  [&>*]:pointer-events-auto",children:e.jsx("div",{className:"mt-4 sm:mt-8 bottom-[56px] left-0 w-full",children:e.jsx(B,{onSubmit:d})})})]})}var T=(t=>(t.LLM_CHAT="llm_chat",t))(T||{});const O=async()=>{const{data:t}=await v.get("/api/spec");return t};function X(){const t=I("spec",O),[n,r]=p.useState(),[s,m]=p.useState();return c.useEffect(()=>{t.data&&(r(t.data),m(t.data.apis[0]))},[t.data]),e.jsxs("div",{children:[(n?.apis?.length??0)>1&&e.jsx(K,{options:w.map(n?.apis,o=>({id:o.inference_type,label:o.inference_type})),value:[{id:s?.inference_type,label:s?.inference_type}],onChange:({option:o})=>{if(!o||!n?.apis)return;const x=w.find(n.apis,{inference_type:o.id});x&&m(x)}}),s?.inference_type===T.LLM_CHAT&&e.jsx(G,{api:s})]})}function Y(){return e.jsx(L,{value:new R,children:e.jsx(V,{theme:E,children:e.jsx(A,{client:new H,children:e.jsx(X,{})})})})}const Z=$.createRoot(document.getElementById("root"));Z.render(e.jsx(Y,{}));

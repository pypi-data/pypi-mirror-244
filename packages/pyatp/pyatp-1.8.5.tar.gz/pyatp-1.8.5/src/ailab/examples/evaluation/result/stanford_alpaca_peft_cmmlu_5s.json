{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3136094674556213,
      "acc_stderr": 0.03579526516456226,
      "acc_norm": 0.3136094674556213,
      "acc_norm_stderr": 0.03579526516456226
    },
    "Cmmlu-anatomy": {
      "acc": 0.24324324324324326,
      "acc_stderr": 0.03538668490313391,
      "acc_norm": 0.24324324324324326,
      "acc_norm_stderr": 0.03538668490313391
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.23170731707317074,
      "acc_stderr": 0.03304756158810787,
      "acc_norm": 0.23170731707317074,
      "acc_norm_stderr": 0.03304756158810787
    },
    "Cmmlu-arts": {
      "acc": 0.24375,
      "acc_stderr": 0.034049163262375844,
      "acc_norm": 0.24375,
      "acc_norm_stderr": 0.034049163262375844
    },
    "Cmmlu-astronomy": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.03192271569548299,
      "acc_norm": 0.21212121212121213,
      "acc_norm_stderr": 0.03192271569548299
    },
    "Cmmlu-business_ethics": {
      "acc": 0.2535885167464115,
      "acc_stderr": 0.030166316298848004,
      "acc_norm": 0.2535885167464115,
      "acc_norm_stderr": 0.030166316298848004
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.275,
      "acc_stderr": 0.03541088558070895,
      "acc_norm": 0.275,
      "acc_norm_stderr": 0.03541088558070895
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.3511450381679389,
      "acc_stderr": 0.04186445163013751,
      "acc_norm": 0.3511450381679389,
      "acc_norm_stderr": 0.04186445163013751
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.22794117647058823,
      "acc_stderr": 0.03610519574180446,
      "acc_norm": 0.22794117647058823,
      "acc_norm_stderr": 0.03610519574180446
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.2897196261682243,
      "acc_stderr": 0.0440606533474851,
      "acc_norm": 0.2897196261682243,
      "acc_norm_stderr": 0.0440606533474851
    },
    "Cmmlu-chinese_history": {
      "acc": 0.2755417956656347,
      "acc_stderr": 0.024898459287000824,
      "acc_norm": 0.2755417956656347,
      "acc_norm_stderr": 0.024898459287000824
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.25980392156862747,
      "acc_stderr": 0.03077855467869326,
      "acc_norm": 0.25980392156862747,
      "acc_norm_stderr": 0.03077855467869326
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.29608938547486036,
      "acc_stderr": 0.0342184375430487,
      "acc_norm": 0.29608938547486036,
      "acc_norm_stderr": 0.0342184375430487
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.2489451476793249,
      "acc_stderr": 0.028146970599422644,
      "acc_norm": 0.2489451476793249,
      "acc_norm_stderr": 0.028146970599422644
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.14150943396226415,
      "acc_stderr": 0.03401463467418859,
      "acc_norm": 0.14150943396226415,
      "acc_norm_stderr": 0.03401463467418859
    },
    "Cmmlu-college_education": {
      "acc": 0.24299065420560748,
      "acc_stderr": 0.041657429989652724,
      "acc_norm": 0.24299065420560748,
      "acc_norm_stderr": 0.041657429989652724
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.2830188679245283,
      "acc_stderr": 0.04396093377439375,
      "acc_norm": 0.2830188679245283,
      "acc_norm_stderr": 0.04396093377439375
    },
    "Cmmlu-college_law": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.04133119440243839,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.04133119440243839
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.24761904761904763,
      "acc_stderr": 0.042324735320550415,
      "acc_norm": 0.24761904761904763,
      "acc_norm_stderr": 0.042324735320550415
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.044801270921106716,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.044801270921106716
    },
    "Cmmlu-college_medicine": {
      "acc": 0.27106227106227104,
      "acc_stderr": 0.026952266920703325,
      "acc_norm": 0.27106227106227104,
      "acc_norm_stderr": 0.026952266920703325
    },
    "Cmmlu-computer_science": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.03198001660115071,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.03198001660115071
    },
    "Cmmlu-computer_security": {
      "acc": 0.25146198830409355,
      "acc_stderr": 0.033275044238468436,
      "acc_norm": 0.25146198830409355,
      "acc_norm_stderr": 0.033275044238468436
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.037387423042158106,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.037387423042158106
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2589928057553957,
      "acc_stderr": 0.037291986581642324,
      "acc_norm": 0.2589928057553957,
      "acc_norm_stderr": 0.037291986581642324
    },
    "Cmmlu-economics": {
      "acc": 0.2830188679245283,
      "acc_stderr": 0.03583711288976435,
      "acc_norm": 0.2830188679245283,
      "acc_norm_stderr": 0.03583711288976435
    },
    "Cmmlu-education": {
      "acc": 0.2392638036809816,
      "acc_stderr": 0.03351953879521269,
      "acc_norm": 0.2392638036809816,
      "acc_norm_stderr": 0.03351953879521269
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.27325581395348836,
      "acc_stderr": 0.03407826167337436,
      "acc_norm": 0.27325581395348836,
      "acc_norm_stderr": 0.03407826167337436
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.2261904761904762,
      "acc_stderr": 0.026406894598861592,
      "acc_norm": 0.2261904761904762,
      "acc_norm_stderr": 0.026406894598861592
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.23737373737373738,
      "acc_stderr": 0.03031371053819889,
      "acc_norm": 0.23737373737373738,
      "acc_norm_stderr": 0.03031371053819889
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.2689075630252101,
      "acc_stderr": 0.028801392193631276,
      "acc_norm": 0.2689075630252101,
      "acc_norm_stderr": 0.028801392193631276
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.02725685083881996,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.02725685083881996
    },
    "Cmmlu-ethnology": {
      "acc": 0.24444444444444444,
      "acc_stderr": 0.037125378336148665,
      "acc_norm": 0.24444444444444444,
      "acc_norm_stderr": 0.037125378336148665
    },
    "Cmmlu-food_science": {
      "acc": 0.25874125874125875,
      "acc_stderr": 0.03675137438900237,
      "acc_norm": 0.25874125874125875,
      "acc_norm_stderr": 0.03675137438900237
    },
    "Cmmlu-genetics": {
      "acc": 0.2556818181818182,
      "acc_stderr": 0.03297692925434459,
      "acc_norm": 0.2556818181818182,
      "acc_norm_stderr": 0.03297692925434459
    },
    "Cmmlu-global_facts": {
      "acc": 0.2348993288590604,
      "acc_stderr": 0.03484731504650187,
      "acc_norm": 0.2348993288590604,
      "acc_norm_stderr": 0.03484731504650187
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.2781065088757396,
      "acc_stderr": 0.034569054303762434,
      "acc_norm": 0.2781065088757396,
      "acc_norm_stderr": 0.034569054303762434
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.20454545454545456,
      "acc_stderr": 0.03524251981380331,
      "acc_norm": 0.20454545454545456,
      "acc_norm_stderr": 0.03524251981380331
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3389830508474576,
      "acc_stderr": 0.04376252368595952,
      "acc_norm": 0.3389830508474576,
      "acc_norm_stderr": 0.04376252368595952
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.033635910482728223,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.033635910482728223
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.20909090909090908,
      "acc_stderr": 0.038950910157241364,
      "acc_norm": 0.20909090909090908,
      "acc_norm_stderr": 0.038950910157241364
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.21678321678321677,
      "acc_stderr": 0.034578778571478445,
      "acc_norm": 0.21678321678321677,
      "acc_norm_stderr": 0.034578778571478445
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.03932537680392872,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.03932537680392872
    },
    "Cmmlu-international_law": {
      "acc": 0.2918918918918919,
      "acc_stderr": 0.03351597731741765,
      "acc_norm": 0.2918918918918919,
      "acc_norm_stderr": 0.03351597731741765
    },
    "Cmmlu-journalism": {
      "acc": 0.29651162790697677,
      "acc_stderr": 0.03492619473255953,
      "acc_norm": 0.29651162790697677,
      "acc_norm_stderr": 0.03492619473255953
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.2822384428223844,
      "acc_stderr": 0.022228300145424457,
      "acc_norm": 0.2822384428223844,
      "acc_norm_stderr": 0.022228300145424457
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.308411214953271,
      "acc_stderr": 0.03164457376920024,
      "acc_norm": 0.308411214953271,
      "acc_norm_stderr": 0.03164457376920024
    },
    "Cmmlu-logical": {
      "acc": 0.2764227642276423,
      "acc_stderr": 0.04049015460622489,
      "acc_norm": 0.2764227642276423,
      "acc_norm_stderr": 0.04049015460622489
    },
    "Cmmlu-machine_learning": {
      "acc": 0.319672131147541,
      "acc_stderr": 0.04239540943837383,
      "acc_norm": 0.319672131147541,
      "acc_norm_stderr": 0.04239540943837383
    },
    "Cmmlu-management": {
      "acc": 0.21428571428571427,
      "acc_stderr": 0.028382836222822345,
      "acc_norm": 0.21428571428571427,
      "acc_norm_stderr": 0.028382836222822345
    },
    "Cmmlu-marketing": {
      "acc": 0.3111111111111111,
      "acc_stderr": 0.03460236918732731,
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.03460236918732731
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.291005291005291,
      "acc_stderr": 0.03312783200356568,
      "acc_norm": 0.291005291005291,
      "acc_norm_stderr": 0.03312783200356568
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.31896551724137934,
      "acc_stderr": 0.04346177891598432,
      "acc_norm": 0.31896551724137934,
      "acc_norm_stderr": 0.04346177891598432
    },
    "Cmmlu-nutrition": {
      "acc": 0.25517241379310346,
      "acc_stderr": 0.03632984052707842,
      "acc_norm": 0.25517241379310346,
      "acc_norm_stderr": 0.03632984052707842
    },
    "Cmmlu-philosophy": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.04429811949614585,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.04429811949614585
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.26285714285714284,
      "acc_stderr": 0.03337037585221274,
      "acc_norm": 0.26285714285714284,
      "acc_norm_stderr": 0.03337037585221274
    },
    "Cmmlu-professional_law": {
      "acc": 0.2796208530805687,
      "acc_stderr": 0.030971033440870904,
      "acc_norm": 0.2796208530805687,
      "acc_norm_stderr": 0.030971033440870904
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2553191489361702,
      "acc_stderr": 0.02251703243459228,
      "acc_norm": 0.2553191489361702,
      "acc_norm_stderr": 0.02251703243459228
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.2672413793103448,
      "acc_stderr": 0.029115639308759603,
      "acc_norm": 0.2672413793103448,
      "acc_norm_stderr": 0.029115639308759603
    },
    "Cmmlu-public_relations": {
      "acc": 0.28160919540229884,
      "acc_stderr": 0.03419642820708564,
      "acc_norm": 0.28160919540229884,
      "acc_norm_stderr": 0.03419642820708564
    },
    "Cmmlu-security_study": {
      "acc": 0.2814814814814815,
      "acc_stderr": 0.03885004245800254,
      "acc_norm": 0.2814814814814815,
      "acc_norm_stderr": 0.03885004245800254
    },
    "Cmmlu-sociology": {
      "acc": 0.252212389380531,
      "acc_stderr": 0.028952167450890808,
      "acc_norm": 0.252212389380531,
      "acc_norm_stderr": 0.028952167450890808
    },
    "Cmmlu-sports_science": {
      "acc": 0.2787878787878788,
      "acc_stderr": 0.03501438706296781,
      "acc_norm": 0.2787878787878788,
      "acc_norm_stderr": 0.03501438706296781
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.22702702702702704,
      "acc_stderr": 0.030882469702495,
      "acc_norm": 0.22702702702702704,
      "acc_norm_stderr": 0.030882469702495
    },
    "Cmmlu-virology": {
      "acc": 0.24260355029585798,
      "acc_stderr": 0.03307162750323177,
      "acc_norm": 0.24260355029585798,
      "acc_norm_stderr": 0.03307162750323177
    },
    "Cmmlu-world_history": {
      "acc": 0.2670807453416149,
      "acc_stderr": 0.03497754822823695,
      "acc_norm": 0.2670807453416149,
      "acc_norm_stderr": 0.03497754822823695
    },
    "Cmmlu-world_religions": {
      "acc": 0.275,
      "acc_stderr": 0.035410885580708956,
      "acc_norm": 0.275,
      "acc_norm_stderr": 0.035410885580708956
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama-7b-hf,load_in_8bit=True,dtype='float16',tokenizer=/home/sdk_token/llama-7b-hf_tokenizer,use_accelerate=False,peft=/home/finetuned_models/my_standford_alpaca_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:00:57.324048",
    "model_name": "stanford_alpaca"
  }
}
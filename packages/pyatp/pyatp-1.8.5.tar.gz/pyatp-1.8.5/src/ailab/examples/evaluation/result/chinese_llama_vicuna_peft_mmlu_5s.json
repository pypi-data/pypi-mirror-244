{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-anatomy": {
      "acc": 0.37777777777777777,
      "acc_stderr": 0.04188307537595853,
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.04188307537595853
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3092105263157895,
      "acc_stderr": 0.037610708698674805,
      "acc_norm": 0.3092105263157895,
      "acc_norm_stderr": 0.037610708698674805
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3471698113207547,
      "acc_stderr": 0.029300101705549652,
      "acc_norm": 0.3471698113207547,
      "acc_norm_stderr": 0.029300101705549652
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.03852084696008534,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.03852084696008534
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.18,
      "acc_stderr": 0.038612291966536955,
      "acc_norm": 0.18,
      "acc_norm_stderr": 0.038612291966536955
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909282,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909282
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2658959537572254,
      "acc_stderr": 0.0336876293225943,
      "acc_norm": 0.2658959537572254,
      "acc_norm_stderr": 0.0336876293225943
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237656,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237656
    },
    "hendrycksTest-computer_security": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110175,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110175
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.37872340425531914,
      "acc_stderr": 0.03170995606040655,
      "acc_norm": 0.37872340425531914,
      "acc_norm_stderr": 0.03170995606040655
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.042270544512322004,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.042270544512322004
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2620689655172414,
      "acc_stderr": 0.03664666337225257,
      "acc_norm": 0.2620689655172414,
      "acc_norm_stderr": 0.03664666337225257
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2671957671957672,
      "acc_stderr": 0.02278967314577656,
      "acc_norm": 0.2671957671957672,
      "acc_norm_stderr": 0.02278967314577656
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.037184890068181146,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.037184890068181146
    },
    "hendrycksTest-global_facts": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3032258064516129,
      "acc_stderr": 0.026148685930671746,
      "acc_norm": 0.3032258064516129,
      "acc_norm_stderr": 0.026148685930671746
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.26108374384236455,
      "acc_stderr": 0.030903796952114468,
      "acc_norm": 0.26108374384236455,
      "acc_norm_stderr": 0.030903796952114468
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3575757575757576,
      "acc_stderr": 0.03742597043806587,
      "acc_norm": 0.3575757575757576,
      "acc_norm_stderr": 0.03742597043806587
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.2878787878787879,
      "acc_stderr": 0.03225883512300993,
      "acc_norm": 0.2878787878787879,
      "acc_norm_stderr": 0.03225883512300993
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.3626943005181347,
      "acc_stderr": 0.03469713791704372,
      "acc_norm": 0.3626943005181347,
      "acc_norm_stderr": 0.03469713791704372
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2846153846153846,
      "acc_stderr": 0.022878322799706283,
      "acc_norm": 0.2846153846153846,
      "acc_norm_stderr": 0.022878322799706283
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2518518518518518,
      "acc_stderr": 0.026466117538959916,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.026466117538959916
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3067226890756303,
      "acc_stderr": 0.02995382389188704,
      "acc_norm": 0.3067226890756303,
      "acc_norm_stderr": 0.02995382389188704
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.1986754966887417,
      "acc_stderr": 0.032578473844367746,
      "acc_norm": 0.1986754966887417,
      "acc_norm_stderr": 0.032578473844367746
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.30642201834862387,
      "acc_stderr": 0.019765517220458523,
      "acc_norm": 0.30642201834862387,
      "acc_norm_stderr": 0.019765517220458523
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.02988691054762699,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.02988691054762699
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.37254901960784315,
      "acc_stderr": 0.03393388584958404,
      "acc_norm": 0.37254901960784315,
      "acc_norm_stderr": 0.03393388584958404
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4092827004219409,
      "acc_stderr": 0.032007041833595914,
      "acc_norm": 0.4092827004219409,
      "acc_norm_stderr": 0.032007041833595914
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4260089686098655,
      "acc_stderr": 0.033188332862172806,
      "acc_norm": 0.4260089686098655,
      "acc_norm_stderr": 0.033188332862172806
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3053435114503817,
      "acc_stderr": 0.04039314978724561,
      "acc_norm": 0.3053435114503817,
      "acc_norm_stderr": 0.04039314978724561
    },
    "hendrycksTest-international_law": {
      "acc": 0.49586776859504134,
      "acc_stderr": 0.045641987674327526,
      "acc_norm": 0.49586776859504134,
      "acc_norm_stderr": 0.045641987674327526
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3611111111111111,
      "acc_stderr": 0.04643454608906274,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.04643454608906274
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3006134969325153,
      "acc_stderr": 0.03602511318806771,
      "acc_norm": 0.3006134969325153,
      "acc_norm_stderr": 0.03602511318806771
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.29464285714285715,
      "acc_stderr": 0.04327040932578728,
      "acc_norm": 0.29464285714285715,
      "acc_norm_stderr": 0.04327040932578728
    },
    "hendrycksTest-management": {
      "acc": 0.3300970873786408,
      "acc_stderr": 0.04656147110012351,
      "acc_norm": 0.3300970873786408,
      "acc_norm_stderr": 0.04656147110012351
    },
    "hendrycksTest-marketing": {
      "acc": 0.42735042735042733,
      "acc_stderr": 0.032408473935163266,
      "acc_norm": 0.42735042735042733,
      "acc_norm_stderr": 0.032408473935163266
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.47,
      "acc_stderr": 0.050161355804659205,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.050161355804659205
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.4278416347381865,
      "acc_stderr": 0.01769278792780373,
      "acc_norm": 0.4278416347381865,
      "acc_norm_stderr": 0.01769278792780373
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3352601156069364,
      "acc_stderr": 0.02541600377316555,
      "acc_norm": 0.3352601156069364,
      "acc_norm_stderr": 0.02541600377316555
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-nutrition": {
      "acc": 0.3202614379084967,
      "acc_stderr": 0.026716118380156837,
      "acc_norm": 0.3202614379084967,
      "acc_norm_stderr": 0.026716118380156837
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2733118971061093,
      "acc_stderr": 0.025311765975426122,
      "acc_norm": 0.2733118971061093,
      "acc_norm_stderr": 0.025311765975426122
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3271604938271605,
      "acc_stderr": 0.026105673861409814,
      "acc_norm": 0.3271604938271605,
      "acc_norm_stderr": 0.026105673861409814
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.026684564340461,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.026684564340461
    },
    "hendrycksTest-professional_law": {
      "acc": 0.27183833116036504,
      "acc_stderr": 0.011363135278651411,
      "acc_norm": 0.27183833116036504,
      "acc_norm_stderr": 0.011363135278651411
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.3639705882352941,
      "acc_stderr": 0.029227192460032025,
      "acc_norm": 0.3639705882352941,
      "acc_norm_stderr": 0.029227192460032025
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3349673202614379,
      "acc_stderr": 0.01909422816700033,
      "acc_norm": 0.3349673202614379,
      "acc_norm_stderr": 0.01909422816700033
    },
    "hendrycksTest-public_relations": {
      "acc": 0.39090909090909093,
      "acc_stderr": 0.04673752333670238,
      "acc_norm": 0.39090909090909093,
      "acc_norm_stderr": 0.04673752333670238
    },
    "hendrycksTest-security_studies": {
      "acc": 0.24489795918367346,
      "acc_stderr": 0.027529637440174917,
      "acc_norm": 0.24489795918367346,
      "acc_norm_stderr": 0.027529637440174917
    },
    "hendrycksTest-sociology": {
      "acc": 0.373134328358209,
      "acc_stderr": 0.034198326081760065,
      "acc_norm": 0.373134328358209,
      "acc_norm_stderr": 0.034198326081760065
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.47,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.47,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-virology": {
      "acc": 0.29518072289156627,
      "acc_stderr": 0.0355092018568963,
      "acc_norm": 0.29518072289156627,
      "acc_norm_stderr": 0.0355092018568963
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4678362573099415,
      "acc_stderr": 0.038268824176603676,
      "acc_norm": 0.4678362573099415,
      "acc_norm_stderr": 0.038268824176603676
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/home/sdk_models/llama-7b-hf',load_in_8bit=True,dtype='float16',tokenizer='/data1/cgzhang6/tokenizer/chinese_llama_vicuna_tokenizer',use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_chinese_llama_vicuna_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "3:05:03.289278",
    "model_name": "chinese_llama_vicuna"
  }
}
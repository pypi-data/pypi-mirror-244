{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421296,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421296
    },
    "hendrycksTest-anatomy": {
      "acc": 0.31851851851851853,
      "acc_stderr": 0.040247784019771096,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.040247784019771096
    },
    "hendrycksTest-astronomy": {
      "acc": 0.23026315789473684,
      "acc_stderr": 0.03426059424403165,
      "acc_norm": 0.23026315789473684,
      "acc_norm_stderr": 0.03426059424403165
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939098,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939098
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.03010279378179119,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.03010279378179119
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3263888888888889,
      "acc_stderr": 0.03921067198982266,
      "acc_norm": 0.3263888888888889,
      "acc_norm_stderr": 0.03921067198982266
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.23,
      "acc_stderr": 0.042295258468165065,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.042295258468165065
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3236994219653179,
      "acc_stderr": 0.035676037996391685,
      "acc_norm": 0.3236994219653179,
      "acc_norm_stderr": 0.035676037996391685
    },
    "hendrycksTest-college_physics": {
      "acc": 0.18627450980392157,
      "acc_stderr": 0.03873958714149351,
      "acc_norm": 0.18627450980392157,
      "acc_norm_stderr": 0.03873958714149351
    },
    "hendrycksTest-computer_security": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.32340425531914896,
      "acc_stderr": 0.030579442773610337,
      "acc_norm": 0.32340425531914896,
      "acc_norm_stderr": 0.030579442773610337
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2894736842105263,
      "acc_stderr": 0.042663394431593935,
      "acc_norm": 0.2894736842105263,
      "acc_norm_stderr": 0.042663394431593935
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3103448275862069,
      "acc_stderr": 0.03855289616378949,
      "acc_norm": 0.3103448275862069,
      "acc_norm_stderr": 0.03855289616378949
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2671957671957672,
      "acc_stderr": 0.022789673145776564,
      "acc_norm": 0.2671957671957672,
      "acc_norm_stderr": 0.022789673145776564
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.03809523809523812,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.03809523809523812
    },
    "hendrycksTest-global_facts": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3096774193548387,
      "acc_stderr": 0.026302774983517418,
      "acc_norm": 0.3096774193548387,
      "acc_norm_stderr": 0.026302774983517418
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.26108374384236455,
      "acc_stderr": 0.030903796952114468,
      "acc_norm": 0.26108374384236455,
      "acc_norm_stderr": 0.030903796952114468
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542129,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542129
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.03546563019624336,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.03546563019624336
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3484848484848485,
      "acc_stderr": 0.033948539651564025,
      "acc_norm": 0.3484848484848485,
      "acc_norm_stderr": 0.033948539651564025
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.35751295336787564,
      "acc_stderr": 0.03458816042181005,
      "acc_norm": 0.35751295336787564,
      "acc_norm_stderr": 0.03458816042181005
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3641025641025641,
      "acc_stderr": 0.024396672985094778,
      "acc_norm": 0.3641025641025641,
      "acc_norm_stderr": 0.024396672985094778
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.24814814814814815,
      "acc_stderr": 0.0263357394040558,
      "acc_norm": 0.24814814814814815,
      "acc_norm_stderr": 0.0263357394040558
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.31092436974789917,
      "acc_stderr": 0.030066761582977924,
      "acc_norm": 0.31092436974789917,
      "acc_norm_stderr": 0.030066761582977924
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.25165562913907286,
      "acc_stderr": 0.035433042343899844,
      "acc_norm": 0.25165562913907286,
      "acc_norm_stderr": 0.035433042343899844
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3596330275229358,
      "acc_stderr": 0.020575234660123783,
      "acc_norm": 0.3596330275229358,
      "acc_norm_stderr": 0.020575234660123783
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.38425925925925924,
      "acc_stderr": 0.03317354514310742,
      "acc_norm": 0.38425925925925924,
      "acc_norm_stderr": 0.03317354514310742
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3235294117647059,
      "acc_stderr": 0.03283472056108568,
      "acc_norm": 0.3235294117647059,
      "acc_norm_stderr": 0.03283472056108568
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3037974683544304,
      "acc_stderr": 0.029936696387138605,
      "acc_norm": 0.3037974683544304,
      "acc_norm_stderr": 0.029936696387138605
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2914798206278027,
      "acc_stderr": 0.03050028317654591,
      "acc_norm": 0.2914798206278027,
      "acc_norm_stderr": 0.03050028317654591
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.2900763358778626,
      "acc_stderr": 0.03980066246467765,
      "acc_norm": 0.2900763358778626,
      "acc_norm_stderr": 0.03980066246467765
    },
    "hendrycksTest-international_law": {
      "acc": 0.38016528925619836,
      "acc_stderr": 0.04431324501968431,
      "acc_norm": 0.38016528925619836,
      "acc_norm_stderr": 0.04431324501968431
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.35185185185185186,
      "acc_stderr": 0.04616631111801714,
      "acc_norm": 0.35185185185185186,
      "acc_norm_stderr": 0.04616631111801714
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3006134969325153,
      "acc_stderr": 0.03602511318806771,
      "acc_norm": 0.3006134969325153,
      "acc_norm_stderr": 0.03602511318806771
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.24107142857142858,
      "acc_stderr": 0.04059867246952687,
      "acc_norm": 0.24107142857142858,
      "acc_norm_stderr": 0.04059867246952687
    },
    "hendrycksTest-management": {
      "acc": 0.2524271844660194,
      "acc_stderr": 0.04301250399690877,
      "acc_norm": 0.2524271844660194,
      "acc_norm_stderr": 0.04301250399690877
    },
    "hendrycksTest-marketing": {
      "acc": 0.36324786324786323,
      "acc_stderr": 0.03150712523091265,
      "acc_norm": 0.36324786324786323,
      "acc_norm_stderr": 0.03150712523091265
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.36909323116219667,
      "acc_stderr": 0.017256283109124613,
      "acc_norm": 0.36909323116219667,
      "acc_norm_stderr": 0.017256283109124613
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3236994219653179,
      "acc_stderr": 0.02519018132760841,
      "acc_norm": 0.3236994219653179,
      "acc_norm_stderr": 0.02519018132760841
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808835
    },
    "hendrycksTest-nutrition": {
      "acc": 0.32679738562091504,
      "acc_stderr": 0.02685729466328142,
      "acc_norm": 0.32679738562091504,
      "acc_norm_stderr": 0.02685729466328142
    },
    "hendrycksTest-philosophy": {
      "acc": 0.2797427652733119,
      "acc_stderr": 0.02549425935069491,
      "acc_norm": 0.2797427652733119,
      "acc_norm_stderr": 0.02549425935069491
    },
    "hendrycksTest-prehistory": {
      "acc": 0.32098765432098764,
      "acc_stderr": 0.025976566010862744,
      "acc_norm": 0.32098765432098764,
      "acc_norm_stderr": 0.025976566010862744
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2695035460992908,
      "acc_stderr": 0.026469036818590627,
      "acc_norm": 0.2695035460992908,
      "acc_norm_stderr": 0.026469036818590627
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2685788787483703,
      "acc_stderr": 0.01132005662912172,
      "acc_norm": 0.2685788787483703,
      "acc_norm_stderr": 0.01132005662912172
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.24632352941176472,
      "acc_stderr": 0.02617343857052,
      "acc_norm": 0.24632352941176472,
      "acc_norm_stderr": 0.02617343857052
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.26633986928104575,
      "acc_stderr": 0.017883188134667192,
      "acc_norm": 0.26633986928104575,
      "acc_norm_stderr": 0.017883188134667192
    },
    "hendrycksTest-public_relations": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.04653429807913508,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.04653429807913508
    },
    "hendrycksTest-security_studies": {
      "acc": 0.24489795918367346,
      "acc_stderr": 0.027529637440174923,
      "acc_norm": 0.24489795918367346,
      "acc_norm_stderr": 0.027529637440174923
    },
    "hendrycksTest-sociology": {
      "acc": 0.2537313432835821,
      "acc_stderr": 0.03076944496729601,
      "acc_norm": 0.2537313432835821,
      "acc_norm_stderr": 0.03076944496729601
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-virology": {
      "acc": 0.35542168674698793,
      "acc_stderr": 0.03726214354322415,
      "acc_norm": 0.35542168674698793,
      "acc_norm_stderr": 0.03726214354322415
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.036996580176568775,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.036996580176568775
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/open_llama_7b',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:51:36.180551",
    "model_name": "open_llama"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.19,
      "acc_stderr": 0.03942772444036623,
      "acc_norm": 0.19,
      "acc_norm_stderr": 0.03942772444036623
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4148148148148148,
      "acc_stderr": 0.042561937679014075,
      "acc_norm": 0.4148148148148148,
      "acc_norm_stderr": 0.042561937679014075
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5460526315789473,
      "acc_stderr": 0.04051646342874142,
      "acc_norm": 0.5460526315789473,
      "acc_norm_stderr": 0.04051646342874142
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.61,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.49056603773584906,
      "acc_stderr": 0.030767394707808093,
      "acc_norm": 0.49056603773584906,
      "acc_norm_stderr": 0.030767394707808093
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3958333333333333,
      "acc_stderr": 0.04089465449325582,
      "acc_norm": 0.3958333333333333,
      "acc_norm_stderr": 0.04089465449325582
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4393063583815029,
      "acc_stderr": 0.03784271932887467,
      "acc_norm": 0.4393063583815029,
      "acc_norm_stderr": 0.03784271932887467
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.04389869956808779,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.04389869956808779
    },
    "hendrycksTest-computer_security": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956911,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956911
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.40425531914893614,
      "acc_stderr": 0.03208115750788684,
      "acc_norm": 0.40425531914893614,
      "acc_norm_stderr": 0.03208115750788684
    },
    "hendrycksTest-econometrics": {
      "acc": 0.22807017543859648,
      "acc_stderr": 0.03947152782669415,
      "acc_norm": 0.22807017543859648,
      "acc_norm_stderr": 0.03947152782669415
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.04144311810878151,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878151
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3492063492063492,
      "acc_stderr": 0.024552292209342658,
      "acc_norm": 0.3492063492063492,
      "acc_norm_stderr": 0.024552292209342658
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.40476190476190477,
      "acc_stderr": 0.04390259265377561,
      "acc_norm": 0.40476190476190477,
      "acc_norm_stderr": 0.04390259265377561
    },
    "hendrycksTest-global_facts": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322695
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5580645161290323,
      "acc_stderr": 0.02825155790684973,
      "acc_norm": 0.5580645161290323,
      "acc_norm_stderr": 0.02825155790684973
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.4187192118226601,
      "acc_stderr": 0.03471192860518468,
      "acc_norm": 0.4187192118226601,
      "acc_norm_stderr": 0.03471192860518468
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.19393939393939394,
      "acc_stderr": 0.030874145136562094,
      "acc_norm": 0.19393939393939394,
      "acc_norm_stderr": 0.030874145136562094
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5757575757575758,
      "acc_stderr": 0.035212249088415866,
      "acc_norm": 0.5757575757575758,
      "acc_norm_stderr": 0.035212249088415866
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.616580310880829,
      "acc_stderr": 0.03508984236295341,
      "acc_norm": 0.616580310880829,
      "acc_norm_stderr": 0.03508984236295341
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4512820512820513,
      "acc_stderr": 0.025230381238934837,
      "acc_norm": 0.4512820512820513,
      "acc_norm_stderr": 0.025230381238934837
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2518518518518518,
      "acc_stderr": 0.026466117538959912,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.026466117538959912
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4495798319327731,
      "acc_stderr": 0.03231293497137707,
      "acc_norm": 0.4495798319327731,
      "acc_norm_stderr": 0.03231293497137707
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2913907284768212,
      "acc_stderr": 0.03710185726119995,
      "acc_norm": 0.2913907284768212,
      "acc_norm_stderr": 0.03710185726119995
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6128440366972477,
      "acc_stderr": 0.02088423199264345,
      "acc_norm": 0.6128440366972477,
      "acc_norm_stderr": 0.02088423199264345
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.03293377139415191,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.03293377139415191
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.22058823529411764,
      "acc_stderr": 0.029102254389674082,
      "acc_norm": 0.22058823529411764,
      "acc_norm_stderr": 0.029102254389674082
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6624472573839663,
      "acc_stderr": 0.03078154910202621,
      "acc_norm": 0.6624472573839663,
      "acc_norm_stderr": 0.03078154910202621
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4663677130044843,
      "acc_stderr": 0.033481800170603065,
      "acc_norm": 0.4663677130044843,
      "acc_norm_stderr": 0.033481800170603065
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4961832061068702,
      "acc_stderr": 0.043851623256015534,
      "acc_norm": 0.4961832061068702,
      "acc_norm_stderr": 0.043851623256015534
    },
    "hendrycksTest-international_law": {
      "acc": 0.6115702479338843,
      "acc_stderr": 0.04449270350068383,
      "acc_norm": 0.6115702479338843,
      "acc_norm_stderr": 0.04449270350068383
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5462962962962963,
      "acc_stderr": 0.04812917324536823,
      "acc_norm": 0.5462962962962963,
      "acc_norm_stderr": 0.04812917324536823
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.4785276073619632,
      "acc_stderr": 0.03924746876751129,
      "acc_norm": 0.4785276073619632,
      "acc_norm_stderr": 0.03924746876751129
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764377,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764377
    },
    "hendrycksTest-management": {
      "acc": 0.6213592233009708,
      "acc_stderr": 0.04802694698258974,
      "acc_norm": 0.6213592233009708,
      "acc_norm_stderr": 0.04802694698258974
    },
    "hendrycksTest-marketing": {
      "acc": 0.6965811965811965,
      "acc_stderr": 0.030118210106942656,
      "acc_norm": 0.6965811965811965,
      "acc_norm_stderr": 0.030118210106942656
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.45,
      "acc_stderr": 0.05,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6040868454661558,
      "acc_stderr": 0.017488247006979273,
      "acc_norm": 0.6040868454661558,
      "acc_norm_stderr": 0.017488247006979273
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5317919075144508,
      "acc_stderr": 0.026864624366756643,
      "acc_norm": 0.5317919075144508,
      "acc_norm_stderr": 0.026864624366756643
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2335195530726257,
      "acc_stderr": 0.014149575348976269,
      "acc_norm": 0.2335195530726257,
      "acc_norm_stderr": 0.014149575348976269
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.02845263998508801,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.02845263998508801
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5209003215434084,
      "acc_stderr": 0.02837327096106942,
      "acc_norm": 0.5209003215434084,
      "acc_norm_stderr": 0.02837327096106942
    },
    "hendrycksTest-prehistory": {
      "acc": 0.49382716049382713,
      "acc_stderr": 0.02781862396258329,
      "acc_norm": 0.49382716049382713,
      "acc_norm_stderr": 0.02781862396258329
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3475177304964539,
      "acc_stderr": 0.028406627809590954,
      "acc_norm": 0.3475177304964539,
      "acc_norm_stderr": 0.028406627809590954
    },
    "hendrycksTest-professional_law": {
      "acc": 0.36114732724902215,
      "acc_stderr": 0.012267935477519023,
      "acc_norm": 0.36114732724902215,
      "acc_norm_stderr": 0.012267935477519023
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.39338235294117646,
      "acc_stderr": 0.029674288281311183,
      "acc_norm": 0.39338235294117646,
      "acc_norm_stderr": 0.029674288281311183
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4264705882352941,
      "acc_stderr": 0.02000791273935936,
      "acc_norm": 0.4264705882352941,
      "acc_norm_stderr": 0.02000791273935936
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.04769300568972745,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.04769300568972745
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5673469387755102,
      "acc_stderr": 0.031717528240626645,
      "acc_norm": 0.5673469387755102,
      "acc_norm_stderr": 0.031717528240626645
    },
    "hendrycksTest-sociology": {
      "acc": 0.6467661691542289,
      "acc_stderr": 0.03379790611796777,
      "acc_norm": 0.6467661691542289,
      "acc_norm_stderr": 0.03379790611796777
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.71,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.71,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-virology": {
      "acc": 0.42168674698795183,
      "acc_stderr": 0.03844453181770917,
      "acc_norm": 0.42168674698795183,
      "acc_norm_stderr": 0.03844453181770917
    },
    "hendrycksTest-world_religions": {
      "acc": 0.5146198830409356,
      "acc_stderr": 0.038331852752130254,
      "acc_norm": 0.5146198830409356,
      "acc_norm_stderr": 0.038331852752130254
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained='/home/sdk_models/chatglm2_6b',add_special_tokens=True,trust_remote_code=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:48:53.799697",
    "model_name": "chatglm2_6b"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.33727810650887574,
      "acc_stderr": 0.03647582250277504,
      "acc_norm": 0.33727810650887574,
      "acc_norm_stderr": 0.03647582250277504
    },
    "Cmmlu-anatomy": {
      "acc": 0.22297297297297297,
      "acc_stderr": 0.03433092518104002,
      "acc_norm": 0.22297297297297297,
      "acc_norm_stderr": 0.03433092518104002
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364998,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364998
    },
    "Cmmlu-arts": {
      "acc": 0.50625,
      "acc_stderr": 0.03964948130713095,
      "acc_norm": 0.50625,
      "acc_norm_stderr": 0.03964948130713095
    },
    "Cmmlu-astronomy": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.037131580674819135,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.037131580674819135
    },
    "Cmmlu-business_ethics": {
      "acc": 0.44976076555023925,
      "acc_stderr": 0.03449331173477288,
      "acc_norm": 0.44976076555023925,
      "acc_norm_stderr": 0.03449331173477288
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.34375,
      "acc_stderr": 0.03766668927755763,
      "acc_norm": 0.34375,
      "acc_norm_stderr": 0.03766668927755763
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.6106870229007634,
      "acc_stderr": 0.04276486542814591,
      "acc_norm": 0.6106870229007634,
      "acc_norm_stderr": 0.04276486542814591
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.34558823529411764,
      "acc_stderr": 0.04092966025145302,
      "acc_norm": 0.34558823529411764,
      "acc_norm_stderr": 0.04092966025145302
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.45794392523364486,
      "acc_stderr": 0.04839219555189162,
      "acc_norm": 0.45794392523364486,
      "acc_norm_stderr": 0.04839219555189162
    },
    "Cmmlu-chinese_history": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.027825291191456943,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.027825291191456943
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.30392156862745096,
      "acc_stderr": 0.03228210387037892,
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.03228210387037892
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.553072625698324,
      "acc_stderr": 0.03726486555057904,
      "acc_norm": 0.553072625698324,
      "acc_norm_stderr": 0.03726486555057904
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.39662447257383965,
      "acc_stderr": 0.03184399873811225,
      "acc_norm": 0.39662447257383965,
      "acc_norm_stderr": 0.03184399873811225
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2641509433962264,
      "acc_stderr": 0.043025487739590106,
      "acc_norm": 0.2641509433962264,
      "acc_norm_stderr": 0.043025487739590106
    },
    "Cmmlu-college_education": {
      "acc": 0.5794392523364486,
      "acc_stderr": 0.04794743635189595,
      "acc_norm": 0.5794392523364486,
      "acc_norm_stderr": 0.04794743635189595
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.4056603773584906,
      "acc_stderr": 0.04791858528000114,
      "acc_norm": 0.4056603773584906,
      "acc_norm_stderr": 0.04791858528000114
    },
    "Cmmlu-college_law": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.04668408033024931,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.04668408033024931
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2,
      "acc_stderr": 0.03922322702763679,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.03922322702763679
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.32075471698113206,
      "acc_stderr": 0.04555176317903525,
      "acc_norm": 0.32075471698113206,
      "acc_norm_stderr": 0.04555176317903525
    },
    "Cmmlu-college_medicine": {
      "acc": 0.34065934065934067,
      "acc_stderr": 0.028736285365734006,
      "acc_norm": 0.34065934065934067,
      "acc_norm_stderr": 0.028736285365734006
    },
    "Cmmlu-computer_science": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.033540924375915174,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.033540924375915174
    },
    "Cmmlu-computer_security": {
      "acc": 0.4619883040935672,
      "acc_stderr": 0.03823727092882307,
      "acc_norm": 0.4619883040935672,
      "acc_norm_stderr": 0.03823727092882307
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.43537414965986393,
      "acc_stderr": 0.04103318899494186,
      "acc_norm": 0.43537414965986393,
      "acc_norm_stderr": 0.04103318899494186
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.35251798561151076,
      "acc_stderr": 0.04066913648640819,
      "acc_norm": 0.35251798561151076,
      "acc_norm_stderr": 0.04066913648640819
    },
    "Cmmlu-economics": {
      "acc": 0.389937106918239,
      "acc_stderr": 0.038802172682094724,
      "acc_norm": 0.389937106918239,
      "acc_norm_stderr": 0.038802172682094724
    },
    "Cmmlu-education": {
      "acc": 0.4171779141104294,
      "acc_stderr": 0.038741028598180814,
      "acc_norm": 0.4171779141104294,
      "acc_norm_stderr": 0.038741028598180814
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.36627906976744184,
      "acc_stderr": 0.03684317268101587,
      "acc_norm": 0.36627906976744184,
      "acc_norm_stderr": 0.03684317268101587
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.4087301587301587,
      "acc_stderr": 0.031029469097752227,
      "acc_norm": 0.4087301587301587,
      "acc_norm_stderr": 0.031029469097752227
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.4292929292929293,
      "acc_stderr": 0.035265527246011986,
      "acc_norm": 0.4292929292929293,
      "acc_norm_stderr": 0.035265527246011986
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.5504201680672269,
      "acc_stderr": 0.03231293497137707,
      "acc_norm": 0.5504201680672269,
      "acc_norm_stderr": 0.03231293497137707
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.28695652173913044,
      "acc_stderr": 0.02989154167363545,
      "acc_norm": 0.28695652173913044,
      "acc_norm_stderr": 0.02989154167363545
    },
    "Cmmlu-ethnology": {
      "acc": 0.4222222222222222,
      "acc_stderr": 0.04266763404099582,
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.04266763404099582
    },
    "Cmmlu-food_science": {
      "acc": 0.34965034965034963,
      "acc_stderr": 0.04001716028382393,
      "acc_norm": 0.34965034965034963,
      "acc_norm_stderr": 0.04001716028382393
    },
    "Cmmlu-genetics": {
      "acc": 0.26136363636363635,
      "acc_stderr": 0.03321382551635589,
      "acc_norm": 0.26136363636363635,
      "acc_norm_stderr": 0.03321382551635589
    },
    "Cmmlu-global_facts": {
      "acc": 0.4161073825503356,
      "acc_stderr": 0.040517099221280085,
      "acc_norm": 0.4161073825503356,
      "acc_norm_stderr": 0.040517099221280085
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.28402366863905326,
      "acc_stderr": 0.03479140427262331,
      "acc_norm": 0.28402366863905326,
      "acc_norm_stderr": 0.03479140427262331
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.25757575757575757,
      "acc_stderr": 0.03820699814849796,
      "acc_norm": 0.25757575757575757,
      "acc_norm_stderr": 0.03820699814849796
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3220338983050847,
      "acc_stderr": 0.04319782230261342,
      "acc_norm": 0.3220338983050847,
      "acc_norm_stderr": 0.04319782230261342
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2073170731707317,
      "acc_stderr": 0.03175217536073676,
      "acc_norm": 0.2073170731707317,
      "acc_norm_stderr": 0.03175217536073676
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3,
      "acc_stderr": 0.04389311454644286,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.04389311454644286
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.40559440559440557,
      "acc_stderr": 0.04120436731133788,
      "acc_norm": 0.40559440559440557,
      "acc_norm_stderr": 0.04120436731133788
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.42063492063492064,
      "acc_stderr": 0.04415438226743744,
      "acc_norm": 0.42063492063492064,
      "acc_norm_stderr": 0.04415438226743744
    },
    "Cmmlu-international_law": {
      "acc": 0.3567567567567568,
      "acc_stderr": 0.035315455206482514,
      "acc_norm": 0.3567567567567568,
      "acc_norm_stderr": 0.035315455206482514
    },
    "Cmmlu-journalism": {
      "acc": 0.436046511627907,
      "acc_stderr": 0.03792189197270774,
      "acc_norm": 0.436046511627907,
      "acc_norm_stderr": 0.03792189197270774
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.41605839416058393,
      "acc_stderr": 0.024342766081733646,
      "acc_norm": 0.41605839416058393,
      "acc_norm_stderr": 0.024342766081733646
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.780373831775701,
      "acc_stderr": 0.02836635864201755,
      "acc_norm": 0.780373831775701,
      "acc_norm_stderr": 0.02836635864201755
    },
    "Cmmlu-logical": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04267895997763195,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04267895997763195
    },
    "Cmmlu-machine_learning": {
      "acc": 0.36065573770491804,
      "acc_stderr": 0.04365370645566859,
      "acc_norm": 0.36065573770491804,
      "acc_norm_stderr": 0.04365370645566859
    },
    "Cmmlu-management": {
      "acc": 0.43333333333333335,
      "acc_stderr": 0.034276915911158695,
      "acc_norm": 0.43333333333333335,
      "acc_norm_stderr": 0.034276915911158695
    },
    "Cmmlu-marketing": {
      "acc": 0.5166666666666667,
      "acc_stderr": 0.03735098678123468,
      "acc_norm": 0.5166666666666667,
      "acc_norm_stderr": 0.03735098678123468
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.5132275132275133,
      "acc_stderr": 0.03645348485324587,
      "acc_norm": 0.5132275132275133,
      "acc_norm_stderr": 0.03645348485324587
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3620689655172414,
      "acc_stderr": 0.04481605202782828,
      "acc_norm": 0.3620689655172414,
      "acc_norm_stderr": 0.04481605202782828
    },
    "Cmmlu-nutrition": {
      "acc": 0.4,
      "acc_stderr": 0.040824829046386284,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.040824829046386284
    },
    "Cmmlu-philosophy": {
      "acc": 0.4380952380952381,
      "acc_stderr": 0.048651804501824956,
      "acc_norm": 0.4380952380952381,
      "acc_norm_stderr": 0.048651804501824956
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.44,
      "acc_stderr": 0.0376309972499134,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.0376309972499134
    },
    "Cmmlu-professional_law": {
      "acc": 0.3459715639810427,
      "acc_stderr": 0.03282531259761602,
      "acc_norm": 0.3459715639810427,
      "acc_norm_stderr": 0.03282531259761602
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2872340425531915,
      "acc_stderr": 0.02336553857581675,
      "acc_norm": 0.2872340425531915,
      "acc_norm_stderr": 0.02336553857581675
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.39655172413793105,
      "acc_stderr": 0.03218577394203013,
      "acc_norm": 0.39655172413793105,
      "acc_norm_stderr": 0.03218577394203013
    },
    "Cmmlu-public_relations": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.03781034307794902,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.03781034307794902
    },
    "Cmmlu-security_study": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04292596718256981,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04292596718256981
    },
    "Cmmlu-sociology": {
      "acc": 0.39823008849557523,
      "acc_stderr": 0.032635555601036786,
      "acc_norm": 0.39823008849557523,
      "acc_norm_stderr": 0.032635555601036786
    },
    "Cmmlu-sports_science": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.03756335775187897,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03756335775187897
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3027027027027027,
      "acc_stderr": 0.03386945658791665,
      "acc_norm": 0.3027027027027027,
      "acc_norm_stderr": 0.03386945658791665
    },
    "Cmmlu-virology": {
      "acc": 0.4319526627218935,
      "acc_stderr": 0.038216921573820194,
      "acc_norm": 0.4319526627218935,
      "acc_norm_stderr": 0.038216921573820194
    },
    "Cmmlu-world_history": {
      "acc": 0.453416149068323,
      "acc_stderr": 0.03935653891289662,
      "acc_norm": 0.453416149068323,
      "acc_norm_stderr": 0.03935653891289662
    },
    "Cmmlu-world_religions": {
      "acc": 0.475,
      "acc_stderr": 0.03960298254443846,
      "acc_norm": 0.475,
      "acc_norm_stderr": 0.03960298254443846
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/chinese_llama_alpaca_2,load_in_8bit=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:47:43.056800",
    "model_name": "chinese_llama2_alpaca"
  }
}
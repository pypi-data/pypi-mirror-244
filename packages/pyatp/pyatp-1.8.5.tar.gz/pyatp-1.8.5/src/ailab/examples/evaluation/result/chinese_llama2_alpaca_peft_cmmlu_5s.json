{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3254437869822485,
      "acc_stderr": 0.03614867847292203,
      "acc_norm": 0.3254437869822485,
      "acc_norm_stderr": 0.03614867847292203
    },
    "Cmmlu-anatomy": {
      "acc": 0.24324324324324326,
      "acc_stderr": 0.0353866849031339,
      "acc_norm": 0.24324324324324326,
      "acc_norm_stderr": 0.0353866849031339
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364998,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364998
    },
    "Cmmlu-arts": {
      "acc": 0.54375,
      "acc_stderr": 0.039500492593059405,
      "acc_norm": 0.54375,
      "acc_norm_stderr": 0.039500492593059405
    },
    "Cmmlu-astronomy": {
      "acc": 0.3090909090909091,
      "acc_stderr": 0.036085410115739666,
      "acc_norm": 0.3090909090909091,
      "acc_norm_stderr": 0.036085410115739666
    },
    "Cmmlu-business_ethics": {
      "acc": 0.44019138755980863,
      "acc_stderr": 0.034419843468751564,
      "acc_norm": 0.44019138755980863,
      "acc_norm_stderr": 0.034419843468751564
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.35,
      "acc_stderr": 0.037826149818120415,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.037826149818120415
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5114503816793893,
      "acc_stderr": 0.04384140024078016,
      "acc_norm": 0.5114503816793893,
      "acc_norm_stderr": 0.04384140024078016
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.36764705882352944,
      "acc_stderr": 0.04149812123921663,
      "acc_norm": 0.36764705882352944,
      "acc_norm_stderr": 0.04149812123921663
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.4205607476635514,
      "acc_stderr": 0.04794743635189597,
      "acc_norm": 0.4205607476635514,
      "acc_norm_stderr": 0.04794743635189597
    },
    "Cmmlu-chinese_history": {
      "acc": 0.43962848297213625,
      "acc_stderr": 0.027660052586805196,
      "acc_norm": 0.43962848297213625,
      "acc_norm_stderr": 0.027660052586805196
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.29901960784313725,
      "acc_stderr": 0.03213325717373618,
      "acc_norm": 0.29901960784313725,
      "acc_norm_stderr": 0.03213325717373618
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.5139664804469274,
      "acc_stderr": 0.03746196103854561,
      "acc_norm": 0.5139664804469274,
      "acc_norm_stderr": 0.03746196103854561
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.35864978902953587,
      "acc_stderr": 0.031219569445301847,
      "acc_norm": 0.35864978902953587,
      "acc_norm_stderr": 0.031219569445301847
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2830188679245283,
      "acc_stderr": 0.043960933774393765,
      "acc_norm": 0.2830188679245283,
      "acc_norm_stderr": 0.043960933774393765
    },
    "Cmmlu-college_education": {
      "acc": 0.5700934579439252,
      "acc_stderr": 0.04808472349429953,
      "acc_norm": 0.5700934579439252,
      "acc_norm_stderr": 0.04808472349429953
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.047732492983673595,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.047732492983673595
    },
    "Cmmlu-college_law": {
      "acc": 0.35185185185185186,
      "acc_stderr": 0.04616631111801713,
      "acc_norm": 0.35185185185185186,
      "acc_norm_stderr": 0.04616631111801713
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.21904761904761905,
      "acc_stderr": 0.040556911537178254,
      "acc_norm": 0.21904761904761905,
      "acc_norm_stderr": 0.040556911537178254
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.330188679245283,
      "acc_stderr": 0.045894715469579954,
      "acc_norm": 0.330188679245283,
      "acc_norm_stderr": 0.045894715469579954
    },
    "Cmmlu-college_medicine": {
      "acc": 0.304029304029304,
      "acc_stderr": 0.027891299397152923,
      "acc_norm": 0.304029304029304,
      "acc_norm_stderr": 0.027891299397152923
    },
    "Cmmlu-computer_science": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.033540924375915174,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.033540924375915174
    },
    "Cmmlu-computer_security": {
      "acc": 0.4269005847953216,
      "acc_stderr": 0.03793620616529918,
      "acc_norm": 0.4269005847953216,
      "acc_norm_stderr": 0.03793620616529918
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.41496598639455784,
      "acc_stderr": 0.04077747972773979,
      "acc_norm": 0.41496598639455784,
      "acc_norm_stderr": 0.04077747972773979
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.31654676258992803,
      "acc_stderr": 0.03959440284735793,
      "acc_norm": 0.31654676258992803,
      "acc_norm_stderr": 0.03959440284735793
    },
    "Cmmlu-economics": {
      "acc": 0.389937106918239,
      "acc_stderr": 0.03880217268209473,
      "acc_norm": 0.389937106918239,
      "acc_norm_stderr": 0.03880217268209473
    },
    "Cmmlu-education": {
      "acc": 0.37423312883435583,
      "acc_stderr": 0.03802068102899615,
      "acc_norm": 0.37423312883435583,
      "acc_norm_stderr": 0.03802068102899615
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3546511627906977,
      "acc_stderr": 0.036584734259385424,
      "acc_norm": 0.3546511627906977,
      "acc_norm_stderr": 0.036584734259385424
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.40476190476190477,
      "acc_stderr": 0.030981918979415803,
      "acc_norm": 0.40476190476190477,
      "acc_norm_stderr": 0.030981918979415803
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.03502975799413007,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.03502975799413007
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.5672268907563025,
      "acc_stderr": 0.032183581077426124,
      "acc_norm": 0.5672268907563025,
      "acc_norm_stderr": 0.032183581077426124
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.27391304347826084,
      "acc_stderr": 0.029470189815005897,
      "acc_norm": 0.27391304347826084,
      "acc_norm_stderr": 0.029470189815005897
    },
    "Cmmlu-ethnology": {
      "acc": 0.3851851851851852,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.3851851851851852,
      "acc_norm_stderr": 0.042039210401562783
    },
    "Cmmlu-food_science": {
      "acc": 0.35664335664335667,
      "acc_stderr": 0.04019747669236479,
      "acc_norm": 0.35664335664335667,
      "acc_norm_stderr": 0.04019747669236479
    },
    "Cmmlu-genetics": {
      "acc": 0.2784090909090909,
      "acc_stderr": 0.03388193526335358,
      "acc_norm": 0.2784090909090909,
      "acc_norm_stderr": 0.03388193526335358
    },
    "Cmmlu-global_facts": {
      "acc": 0.4228187919463087,
      "acc_stderr": 0.040607137330584464,
      "acc_norm": 0.4228187919463087,
      "acc_norm_stderr": 0.040607137330584464
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.27218934911242604,
      "acc_stderr": 0.03433919627548533,
      "acc_norm": 0.27218934911242604,
      "acc_norm_stderr": 0.03433919627548533
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.24242424242424243,
      "acc_stderr": 0.03744254928577061,
      "acc_norm": 0.24242424242424243,
      "acc_norm_stderr": 0.03744254928577061
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3305084745762712,
      "acc_stderr": 0.04348814779192273,
      "acc_norm": 0.3305084745762712,
      "acc_norm_stderr": 0.04348814779192273
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2073170731707317,
      "acc_stderr": 0.03175217536073676,
      "acc_norm": 0.2073170731707317,
      "acc_norm_stderr": 0.03175217536073676
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.04461272175910507,
      "acc_norm": 0.3181818181818182,
      "acc_norm_stderr": 0.04461272175910507
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.3776223776223776,
      "acc_stderr": 0.040682878492098076,
      "acc_norm": 0.3776223776223776,
      "acc_norm_stderr": 0.040682878492098076
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.42063492063492064,
      "acc_stderr": 0.04415438226743744,
      "acc_norm": 0.42063492063492064,
      "acc_norm_stderr": 0.04415438226743744
    },
    "Cmmlu-international_law": {
      "acc": 0.34594594594594597,
      "acc_stderr": 0.03506727605846201,
      "acc_norm": 0.34594594594594597,
      "acc_norm_stderr": 0.03506727605846201
    },
    "Cmmlu-journalism": {
      "acc": 0.4127906976744186,
      "acc_stderr": 0.03764985943823199,
      "acc_norm": 0.4127906976744186,
      "acc_norm_stderr": 0.03764985943823199
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.41849148418491483,
      "acc_stderr": 0.0243629247438038,
      "acc_norm": 0.41849148418491483,
      "acc_norm_stderr": 0.0243629247438038
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.7616822429906542,
      "acc_stderr": 0.029192770642101563,
      "acc_norm": 0.7616822429906542,
      "acc_norm_stderr": 0.029192770642101563
    },
    "Cmmlu-logical": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04267895997763195,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04267895997763195
    },
    "Cmmlu-machine_learning": {
      "acc": 0.29508196721311475,
      "acc_stderr": 0.04146178164901212,
      "acc_norm": 0.29508196721311475,
      "acc_norm_stderr": 0.04146178164901212
    },
    "Cmmlu-management": {
      "acc": 0.4095238095238095,
      "acc_stderr": 0.03401477718256437,
      "acc_norm": 0.4095238095238095,
      "acc_norm_stderr": 0.03401477718256437
    },
    "Cmmlu-marketing": {
      "acc": 0.5,
      "acc_stderr": 0.037371754637596795,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.037371754637596795
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.48148148148148145,
      "acc_stderr": 0.03644122814742901,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.03644122814742901
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3706896551724138,
      "acc_stderr": 0.04503900094657779,
      "acc_norm": 0.3706896551724138,
      "acc_norm_stderr": 0.04503900094657779
    },
    "Cmmlu-nutrition": {
      "acc": 0.38620689655172413,
      "acc_stderr": 0.04057324734419036,
      "acc_norm": 0.38620689655172413,
      "acc_norm_stderr": 0.04057324734419036
    },
    "Cmmlu-philosophy": {
      "acc": 0.4095238095238095,
      "acc_stderr": 0.04821965555951261,
      "acc_norm": 0.4095238095238095,
      "acc_norm_stderr": 0.04821965555951261
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.4,
      "acc_stderr": 0.03713906763541032,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.03713906763541032
    },
    "Cmmlu-professional_law": {
      "acc": 0.38388625592417064,
      "acc_stderr": 0.03356001010533163,
      "acc_norm": 0.38388625592417064,
      "acc_norm_stderr": 0.03356001010533163
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2845744680851064,
      "acc_stderr": 0.023300462953866237,
      "acc_norm": 0.2845744680851064,
      "acc_norm_stderr": 0.023300462953866237
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.3879310344827586,
      "acc_stderr": 0.03206058570433763,
      "acc_norm": 0.3879310344827586,
      "acc_norm_stderr": 0.03206058570433763
    },
    "Cmmlu-public_relations": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.03781034307794902,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.03781034307794902
    },
    "Cmmlu-security_study": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04292596718256981,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04292596718256981
    },
    "Cmmlu-sociology": {
      "acc": 0.3805309734513274,
      "acc_stderr": 0.03236782707810076,
      "acc_norm": 0.3805309734513274,
      "acc_norm_stderr": 0.03236782707810076
    },
    "Cmmlu-sports_science": {
      "acc": 0.3575757575757576,
      "acc_stderr": 0.03742597043806586,
      "acc_norm": 0.3575757575757576,
      "acc_norm_stderr": 0.03742597043806586
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3027027027027027,
      "acc_stderr": 0.03386945658791665,
      "acc_norm": 0.3027027027027027,
      "acc_norm_stderr": 0.03386945658791665
    },
    "Cmmlu-virology": {
      "acc": 0.42011834319526625,
      "acc_stderr": 0.03808034433196808,
      "acc_norm": 0.42011834319526625,
      "acc_norm_stderr": 0.03808034433196808
    },
    "Cmmlu-world_history": {
      "acc": 0.4720496894409938,
      "acc_stderr": 0.0394666615749653,
      "acc_norm": 0.4720496894409938,
      "acc_norm_stderr": 0.0394666615749653
    },
    "Cmmlu-world_religions": {
      "acc": 0.475,
      "acc_stderr": 0.039602982544438455,
      "acc_norm": 0.475,
      "acc_norm_stderr": 0.039602982544438455
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/chinese_llama_alpaca_2,load_in_8bit=True,dtype='float16',use_accelerate=False,peft=/home/finetuned_models/my_chinese_alpaca_2_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:53:39.554913",
    "model_name": "chinese_llama2_alpaca"
  }
}
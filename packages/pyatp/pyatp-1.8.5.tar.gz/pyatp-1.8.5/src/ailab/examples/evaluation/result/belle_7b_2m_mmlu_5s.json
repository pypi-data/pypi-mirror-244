{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542129,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542129
    },
    "hendrycksTest-anatomy": {
      "acc": 0.31851851851851853,
      "acc_stderr": 0.040247784019771096,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.040247784019771096
    },
    "hendrycksTest-astronomy": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.03583496176361061,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.03583496176361061
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.38113207547169814,
      "acc_stderr": 0.029890609686286644,
      "acc_norm": 0.38113207547169814,
      "acc_norm_stderr": 0.029890609686286644
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3402777777777778,
      "acc_stderr": 0.03962135573486219,
      "acc_norm": 0.3402777777777778,
      "acc_norm_stderr": 0.03962135573486219
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.35260115606936415,
      "acc_stderr": 0.03643037168958548,
      "acc_norm": 0.35260115606936415,
      "acc_norm_stderr": 0.03643037168958548
    },
    "hendrycksTest-college_physics": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.04280105837364395,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364395
    },
    "hendrycksTest-computer_security": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4085106382978723,
      "acc_stderr": 0.03213418026701576,
      "acc_norm": 0.4085106382978723,
      "acc_norm_stderr": 0.03213418026701576
    },
    "hendrycksTest-econometrics": {
      "acc": 0.23684210526315788,
      "acc_stderr": 0.039994238792813344,
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.039994238792813344
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.296551724137931,
      "acc_stderr": 0.038061426873099935,
      "acc_norm": 0.296551724137931,
      "acc_norm_stderr": 0.038061426873099935
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30423280423280424,
      "acc_stderr": 0.023695415009463087,
      "acc_norm": 0.30423280423280424,
      "acc_norm_stderr": 0.023695415009463087
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.25396825396825395,
      "acc_stderr": 0.03893259610604674,
      "acc_norm": 0.25396825396825395,
      "acc_norm_stderr": 0.03893259610604674
    },
    "hendrycksTest-global_facts": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816505,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816505
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.33548387096774196,
      "acc_stderr": 0.026860206444724342,
      "acc_norm": 0.33548387096774196,
      "acc_norm_stderr": 0.026860206444724342
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.30049261083743845,
      "acc_stderr": 0.03225799476233484,
      "acc_norm": 0.30049261083743845,
      "acc_norm_stderr": 0.03225799476233484
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.03401506715249039,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.03401506715249039
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.398989898989899,
      "acc_stderr": 0.03488901616852731,
      "acc_norm": 0.398989898989899,
      "acc_norm_stderr": 0.03488901616852731
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.30569948186528495,
      "acc_stderr": 0.033248379397581594,
      "acc_norm": 0.30569948186528495,
      "acc_norm_stderr": 0.033248379397581594
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3384615384615385,
      "acc_stderr": 0.023991500500313036,
      "acc_norm": 0.3384615384615385,
      "acc_norm_stderr": 0.023991500500313036
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.1962962962962963,
      "acc_stderr": 0.024217421327417162,
      "acc_norm": 0.1962962962962963,
      "acc_norm_stderr": 0.024217421327417162
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.36134453781512604,
      "acc_stderr": 0.03120469122515002,
      "acc_norm": 0.36134453781512604,
      "acc_norm_stderr": 0.03120469122515002
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.31788079470198677,
      "acc_stderr": 0.038020397601079024,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.038020397601079024
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.42935779816513764,
      "acc_stderr": 0.021222286397236497,
      "acc_norm": 0.42935779816513764,
      "acc_norm_stderr": 0.021222286397236497
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.25462962962962965,
      "acc_stderr": 0.029711275860005357,
      "acc_norm": 0.25462962962962965,
      "acc_norm_stderr": 0.029711275860005357
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.03149328104507955,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.03149328104507955
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.37130801687763715,
      "acc_stderr": 0.03145068600744858,
      "acc_norm": 0.37130801687763715,
      "acc_norm_stderr": 0.03145068600744858
    },
    "hendrycksTest-human_aging": {
      "acc": 0.484304932735426,
      "acc_stderr": 0.0335412657542081,
      "acc_norm": 0.484304932735426,
      "acc_norm_stderr": 0.0335412657542081
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.45038167938931295,
      "acc_stderr": 0.04363643698524779,
      "acc_norm": 0.45038167938931295,
      "acc_norm_stderr": 0.04363643698524779
    },
    "hendrycksTest-international_law": {
      "acc": 0.33884297520661155,
      "acc_stderr": 0.043207678075366684,
      "acc_norm": 0.33884297520661155,
      "acc_norm_stderr": 0.043207678075366684
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04803752235190193,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04803752235190193
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.34355828220858897,
      "acc_stderr": 0.03731133519673893,
      "acc_norm": 0.34355828220858897,
      "acc_norm_stderr": 0.03731133519673893
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.33035714285714285,
      "acc_stderr": 0.04464285714285713,
      "acc_norm": 0.33035714285714285,
      "acc_norm_stderr": 0.04464285714285713
    },
    "hendrycksTest-management": {
      "acc": 0.3592233009708738,
      "acc_stderr": 0.04750458399041693,
      "acc_norm": 0.3592233009708738,
      "acc_norm_stderr": 0.04750458399041693
    },
    "hendrycksTest-marketing": {
      "acc": 0.5,
      "acc_stderr": 0.03275608910402091,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.03275608910402091
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.43,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.04975698519562428
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.43167305236270753,
      "acc_stderr": 0.017712228939299787,
      "acc_norm": 0.43167305236270753,
      "acc_norm_stderr": 0.017712228939299787
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3786127167630058,
      "acc_stderr": 0.026113749361310338,
      "acc_norm": 0.3786127167630058,
      "acc_norm_stderr": 0.026113749361310338
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.30392156862745096,
      "acc_stderr": 0.02633661346904664,
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.02633661346904664
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3311897106109325,
      "acc_stderr": 0.02673062072800491,
      "acc_norm": 0.3311897106109325,
      "acc_norm_stderr": 0.02673062072800491
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3487654320987654,
      "acc_stderr": 0.02651759772446501,
      "acc_norm": 0.3487654320987654,
      "acc_norm_stderr": 0.02651759772446501
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2872340425531915,
      "acc_stderr": 0.026992199173064356,
      "acc_norm": 0.2872340425531915,
      "acc_norm_stderr": 0.026992199173064356
    },
    "hendrycksTest-professional_law": {
      "acc": 0.24967405475880053,
      "acc_stderr": 0.01105453837783232,
      "acc_norm": 0.24967405475880053,
      "acc_norm_stderr": 0.01105453837783232
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.375,
      "acc_stderr": 0.029408372932278746,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.029408372932278746
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3284313725490196,
      "acc_stderr": 0.018999707383162666,
      "acc_norm": 0.3284313725490196,
      "acc_norm_stderr": 0.018999707383162666
    },
    "hendrycksTest-public_relations": {
      "acc": 0.4909090909090909,
      "acc_stderr": 0.0478833976870286,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.0478833976870286
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2612244897959184,
      "acc_stderr": 0.028123429335142783,
      "acc_norm": 0.2612244897959184,
      "acc_norm_stderr": 0.028123429335142783
    },
    "hendrycksTest-sociology": {
      "acc": 0.34328358208955223,
      "acc_stderr": 0.03357379665433431,
      "acc_norm": 0.34328358208955223,
      "acc_norm_stderr": 0.03357379665433431
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-virology": {
      "acc": 0.3674698795180723,
      "acc_stderr": 0.03753267402120574,
      "acc_norm": 0.3674698795180723,
      "acc_norm_stderr": 0.03753267402120574
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4093567251461988,
      "acc_stderr": 0.037712831076265434,
      "acc_norm": 0.4093567251461988,
      "acc_norm_stderr": 0.037712831076265434
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/belle_7b_2m,dtype='bfloat16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "4:13:53.957140",
    "model_name": "belle_7b_2m"
  }
}
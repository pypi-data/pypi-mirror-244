{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.40236686390532544,
      "acc_stderr": 0.03783326285416536,
      "acc_norm": 0.40236686390532544,
      "acc_norm_stderr": 0.03783326285416536
    },
    "Cmmlu-anatomy": {
      "acc": 0.28378378378378377,
      "acc_stderr": 0.03718409321285373,
      "acc_norm": 0.28378378378378377,
      "acc_norm_stderr": 0.03718409321285373
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.32926829268292684,
      "acc_stderr": 0.03680913164548252,
      "acc_norm": 0.32926829268292684,
      "acc_norm_stderr": 0.03680913164548252
    },
    "Cmmlu-arts": {
      "acc": 0.525,
      "acc_stderr": 0.03960298254443845,
      "acc_norm": 0.525,
      "acc_norm_stderr": 0.03960298254443845
    },
    "Cmmlu-astronomy": {
      "acc": 0.3515151515151515,
      "acc_stderr": 0.0372820699868265,
      "acc_norm": 0.3515151515151515,
      "acc_norm_stderr": 0.0372820699868265
    },
    "Cmmlu-business_ethics": {
      "acc": 0.44019138755980863,
      "acc_stderr": 0.03441984346875156,
      "acc_norm": 0.44019138755980863,
      "acc_norm_stderr": 0.03441984346875156
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.34375,
      "acc_stderr": 0.03766668927755763,
      "acc_norm": 0.34375,
      "acc_norm_stderr": 0.03766668927755763
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5877862595419847,
      "acc_stderr": 0.04317171194870254,
      "acc_norm": 0.5877862595419847,
      "acc_norm_stderr": 0.04317171194870254
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.3161764705882353,
      "acc_stderr": 0.040019338846834944,
      "acc_norm": 0.3161764705882353,
      "acc_norm_stderr": 0.040019338846834944
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.4392523364485981,
      "acc_stderr": 0.048204529006379074,
      "acc_norm": 0.4392523364485981,
      "acc_norm_stderr": 0.048204529006379074
    },
    "Cmmlu-chinese_history": {
      "acc": 0.4520123839009288,
      "acc_stderr": 0.02773528308208253,
      "acc_norm": 0.4520123839009288,
      "acc_norm_stderr": 0.02773528308208253
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.28921568627450983,
      "acc_stderr": 0.031822318676475544,
      "acc_norm": 0.28921568627450983,
      "acc_norm_stderr": 0.031822318676475544
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.44692737430167595,
      "acc_stderr": 0.03726486555057904,
      "acc_norm": 0.44692737430167595,
      "acc_norm_stderr": 0.03726486555057904
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.379746835443038,
      "acc_stderr": 0.03159188752965849,
      "acc_norm": 0.379746835443038,
      "acc_norm_stderr": 0.03159188752965849
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.16981132075471697,
      "acc_stderr": 0.036641823111517896,
      "acc_norm": 0.16981132075471697,
      "acc_norm_stderr": 0.036641823111517896
    },
    "Cmmlu-college_education": {
      "acc": 0.4485981308411215,
      "acc_stderr": 0.04830698295619321,
      "acc_norm": 0.4485981308411215,
      "acc_norm_stderr": 0.04830698295619321
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.04730439022852894,
      "acc_norm": 0.37735849056603776,
      "acc_norm_stderr": 0.04730439022852894
    },
    "Cmmlu-college_law": {
      "acc": 0.26851851851851855,
      "acc_stderr": 0.04284467968052191,
      "acc_norm": 0.26851851851851855,
      "acc_norm_stderr": 0.04284467968052191
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.24761904761904763,
      "acc_stderr": 0.04232473532055043,
      "acc_norm": 0.24761904761904763,
      "acc_norm_stderr": 0.04232473532055043
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3584905660377358,
      "acc_stderr": 0.04679998780012862,
      "acc_norm": 0.3584905660377358,
      "acc_norm_stderr": 0.04679998780012862
    },
    "Cmmlu-college_medicine": {
      "acc": 0.326007326007326,
      "acc_stderr": 0.02842214271148565,
      "acc_norm": 0.326007326007326,
      "acc_norm_stderr": 0.02842214271148565
    },
    "Cmmlu-computer_science": {
      "acc": 0.39705882352941174,
      "acc_stderr": 0.03434131164719129,
      "acc_norm": 0.39705882352941174,
      "acc_norm_stderr": 0.03434131164719129
    },
    "Cmmlu-computer_security": {
      "acc": 0.4619883040935672,
      "acc_stderr": 0.03823727092882307,
      "acc_norm": 0.4619883040935672,
      "acc_norm_stderr": 0.03823727092882307
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.40816326530612246,
      "acc_stderr": 0.040676304414574864,
      "acc_norm": 0.40816326530612246,
      "acc_norm_stderr": 0.040676304414574864
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.3381294964028777,
      "acc_stderr": 0.04027063698740207,
      "acc_norm": 0.3381294964028777,
      "acc_norm_stderr": 0.04027063698740207
    },
    "Cmmlu-economics": {
      "acc": 0.4025157232704403,
      "acc_stderr": 0.03901450685975137,
      "acc_norm": 0.4025157232704403,
      "acc_norm_stderr": 0.03901450685975137
    },
    "Cmmlu-education": {
      "acc": 0.4049079754601227,
      "acc_stderr": 0.03856672163548914,
      "acc_norm": 0.4049079754601227,
      "acc_norm_stderr": 0.03856672163548914
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3953488372093023,
      "acc_stderr": 0.037389066648335204,
      "acc_norm": 0.3953488372093023,
      "acc_norm_stderr": 0.037389066648335204
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.38095238095238093,
      "acc_stderr": 0.030652119793011915,
      "acc_norm": 0.38095238095238093,
      "acc_norm_stderr": 0.030652119793011915
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.3434343434343434,
      "acc_stderr": 0.03383201223244442,
      "acc_norm": 0.3434343434343434,
      "acc_norm_stderr": 0.03383201223244442
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.5378151260504201,
      "acc_stderr": 0.032385469487589795,
      "acc_norm": 0.5378151260504201,
      "acc_norm_stderr": 0.032385469487589795
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.28695652173913044,
      "acc_stderr": 0.02989154167363546,
      "acc_norm": 0.28695652173913044,
      "acc_norm_stderr": 0.02989154167363546
    },
    "Cmmlu-ethnology": {
      "acc": 0.34814814814814815,
      "acc_stderr": 0.041153246103369526,
      "acc_norm": 0.34814814814814815,
      "acc_norm_stderr": 0.041153246103369526
    },
    "Cmmlu-food_science": {
      "acc": 0.4405594405594406,
      "acc_stderr": 0.041661514977676506,
      "acc_norm": 0.4405594405594406,
      "acc_norm_stderr": 0.041661514977676506
    },
    "Cmmlu-genetics": {
      "acc": 0.3465909090909091,
      "acc_stderr": 0.0359734545643587,
      "acc_norm": 0.3465909090909091,
      "acc_norm_stderr": 0.0359734545643587
    },
    "Cmmlu-global_facts": {
      "acc": 0.40939597315436244,
      "acc_stderr": 0.040419331600394785,
      "acc_norm": 0.40939597315436244,
      "acc_norm_stderr": 0.040419331600394785
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.26627218934911245,
      "acc_stderr": 0.03410167836676975,
      "acc_norm": 0.26627218934911245,
      "acc_norm_stderr": 0.03410167836676975
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.2803030303030303,
      "acc_stderr": 0.03924217639788229,
      "acc_norm": 0.2803030303030303,
      "acc_norm_stderr": 0.03924217639788229
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3559322033898305,
      "acc_stderr": 0.044264595833155146,
      "acc_norm": 0.3559322033898305,
      "acc_norm_stderr": 0.044264595833155146
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.03363591048272823,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.03363591048272823
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.04461272175910509,
      "acc_norm": 0.3181818181818182,
      "acc_norm_stderr": 0.04461272175910509
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.40559440559440557,
      "acc_stderr": 0.041204367311337864,
      "acc_norm": 0.40559440559440557,
      "acc_norm_stderr": 0.041204367311337864
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3968253968253968,
      "acc_stderr": 0.043758884927270605,
      "acc_norm": 0.3968253968253968,
      "acc_norm_stderr": 0.043758884927270605
    },
    "Cmmlu-international_law": {
      "acc": 0.3621621621621622,
      "acc_stderr": 0.035432171151384854,
      "acc_norm": 0.3621621621621622,
      "acc_norm_stderr": 0.035432171151384854
    },
    "Cmmlu-journalism": {
      "acc": 0.4069767441860465,
      "acc_stderr": 0.03756839173779933,
      "acc_norm": 0.4069767441860465,
      "acc_norm_stderr": 0.03756839173779933
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.44525547445255476,
      "acc_stderr": 0.024544784201913448,
      "acc_norm": 0.44525547445255476,
      "acc_norm_stderr": 0.024544784201913448
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.677570093457944,
      "acc_stderr": 0.03202616755131743,
      "acc_norm": 0.677570093457944,
      "acc_norm_stderr": 0.03202616755131743
    },
    "Cmmlu-logical": {
      "acc": 0.3008130081300813,
      "acc_stderr": 0.04152073768551428,
      "acc_norm": 0.3008130081300813,
      "acc_norm_stderr": 0.04152073768551428
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2786885245901639,
      "acc_stderr": 0.04075944659069251,
      "acc_norm": 0.2786885245901639,
      "acc_norm_stderr": 0.04075944659069251
    },
    "Cmmlu-management": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.0342309884498945,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.0342309884498945
    },
    "Cmmlu-marketing": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.03714034835915976,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.03714034835915976
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.42328042328042326,
      "acc_stderr": 0.03603441813251288,
      "acc_norm": 0.42328042328042326,
      "acc_norm_stderr": 0.03603441813251288
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3620689655172414,
      "acc_stderr": 0.04481605202782828,
      "acc_norm": 0.3620689655172414,
      "acc_norm_stderr": 0.04481605202782828
    },
    "Cmmlu-nutrition": {
      "acc": 0.4206896551724138,
      "acc_stderr": 0.0411391498118926,
      "acc_norm": 0.4206896551724138,
      "acc_norm_stderr": 0.0411391498118926
    },
    "Cmmlu-philosophy": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.048526158606197,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.048526158606197
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.37714285714285717,
      "acc_stderr": 0.036742824966085344,
      "acc_norm": 0.37714285714285717,
      "acc_norm_stderr": 0.036742824966085344
    },
    "Cmmlu-professional_law": {
      "acc": 0.35071090047393366,
      "acc_stderr": 0.03292941692271489,
      "acc_norm": 0.35071090047393366,
      "acc_norm_stderr": 0.03292941692271489
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.023099237430720312,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.023099237430720312
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.40948275862068967,
      "acc_stderr": 0.03235400970172884,
      "acc_norm": 0.40948275862068967,
      "acc_norm_stderr": 0.03235400970172884
    },
    "Cmmlu-public_relations": {
      "acc": 0.4425287356321839,
      "acc_stderr": 0.037762342756690645,
      "acc_norm": 0.4425287356321839,
      "acc_norm_stderr": 0.037762342756690645
    },
    "Cmmlu-security_study": {
      "acc": 0.4962962962962963,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.4962962962962963,
      "acc_norm_stderr": 0.04319223625811331
    },
    "Cmmlu-sociology": {
      "acc": 0.3938053097345133,
      "acc_stderr": 0.03257283720180345,
      "acc_norm": 0.3938053097345133,
      "acc_norm_stderr": 0.03257283720180345
    },
    "Cmmlu-sports_science": {
      "acc": 0.44242424242424244,
      "acc_stderr": 0.038783721137112745,
      "acc_norm": 0.44242424242424244,
      "acc_norm_stderr": 0.038783721137112745
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3081081081081081,
      "acc_stderr": 0.03403782277834384,
      "acc_norm": 0.3081081081081081,
      "acc_norm_stderr": 0.03403782277834384
    },
    "Cmmlu-virology": {
      "acc": 0.4260355029585799,
      "acc_stderr": 0.03815142551613447,
      "acc_norm": 0.4260355029585799,
      "acc_norm_stderr": 0.03815142551613447
    },
    "Cmmlu-world_history": {
      "acc": 0.4782608695652174,
      "acc_stderr": 0.03949109157518469,
      "acc_norm": 0.4782608695652174,
      "acc_norm_stderr": 0.03949109157518469
    },
    "Cmmlu-world_religions": {
      "acc": 0.44375,
      "acc_stderr": 0.039400853796259426,
      "acc_norm": 0.44375,
      "acc_norm_stderr": 0.039400853796259426
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/ziya_llama_13b/Ziya-LLaMA-13B,load_in_8bit=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:24:53.022281",
    "model_name": "ziya_llama_13b"
  }
}
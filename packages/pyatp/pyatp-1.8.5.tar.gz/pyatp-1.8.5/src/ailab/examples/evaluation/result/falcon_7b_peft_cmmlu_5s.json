{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.22485207100591717,
      "acc_stderr": 0.032209657045145224,
      "acc_norm": 0.22485207100591717,
      "acc_norm_stderr": 0.032209657045145224
    },
    "Cmmlu-anatomy": {
      "acc": 0.25675675675675674,
      "acc_stderr": 0.036030290036472144,
      "acc_norm": 0.25675675675675674,
      "acc_norm_stderr": 0.036030290036472144
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2682926829268293,
      "acc_stderr": 0.03470398212814534,
      "acc_norm": 0.2682926829268293,
      "acc_norm_stderr": 0.03470398212814534
    },
    "Cmmlu-arts": {
      "acc": 0.28125,
      "acc_stderr": 0.03565632932250201,
      "acc_norm": 0.28125,
      "acc_norm_stderr": 0.03565632932250201
    },
    "Cmmlu-astronomy": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03477691162163659,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03477691162163659
    },
    "Cmmlu-business_ethics": {
      "acc": 0.2583732057416268,
      "acc_stderr": 0.030351822614803417,
      "acc_norm": 0.2583732057416268,
      "acc_norm_stderr": 0.030351822614803417
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.2375,
      "acc_stderr": 0.03374839851779223,
      "acc_norm": 0.2375,
      "acc_norm_stderr": 0.03374839851779223
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.2824427480916031,
      "acc_stderr": 0.03948406125768361,
      "acc_norm": 0.2824427480916031,
      "acc_norm_stderr": 0.03948406125768361
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.03797042496281785,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.03797042496281785
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.308411214953271,
      "acc_stderr": 0.04485760883316699,
      "acc_norm": 0.308411214953271,
      "acc_norm_stderr": 0.04485760883316699
    },
    "Cmmlu-chinese_history": {
      "acc": 0.25386996904024767,
      "acc_stderr": 0.02425409025245804,
      "acc_norm": 0.25386996904024767,
      "acc_norm_stderr": 0.02425409025245804
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.24019607843137256,
      "acc_stderr": 0.02998373305591362,
      "acc_norm": 0.24019607843137256,
      "acc_norm_stderr": 0.02998373305591362
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.25139664804469275,
      "acc_stderr": 0.03251588837184111,
      "acc_norm": 0.25139664804469275,
      "acc_norm_stderr": 0.03251588837184111
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.2320675105485232,
      "acc_stderr": 0.02747974455080852,
      "acc_norm": 0.2320675105485232,
      "acc_norm_stderr": 0.02747974455080852
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.3113207547169811,
      "acc_stderr": 0.0451874553177075,
      "acc_norm": 0.3113207547169811,
      "acc_norm_stderr": 0.0451874553177075
    },
    "Cmmlu-college_education": {
      "acc": 0.308411214953271,
      "acc_stderr": 0.044857608833166994,
      "acc_norm": 0.308411214953271,
      "acc_norm_stderr": 0.044857608833166994
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.20754716981132076,
      "acc_stderr": 0.039577692383779325,
      "acc_norm": 0.20754716981132076,
      "acc_norm_stderr": 0.039577692383779325
    },
    "Cmmlu-college_law": {
      "acc": 0.25,
      "acc_stderr": 0.04186091791394607,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04186091791394607
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2,
      "acc_stderr": 0.03922322702763677,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.03922322702763677
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3113207547169811,
      "acc_stderr": 0.0451874553177075,
      "acc_norm": 0.3113207547169811,
      "acc_norm_stderr": 0.0451874553177075
    },
    "Cmmlu-college_medicine": {
      "acc": 0.25274725274725274,
      "acc_stderr": 0.026350722655564398,
      "acc_norm": 0.25274725274725274,
      "acc_norm_stderr": 0.026350722655564398
    },
    "Cmmlu-computer_science": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.031321798030832904,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.031321798030832904
    },
    "Cmmlu-computer_security": {
      "acc": 0.2573099415204678,
      "acc_stderr": 0.03352799844161865,
      "acc_norm": 0.2573099415204678,
      "acc_norm_stderr": 0.03352799844161865
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2789115646258503,
      "acc_stderr": 0.037115139596751764,
      "acc_norm": 0.2789115646258503,
      "acc_norm_stderr": 0.037115139596751764
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2302158273381295,
      "acc_stderr": 0.035835422943570255,
      "acc_norm": 0.2302158273381295,
      "acc_norm_stderr": 0.035835422943570255
    },
    "Cmmlu-economics": {
      "acc": 0.24528301886792453,
      "acc_stderr": 0.0342292401764445,
      "acc_norm": 0.24528301886792453,
      "acc_norm_stderr": 0.0342292401764445
    },
    "Cmmlu-education": {
      "acc": 0.22085889570552147,
      "acc_stderr": 0.03259177392742178,
      "acc_norm": 0.22085889570552147,
      "acc_norm_stderr": 0.03259177392742178
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.2616279069767442,
      "acc_stderr": 0.033611014038904936,
      "acc_norm": 0.2616279069767442,
      "acc_norm_stderr": 0.033611014038904936
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.24206349206349206,
      "acc_stderr": 0.027036109679236968,
      "acc_norm": 0.24206349206349206,
      "acc_norm_stderr": 0.027036109679236968
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.25252525252525254,
      "acc_stderr": 0.030954055470365897,
      "acc_norm": 0.25252525252525254,
      "acc_norm_stderr": 0.030954055470365897
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.23949579831932774,
      "acc_stderr": 0.02772206549336127,
      "acc_norm": 0.23949579831932774,
      "acc_norm_stderr": 0.02772206549336127
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.02725685083881996,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.02725685083881996
    },
    "Cmmlu-ethnology": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.037857144650666544,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.037857144650666544
    },
    "Cmmlu-food_science": {
      "acc": 0.25874125874125875,
      "acc_stderr": 0.03675137438900237,
      "acc_norm": 0.25874125874125875,
      "acc_norm_stderr": 0.03675137438900237
    },
    "Cmmlu-genetics": {
      "acc": 0.25,
      "acc_stderr": 0.032732683535398856,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.032732683535398856
    },
    "Cmmlu-global_facts": {
      "acc": 0.28187919463087246,
      "acc_stderr": 0.036982767559851,
      "acc_norm": 0.28187919463087246,
      "acc_norm_stderr": 0.036982767559851
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.23076923076923078,
      "acc_stderr": 0.03250593287417369,
      "acc_norm": 0.23076923076923078,
      "acc_norm_stderr": 0.03250593287417369
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.2803030303030303,
      "acc_stderr": 0.03924217639788229,
      "acc_norm": 0.2803030303030303,
      "acc_norm_stderr": 0.03924217639788229
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2457627118644068,
      "acc_stderr": 0.039803298549204336,
      "acc_norm": 0.2457627118644068,
      "acc_norm_stderr": 0.039803298549204336
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.23780487804878048,
      "acc_stderr": 0.03334645408665339,
      "acc_norm": 0.23780487804878048,
      "acc_norm_stderr": 0.03334645408665339
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.041723430387053825,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.041723430387053825
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03737392962695623,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03737392962695623
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.24603174603174602,
      "acc_stderr": 0.038522733649243183,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.038522733649243183
    },
    "Cmmlu-international_law": {
      "acc": 0.2918918918918919,
      "acc_stderr": 0.03351597731741764,
      "acc_norm": 0.2918918918918919,
      "acc_norm_stderr": 0.03351597731741764
    },
    "Cmmlu-journalism": {
      "acc": 0.2616279069767442,
      "acc_stderr": 0.03361101403890494,
      "acc_norm": 0.2616279069767442,
      "acc_norm_stderr": 0.03361101403890494
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.25790754257907544,
      "acc_stderr": 0.021605737836583268,
      "acc_norm": 0.25790754257907544,
      "acc_norm_stderr": 0.021605737836583268
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.22429906542056074,
      "acc_stderr": 0.028580583273338633,
      "acc_norm": 0.22429906542056074,
      "acc_norm_stderr": 0.028580583273338633
    },
    "Cmmlu-logical": {
      "acc": 0.16260162601626016,
      "acc_stderr": 0.03340785433725259,
      "acc_norm": 0.16260162601626016,
      "acc_norm_stderr": 0.03340785433725259
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2459016393442623,
      "acc_stderr": 0.039147319035957334,
      "acc_norm": 0.2459016393442623,
      "acc_norm_stderr": 0.039147319035957334
    },
    "Cmmlu-management": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.03058876451607486,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.03058876451607486
    },
    "Cmmlu-marketing": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.03305282343736876,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.03305282343736876
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.2804232804232804,
      "acc_stderr": 0.03276171742795849,
      "acc_norm": 0.2804232804232804,
      "acc_norm_stderr": 0.03276171742795849
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.2413793103448276,
      "acc_stderr": 0.0399037253226882,
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.0399037253226882
    },
    "Cmmlu-nutrition": {
      "acc": 0.2206896551724138,
      "acc_stderr": 0.03455930201924811,
      "acc_norm": 0.2206896551724138,
      "acc_norm_stderr": 0.03455930201924811
    },
    "Cmmlu-philosophy": {
      "acc": 0.18095238095238095,
      "acc_stderr": 0.03775026958386237,
      "acc_norm": 0.18095238095238095,
      "acc_norm_stderr": 0.03775026958386237
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.03424737867752743,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.03424737867752743
    },
    "Cmmlu-professional_law": {
      "acc": 0.25118483412322273,
      "acc_stderr": 0.029927771242945194,
      "acc_norm": 0.25118483412322273,
      "acc_norm_stderr": 0.029927771242945194
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2579787234042553,
      "acc_stderr": 0.022593550801056263,
      "acc_norm": 0.2579787234042553,
      "acc_norm_stderr": 0.022593550801056263
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.2543103448275862,
      "acc_stderr": 0.028652009240399658,
      "acc_norm": 0.2543103448275862,
      "acc_norm_stderr": 0.028652009240399658
    },
    "Cmmlu-public_relations": {
      "acc": 0.22413793103448276,
      "acc_stderr": 0.031704936515580984,
      "acc_norm": 0.22413793103448276,
      "acc_norm_stderr": 0.031704936515580984
    },
    "Cmmlu-security_study": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.038201699145179055,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.038201699145179055
    },
    "Cmmlu-sociology": {
      "acc": 0.252212389380531,
      "acc_stderr": 0.028952167450890822,
      "acc_norm": 0.252212389380531,
      "acc_norm_stderr": 0.028952167450890822
    },
    "Cmmlu-sports_science": {
      "acc": 0.2787878787878788,
      "acc_stderr": 0.03501438706296781,
      "acc_norm": 0.2787878787878788,
      "acc_norm_stderr": 0.03501438706296781
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2594594594594595,
      "acc_stderr": 0.032314709966177586,
      "acc_norm": 0.2594594594594595,
      "acc_norm_stderr": 0.032314709966177586
    },
    "Cmmlu-virology": {
      "acc": 0.20118343195266272,
      "acc_stderr": 0.030928937238335927,
      "acc_norm": 0.20118343195266272,
      "acc_norm_stderr": 0.030928937238335927
    },
    "Cmmlu-world_history": {
      "acc": 0.2732919254658385,
      "acc_stderr": 0.0352316839773709,
      "acc_norm": 0.2732919254658385,
      "acc_norm_stderr": 0.0352316839773709
    },
    "Cmmlu-world_religions": {
      "acc": 0.2875,
      "acc_stderr": 0.035893251060583956,
      "acc_norm": 0.2875,
      "acc_norm_stderr": 0.035893251060583956
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/falcon_7b,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_falcon_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:13:55.797949",
    "model_name": "falcon_7b"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.34,
      "acc_stderr": 0.047609522856952365,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.047609522856952365
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3925925925925926,
      "acc_stderr": 0.04218506215368879,
      "acc_norm": 0.3925925925925926,
      "acc_norm_stderr": 0.04218506215368879
    },
    "hendrycksTest-astronomy": {
      "acc": 0.4342105263157895,
      "acc_stderr": 0.0403356566784832,
      "acc_norm": 0.4342105263157895,
      "acc_norm_stderr": 0.0403356566784832
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4867924528301887,
      "acc_stderr": 0.030762134874500482,
      "acc_norm": 0.4867924528301887,
      "acc_norm_stderr": 0.030762134874500482
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3958333333333333,
      "acc_stderr": 0.04089465449325582,
      "acc_norm": 0.3958333333333333,
      "acc_norm_stderr": 0.04089465449325582
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3699421965317919,
      "acc_stderr": 0.0368122963339432,
      "acc_norm": 0.3699421965317919,
      "acc_norm_stderr": 0.0368122963339432
    },
    "hendrycksTest-college_physics": {
      "acc": 0.19607843137254902,
      "acc_stderr": 0.03950581861179964,
      "acc_norm": 0.19607843137254902,
      "acc_norm_stderr": 0.03950581861179964
    },
    "hendrycksTest-computer_security": {
      "acc": 0.55,
      "acc_stderr": 0.04999999999999999,
      "acc_norm": 0.55,
      "acc_norm_stderr": 0.04999999999999999
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.425531914893617,
      "acc_stderr": 0.03232146916224468,
      "acc_norm": 0.425531914893617,
      "acc_norm_stderr": 0.03232146916224468
    },
    "hendrycksTest-econometrics": {
      "acc": 0.32456140350877194,
      "acc_stderr": 0.04404556157374767,
      "acc_norm": 0.32456140350877194,
      "acc_norm_stderr": 0.04404556157374767
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4068965517241379,
      "acc_stderr": 0.04093793981266237,
      "acc_norm": 0.4068965517241379,
      "acc_norm_stderr": 0.04093793981266237
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.28835978835978837,
      "acc_stderr": 0.023330654054535886,
      "acc_norm": 0.28835978835978837,
      "acc_norm_stderr": 0.023330654054535886
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.29365079365079366,
      "acc_stderr": 0.04073524322147125,
      "acc_norm": 0.29365079365079366,
      "acc_norm_stderr": 0.04073524322147125
    },
    "hendrycksTest-global_facts": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5,
      "acc_stderr": 0.028444006199428714,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.028444006199428714
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.37438423645320196,
      "acc_stderr": 0.03405155380561952,
      "acc_norm": 0.37438423645320196,
      "acc_norm_stderr": 0.03405155380561952
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.5696969696969697,
      "acc_stderr": 0.03866225962879077,
      "acc_norm": 0.5696969696969697,
      "acc_norm_stderr": 0.03866225962879077
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5858585858585859,
      "acc_stderr": 0.03509438348879629,
      "acc_norm": 0.5858585858585859,
      "acc_norm_stderr": 0.03509438348879629
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6269430051813472,
      "acc_stderr": 0.034902055920485744,
      "acc_norm": 0.6269430051813472,
      "acc_norm_stderr": 0.034902055920485744
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4128205128205128,
      "acc_stderr": 0.0249626835643318,
      "acc_norm": 0.4128205128205128,
      "acc_norm_stderr": 0.0249626835643318
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.02696242432507382,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.02696242432507382
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4327731092436975,
      "acc_stderr": 0.03218358107742613,
      "acc_norm": 0.4327731092436975,
      "acc_norm_stderr": 0.03218358107742613
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2847682119205298,
      "acc_stderr": 0.03684881521389023,
      "acc_norm": 0.2847682119205298,
      "acc_norm_stderr": 0.03684881521389023
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6293577981651376,
      "acc_stderr": 0.02070745816435298,
      "acc_norm": 0.6293577981651376,
      "acc_norm_stderr": 0.02070745816435298
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3425925925925926,
      "acc_stderr": 0.03236585252602157,
      "acc_norm": 0.3425925925925926,
      "acc_norm_stderr": 0.03236585252602157
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.5245098039215687,
      "acc_stderr": 0.03505093194348798,
      "acc_norm": 0.5245098039215687,
      "acc_norm_stderr": 0.03505093194348798
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6244725738396625,
      "acc_stderr": 0.03152256243091156,
      "acc_norm": 0.6244725738396625,
      "acc_norm_stderr": 0.03152256243091156
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4798206278026906,
      "acc_stderr": 0.033530461674123,
      "acc_norm": 0.4798206278026906,
      "acc_norm_stderr": 0.033530461674123
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5572519083969466,
      "acc_stderr": 0.043564472026650695,
      "acc_norm": 0.5572519083969466,
      "acc_norm_stderr": 0.043564472026650695
    },
    "hendrycksTest-international_law": {
      "acc": 0.6033057851239669,
      "acc_stderr": 0.044658697805310094,
      "acc_norm": 0.6033057851239669,
      "acc_norm_stderr": 0.044658697805310094
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.46296296296296297,
      "acc_stderr": 0.04820403072760627,
      "acc_norm": 0.46296296296296297,
      "acc_norm_stderr": 0.04820403072760627
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.4662576687116564,
      "acc_stderr": 0.039194155450484096,
      "acc_norm": 0.4662576687116564,
      "acc_norm_stderr": 0.039194155450484096
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.042878587513404544,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.042878587513404544
    },
    "hendrycksTest-management": {
      "acc": 0.6601941747572816,
      "acc_stderr": 0.04689765937278134,
      "acc_norm": 0.6601941747572816,
      "acc_norm_stderr": 0.04689765937278134
    },
    "hendrycksTest-marketing": {
      "acc": 0.6324786324786325,
      "acc_stderr": 0.031585391577456365,
      "acc_norm": 0.6324786324786325,
      "acc_norm_stderr": 0.031585391577456365
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.49,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.49,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6155810983397191,
      "acc_stderr": 0.01739568874281962,
      "acc_norm": 0.6155810983397191,
      "acc_norm_stderr": 0.01739568874281962
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4421965317919075,
      "acc_stderr": 0.026738603643807396,
      "acc_norm": 0.4421965317919075,
      "acc_norm_stderr": 0.026738603643807396
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.49019607843137253,
      "acc_stderr": 0.028624412550167958,
      "acc_norm": 0.49019607843137253,
      "acc_norm_stderr": 0.028624412550167958
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5176848874598071,
      "acc_stderr": 0.02838032284907713,
      "acc_norm": 0.5176848874598071,
      "acc_norm_stderr": 0.02838032284907713
    },
    "hendrycksTest-prehistory": {
      "acc": 0.48148148148148145,
      "acc_stderr": 0.027801656212323674,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.027801656212323674
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3723404255319149,
      "acc_stderr": 0.028838921471251455,
      "acc_norm": 0.3723404255319149,
      "acc_norm_stderr": 0.028838921471251455
    },
    "hendrycksTest-professional_law": {
      "acc": 0.34419817470664926,
      "acc_stderr": 0.012134433741002574,
      "acc_norm": 0.34419817470664926,
      "acc_norm_stderr": 0.012134433741002574
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.49264705882352944,
      "acc_stderr": 0.030369552523902173,
      "acc_norm": 0.49264705882352944,
      "acc_norm_stderr": 0.030369552523902173
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.43137254901960786,
      "acc_stderr": 0.02003639376835263,
      "acc_norm": 0.43137254901960786,
      "acc_norm_stderr": 0.02003639376835263
    },
    "hendrycksTest-public_relations": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04769300568972745,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04769300568972745
    },
    "hendrycksTest-security_studies": {
      "acc": 0.46122448979591835,
      "acc_stderr": 0.031912820526692774,
      "acc_norm": 0.46122448979591835,
      "acc_norm_stderr": 0.031912820526692774
    },
    "hendrycksTest-sociology": {
      "acc": 0.6218905472636815,
      "acc_stderr": 0.034288678487786564,
      "acc_norm": 0.6218905472636815,
      "acc_norm_stderr": 0.034288678487786564
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.6,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-virology": {
      "acc": 0.3855421686746988,
      "acc_stderr": 0.0378913442461155,
      "acc_norm": 0.3855421686746988,
      "acc_norm_stderr": 0.0378913442461155
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6783625730994152,
      "acc_stderr": 0.03582529442573122,
      "acc_norm": 0.6783625730994152,
      "acc_norm_stderr": 0.03582529442573122
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/chinese_llama_alpaca_2,load_in_8bit=True,dtype='float16',use_accelerate=False,peft=/home/finetuned_models/my_chinese_alpaca_2_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "4:26:07.450526",
    "model_name": "chinese_llama2_alpaca"
  }
}
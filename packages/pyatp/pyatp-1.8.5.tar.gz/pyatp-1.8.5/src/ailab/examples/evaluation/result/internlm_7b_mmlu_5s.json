{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322716,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322716
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4888888888888889,
      "acc_stderr": 0.04318275491977976,
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.04318275491977976
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5592105263157895,
      "acc_stderr": 0.04040311062490436,
      "acc_norm": 0.5592105263157895,
      "acc_norm_stderr": 0.04040311062490436
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.52,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.52,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5320754716981132,
      "acc_stderr": 0.03070948699255655,
      "acc_norm": 0.5320754716981132,
      "acc_norm_stderr": 0.03070948699255655
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5069444444444444,
      "acc_stderr": 0.04180806750294938,
      "acc_norm": 0.5069444444444444,
      "acc_norm_stderr": 0.04180806750294938
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.03794012674697029,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.03794012674697029
    },
    "hendrycksTest-college_physics": {
      "acc": 0.3431372549019608,
      "acc_stderr": 0.04724007352383888,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.04724007352383888
    },
    "hendrycksTest-computer_security": {
      "acc": 0.62,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.451063829787234,
      "acc_stderr": 0.032529096196131965,
      "acc_norm": 0.451063829787234,
      "acc_norm_stderr": 0.032529096196131965
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2894736842105263,
      "acc_stderr": 0.042663394431593935,
      "acc_norm": 0.2894736842105263,
      "acc_norm_stderr": 0.042663394431593935
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.5586206896551724,
      "acc_stderr": 0.04137931034482757,
      "acc_norm": 0.5586206896551724,
      "acc_norm_stderr": 0.04137931034482757
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.024677862841332786,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.024677862841332786
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.24603174603174602,
      "acc_stderr": 0.03852273364924315,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.03852273364924315
    },
    "hendrycksTest-global_facts": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5967741935483871,
      "acc_stderr": 0.027906150826041146,
      "acc_norm": 0.5967741935483871,
      "acc_norm_stderr": 0.027906150826041146
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.03481904844438804,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.03481904844438804
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.42,
      "acc_stderr": 0.04960449637488584,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.04960449637488584
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6,
      "acc_stderr": 0.038254602783800246,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.038254602783800246
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.6464646464646465,
      "acc_stderr": 0.03406086723547155,
      "acc_norm": 0.6464646464646465,
      "acc_norm_stderr": 0.03406086723547155
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7202072538860104,
      "acc_stderr": 0.032396370467357036,
      "acc_norm": 0.7202072538860104,
      "acc_norm_stderr": 0.032396370467357036
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4794871794871795,
      "acc_stderr": 0.025329663163489943,
      "acc_norm": 0.4794871794871795,
      "acc_norm_stderr": 0.025329663163489943
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.3296296296296296,
      "acc_stderr": 0.028661201116524582,
      "acc_norm": 0.3296296296296296,
      "acc_norm_stderr": 0.028661201116524582
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.49159663865546216,
      "acc_stderr": 0.032473902765696686,
      "acc_norm": 0.49159663865546216,
      "acc_norm_stderr": 0.032473902765696686
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33774834437086093,
      "acc_stderr": 0.038615575462551684,
      "acc_norm": 0.33774834437086093,
      "acc_norm_stderr": 0.038615575462551684
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7045871559633028,
      "acc_stderr": 0.019560619182976,
      "acc_norm": 0.7045871559633028,
      "acc_norm_stderr": 0.019560619182976
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.37962962962962965,
      "acc_stderr": 0.03309682581119035,
      "acc_norm": 0.37962962962962965,
      "acc_norm_stderr": 0.03309682581119035
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6372549019607843,
      "acc_stderr": 0.03374499356319354,
      "acc_norm": 0.6372549019607843,
      "acc_norm_stderr": 0.03374499356319354
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6962025316455697,
      "acc_stderr": 0.029936696387138608,
      "acc_norm": 0.6962025316455697,
      "acc_norm_stderr": 0.029936696387138608
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6143497757847534,
      "acc_stderr": 0.03266842214289202,
      "acc_norm": 0.6143497757847534,
      "acc_norm_stderr": 0.03266842214289202
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6335877862595419,
      "acc_stderr": 0.042258754519696365,
      "acc_norm": 0.6335877862595419,
      "acc_norm_stderr": 0.042258754519696365
    },
    "hendrycksTest-international_law": {
      "acc": 0.6528925619834711,
      "acc_stderr": 0.043457245702925335,
      "acc_norm": 0.6528925619834711,
      "acc_norm_stderr": 0.043457245702925335
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6388888888888888,
      "acc_stderr": 0.04643454608906275,
      "acc_norm": 0.6388888888888888,
      "acc_norm_stderr": 0.04643454608906275
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5521472392638037,
      "acc_stderr": 0.03906947479456607,
      "acc_norm": 0.5521472392638037,
      "acc_norm_stderr": 0.03906947479456607
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764376,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764376
    },
    "hendrycksTest-management": {
      "acc": 0.6893203883495146,
      "acc_stderr": 0.045821241601615506,
      "acc_norm": 0.6893203883495146,
      "acc_norm_stderr": 0.045821241601615506
    },
    "hendrycksTest-marketing": {
      "acc": 0.7991452991452992,
      "acc_stderr": 0.026246772946890488,
      "acc_norm": 0.7991452991452992,
      "acc_norm_stderr": 0.026246772946890488
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.54,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.05009082659620332
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6781609195402298,
      "acc_stderr": 0.0167063814150579,
      "acc_norm": 0.6781609195402298,
      "acc_norm_stderr": 0.0167063814150579
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5491329479768786,
      "acc_stderr": 0.026788811931562753,
      "acc_norm": 0.5491329479768786,
      "acc_norm_stderr": 0.026788811931562753
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808835
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5392156862745098,
      "acc_stderr": 0.028541722692618874,
      "acc_norm": 0.5392156862745098,
      "acc_norm_stderr": 0.028541722692618874
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6045016077170418,
      "acc_stderr": 0.027770918531427838,
      "acc_norm": 0.6045016077170418,
      "acc_norm_stderr": 0.027770918531427838
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6049382716049383,
      "acc_stderr": 0.027201117666925654,
      "acc_norm": 0.6049382716049383,
      "acc_norm_stderr": 0.027201117666925654
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.35106382978723405,
      "acc_stderr": 0.028473501272963764,
      "acc_norm": 0.35106382978723405,
      "acc_norm_stderr": 0.028473501272963764
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3767926988265971,
      "acc_stderr": 0.012376459593894397,
      "acc_norm": 0.3767926988265971,
      "acc_norm_stderr": 0.012376459593894397
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.3713235294117647,
      "acc_stderr": 0.02934980313976587,
      "acc_norm": 0.3713235294117647,
      "acc_norm_stderr": 0.02934980313976587
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5065359477124183,
      "acc_stderr": 0.020226106567657807,
      "acc_norm": 0.5065359477124183,
      "acc_norm_stderr": 0.020226106567657807
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6181818181818182,
      "acc_stderr": 0.046534298079135075,
      "acc_norm": 0.6181818181818182,
      "acc_norm_stderr": 0.046534298079135075
    },
    "hendrycksTest-security_studies": {
      "acc": 0.6,
      "acc_stderr": 0.03136250240935893,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.03136250240935893
    },
    "hendrycksTest-sociology": {
      "acc": 0.7313432835820896,
      "acc_stderr": 0.03134328358208954,
      "acc_norm": 0.7313432835820896,
      "acc_norm_stderr": 0.03134328358208954
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.78,
      "acc_stderr": 0.041633319989322626,
      "acc_norm": 0.78,
      "acc_norm_stderr": 0.041633319989322626
    },
    "hendrycksTest-virology": {
      "acc": 0.4578313253012048,
      "acc_stderr": 0.0387862677100236,
      "acc_norm": 0.4578313253012048,
      "acc_norm_stderr": 0.0387862677100236
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6608187134502924,
      "acc_stderr": 0.03631053496488904,
      "acc_norm": 0.6608187134502924,
      "acc_norm_stderr": 0.03631053496488904
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/internlm_7b,dtype='float16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:36:57.692200",
    "model_name": "internlm_7b"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.4556213017751479,
      "acc_stderr": 0.038423589228359284,
      "acc_norm": 0.4556213017751479,
      "acc_norm_stderr": 0.038423589228359284
    },
    "Cmmlu-anatomy": {
      "acc": 0.35135135135135137,
      "acc_stderr": 0.03937466805863152,
      "acc_norm": 0.35135135135135137,
      "acc_norm_stderr": 0.03937466805863152
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.29878048780487804,
      "acc_stderr": 0.035851663369096634,
      "acc_norm": 0.29878048780487804,
      "acc_norm_stderr": 0.035851663369096634
    },
    "Cmmlu-arts": {
      "acc": 0.7375,
      "acc_stderr": 0.034893706520187605,
      "acc_norm": 0.7375,
      "acc_norm_stderr": 0.034893706520187605
    },
    "Cmmlu-astronomy": {
      "acc": 0.3575757575757576,
      "acc_stderr": 0.03742597043806587,
      "acc_norm": 0.3575757575757576,
      "acc_norm_stderr": 0.03742597043806587
    },
    "Cmmlu-business_ethics": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.03452520569603411,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.03452520569603411
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.53125,
      "acc_stderr": 0.039575057062617526,
      "acc_norm": 0.53125,
      "acc_norm_stderr": 0.039575057062617526
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.7175572519083969,
      "acc_stderr": 0.03948406125768361,
      "acc_norm": 0.7175572519083969,
      "acc_norm_stderr": 0.03948406125768361
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.5441176470588235,
      "acc_stderr": 0.04286530438633657,
      "acc_norm": 0.5441176470588235,
      "acc_norm_stderr": 0.04286530438633657
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.6822429906542056,
      "acc_stderr": 0.045223500773820306,
      "acc_norm": 0.6822429906542056,
      "acc_norm_stderr": 0.045223500773820306
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6563467492260062,
      "acc_stderr": 0.026466649235579304,
      "acc_norm": 0.6563467492260062,
      "acc_norm_stderr": 0.026466649235579304
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.034107853389047184,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.034107853389047184
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.6927374301675978,
      "acc_stderr": 0.03458033173302765,
      "acc_norm": 0.6927374301675978,
      "acc_norm_stderr": 0.03458033173302765
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4472573839662447,
      "acc_stderr": 0.03236564251614193,
      "acc_norm": 0.4472573839662447,
      "acc_norm_stderr": 0.03236564251614193
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.20754716981132076,
      "acc_stderr": 0.039577692383779325,
      "acc_norm": 0.20754716981132076,
      "acc_norm_stderr": 0.039577692383779325
    },
    "Cmmlu-college_education": {
      "acc": 0.5794392523364486,
      "acc_stderr": 0.047947436351895946,
      "acc_norm": 0.5794392523364486,
      "acc_norm_stderr": 0.047947436351895946
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.4339622641509434,
      "acc_stderr": 0.048367542978238184,
      "acc_norm": 0.4339622641509434,
      "acc_norm_stderr": 0.048367542978238184
    },
    "Cmmlu-college_law": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199985,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199985
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.043362909039199406,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.043362909039199406
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.44339622641509435,
      "acc_stderr": 0.048481318229754794,
      "acc_norm": 0.44339622641509435,
      "acc_norm_stderr": 0.048481318229754794
    },
    "Cmmlu-college_medicine": {
      "acc": 0.4358974358974359,
      "acc_stderr": 0.030066767691175847,
      "acc_norm": 0.4358974358974359,
      "acc_norm_stderr": 0.030066767691175847
    },
    "Cmmlu-computer_science": {
      "acc": 0.4852941176470588,
      "acc_stderr": 0.035077938347913236,
      "acc_norm": 0.4852941176470588,
      "acc_norm_stderr": 0.035077938347913236
    },
    "Cmmlu-computer_security": {
      "acc": 0.6374269005847953,
      "acc_stderr": 0.0368713061556206,
      "acc_norm": 0.6374269005847953,
      "acc_norm_stderr": 0.0368713061556206
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.6054421768707483,
      "acc_stderr": 0.04044969371311286,
      "acc_norm": 0.6054421768707483,
      "acc_norm_stderr": 0.04044969371311286
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.38848920863309355,
      "acc_stderr": 0.041490818209760694,
      "acc_norm": 0.38848920863309355,
      "acc_norm_stderr": 0.041490818209760694
    },
    "Cmmlu-economics": {
      "acc": 0.49056603773584906,
      "acc_stderr": 0.03977078314701102,
      "acc_norm": 0.49056603773584906,
      "acc_norm_stderr": 0.03977078314701102
    },
    "Cmmlu-education": {
      "acc": 0.6196319018404908,
      "acc_stderr": 0.03814269893261836,
      "acc_norm": 0.6196319018404908,
      "acc_norm_stderr": 0.03814269893261836
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.45348837209302323,
      "acc_stderr": 0.03807016210250966,
      "acc_norm": 0.45348837209302323,
      "acc_norm_stderr": 0.03807016210250966
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.44047619047619047,
      "acc_stderr": 0.03133528465073391,
      "acc_norm": 0.44047619047619047,
      "acc_norm_stderr": 0.03133528465073391
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.5858585858585859,
      "acc_stderr": 0.035094383488796295,
      "acc_norm": 0.5858585858585859,
      "acc_norm_stderr": 0.035094383488796295
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.7142857142857143,
      "acc_stderr": 0.02934457250063434,
      "acc_norm": 0.7142857142857143,
      "acc_norm_stderr": 0.02934457250063434
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.3391304347826087,
      "acc_stderr": 0.03128408938822597,
      "acc_norm": 0.3391304347826087,
      "acc_norm_stderr": 0.03128408938822597
    },
    "Cmmlu-ethnology": {
      "acc": 0.5703703703703704,
      "acc_stderr": 0.04276349494376599,
      "acc_norm": 0.5703703703703704,
      "acc_norm_stderr": 0.04276349494376599
    },
    "Cmmlu-food_science": {
      "acc": 0.5594405594405595,
      "acc_stderr": 0.04166151497767656,
      "acc_norm": 0.5594405594405595,
      "acc_norm_stderr": 0.04166151497767656
    },
    "Cmmlu-genetics": {
      "acc": 0.3977272727272727,
      "acc_stderr": 0.03699731953659028,
      "acc_norm": 0.3977272727272727,
      "acc_norm_stderr": 0.03699731953659028
    },
    "Cmmlu-global_facts": {
      "acc": 0.6040268456375839,
      "acc_stderr": 0.040200377787210195,
      "acc_norm": 0.6040268456375839,
      "acc_norm_stderr": 0.040200377787210195
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.4970414201183432,
      "acc_stderr": 0.038575162160962455,
      "acc_norm": 0.4970414201183432,
      "acc_norm_stderr": 0.038575162160962455
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.4621212121212121,
      "acc_stderr": 0.04355966316653445,
      "acc_norm": 0.4621212121212121,
      "acc_norm_stderr": 0.04355966316653445
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.576271186440678,
      "acc_stderr": 0.04568404181144862,
      "acc_norm": 0.576271186440678,
      "acc_norm_stderr": 0.04568404181144862
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2621951219512195,
      "acc_stderr": 0.03445000289173461,
      "acc_norm": 0.2621951219512195,
      "acc_norm_stderr": 0.03445000289173461
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.4909090909090909,
      "acc_stderr": 0.04788339768702861,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.04788339768702861
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5944055944055944,
      "acc_stderr": 0.04120436731133788,
      "acc_norm": 0.5944055944055944,
      "acc_norm_stderr": 0.04120436731133788
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.5793650793650794,
      "acc_stderr": 0.04415438226743746,
      "acc_norm": 0.5793650793650794,
      "acc_norm_stderr": 0.04415438226743746
    },
    "Cmmlu-international_law": {
      "acc": 0.41081081081081083,
      "acc_stderr": 0.03626931932955475,
      "acc_norm": 0.41081081081081083,
      "acc_norm_stderr": 0.03626931932955475
    },
    "Cmmlu-journalism": {
      "acc": 0.563953488372093,
      "acc_stderr": 0.03792189197270774,
      "acc_norm": 0.563953488372093,
      "acc_norm_stderr": 0.03792189197270774
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5109489051094891,
      "acc_stderr": 0.024687318828080028,
      "acc_norm": 0.5109489051094891,
      "acc_norm_stderr": 0.024687318828080028
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8551401869158879,
      "acc_stderr": 0.024115863483127636,
      "acc_norm": 0.8551401869158879,
      "acc_norm_stderr": 0.024115863483127636
    },
    "Cmmlu-logical": {
      "acc": 0.43089430894308944,
      "acc_stderr": 0.04483342607880305,
      "acc_norm": 0.43089430894308944,
      "acc_norm_stderr": 0.04483342607880305
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4426229508196721,
      "acc_stderr": 0.04515426947106743,
      "acc_norm": 0.4426229508196721,
      "acc_norm_stderr": 0.04515426947106743
    },
    "Cmmlu-management": {
      "acc": 0.6238095238095238,
      "acc_stderr": 0.03350863645112521,
      "acc_norm": 0.6238095238095238,
      "acc_norm_stderr": 0.03350863645112521
    },
    "Cmmlu-marketing": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.03643731289325199,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.03643731289325199
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.6031746031746031,
      "acc_stderr": 0.035681436354467154,
      "acc_norm": 0.6031746031746031,
      "acc_norm_stderr": 0.035681436354467154
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.39655172413793105,
      "acc_stderr": 0.04561640191490673,
      "acc_norm": 0.39655172413793105,
      "acc_norm_stderr": 0.04561640191490673
    },
    "Cmmlu-nutrition": {
      "acc": 0.47586206896551725,
      "acc_stderr": 0.0416180850350153,
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.0416180850350153
    },
    "Cmmlu-philosophy": {
      "acc": 0.6285714285714286,
      "acc_stderr": 0.047380354147934296,
      "acc_norm": 0.6285714285714286,
      "acc_norm_stderr": 0.047380354147934296
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5485714285714286,
      "acc_stderr": 0.037725628985298354,
      "acc_norm": 0.5485714285714286,
      "acc_norm_stderr": 0.037725628985298354
    },
    "Cmmlu-professional_law": {
      "acc": 0.41706161137440756,
      "acc_stderr": 0.03402528637381252,
      "acc_norm": 0.41706161137440756,
      "acc_norm_stderr": 0.03402528637381252
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.34308510638297873,
      "acc_stderr": 0.024515449069850325,
      "acc_norm": 0.34308510638297873,
      "acc_norm_stderr": 0.024515449069850325
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5775862068965517,
      "acc_stderr": 0.032499109578172314,
      "acc_norm": 0.5775862068965517,
      "acc_norm_stderr": 0.032499109578172314
    },
    "Cmmlu-public_relations": {
      "acc": 0.5459770114942529,
      "acc_stderr": 0.037853239139815434,
      "acc_norm": 0.5459770114942529,
      "acc_norm_stderr": 0.037853239139815434
    },
    "Cmmlu-security_study": {
      "acc": 0.6074074074074074,
      "acc_stderr": 0.04218506215368879,
      "acc_norm": 0.6074074074074074,
      "acc_norm_stderr": 0.04218506215368879
    },
    "Cmmlu-sociology": {
      "acc": 0.5575221238938053,
      "acc_stderr": 0.033112012272335754,
      "acc_norm": 0.5575221238938053,
      "acc_norm_stderr": 0.033112012272335754
    },
    "Cmmlu-sports_science": {
      "acc": 0.5636363636363636,
      "acc_stderr": 0.03872592983524754,
      "acc_norm": 0.5636363636363636,
      "acc_norm_stderr": 0.03872592983524754
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.4810810810810811,
      "acc_stderr": 0.036834092970087065,
      "acc_norm": 0.4810810810810811,
      "acc_norm_stderr": 0.036834092970087065
    },
    "Cmmlu-virology": {
      "acc": 0.47337278106508873,
      "acc_stderr": 0.038521097436200316,
      "acc_norm": 0.47337278106508873,
      "acc_norm_stderr": 0.038521097436200316
    },
    "Cmmlu-world_history": {
      "acc": 0.6956521739130435,
      "acc_stderr": 0.03637652289278586,
      "acc_norm": 0.6956521739130435,
      "acc_norm_stderr": 0.03637652289278586
    },
    "Cmmlu-world_religions": {
      "acc": 0.5625,
      "acc_stderr": 0.0393415738622931,
      "acc_norm": 0.5625,
      "acc_norm_stderr": 0.0393415738622931
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/internlm_7b,dtype='float16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:58:35.432010",
    "model_name": "internlm_7b"
  }
}
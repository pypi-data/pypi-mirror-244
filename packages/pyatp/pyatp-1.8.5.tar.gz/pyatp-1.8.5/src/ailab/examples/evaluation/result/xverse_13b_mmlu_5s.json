{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.34,
      "acc_stderr": 0.047609522856952365,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.047609522856952365
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4962962962962963,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.4962962962962963,
      "acc_norm_stderr": 0.04319223625811331
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5460526315789473,
      "acc_stderr": 0.04051646342874142,
      "acc_norm": 0.5460526315789473,
      "acc_norm_stderr": 0.04051646342874142
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.6,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.569811320754717,
      "acc_stderr": 0.030471445867183235,
      "acc_norm": 0.569811320754717,
      "acc_norm_stderr": 0.030471445867183235
    },
    "hendrycksTest-college_biology": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.04076663253918567,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.04076663253918567
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.5491329479768786,
      "acc_stderr": 0.0379401267469703,
      "acc_norm": 0.5491329479768786,
      "acc_norm_stderr": 0.0379401267469703
    },
    "hendrycksTest-college_physics": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.04755129616062946,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.04755129616062946
    },
    "hendrycksTest-computer_security": {
      "acc": 0.73,
      "acc_stderr": 0.04461960433384739,
      "acc_norm": 0.73,
      "acc_norm_stderr": 0.04461960433384739
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4765957446808511,
      "acc_stderr": 0.032650194750335815,
      "acc_norm": 0.4765957446808511,
      "acc_norm_stderr": 0.032650194750335815
    },
    "hendrycksTest-econometrics": {
      "acc": 0.41228070175438597,
      "acc_stderr": 0.04630653203366595,
      "acc_norm": 0.41228070175438597,
      "acc_norm_stderr": 0.04630653203366595
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.5862068965517241,
      "acc_stderr": 0.04104269211806232,
      "acc_norm": 0.5862068965517241,
      "acc_norm_stderr": 0.04104269211806232
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.31216931216931215,
      "acc_stderr": 0.023865206836972602,
      "acc_norm": 0.31216931216931215,
      "acc_norm_stderr": 0.023865206836972602
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2698412698412698,
      "acc_stderr": 0.03970158273235172,
      "acc_norm": 0.2698412698412698,
      "acc_norm_stderr": 0.03970158273235172
    },
    "hendrycksTest-global_facts": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6967741935483871,
      "acc_stderr": 0.026148685930671753,
      "acc_norm": 0.6967741935483871,
      "acc_norm_stderr": 0.026148685930671753
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.43842364532019706,
      "acc_stderr": 0.03491207857486518,
      "acc_norm": 0.43842364532019706,
      "acc_norm_stderr": 0.03491207857486518
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.59,
      "acc_stderr": 0.04943110704237102,
      "acc_norm": 0.59,
      "acc_norm_stderr": 0.04943110704237102
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.703030303030303,
      "acc_stderr": 0.0356796977226805,
      "acc_norm": 0.703030303030303,
      "acc_norm_stderr": 0.0356796977226805
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7373737373737373,
      "acc_stderr": 0.03135305009533084,
      "acc_norm": 0.7373737373737373,
      "acc_norm_stderr": 0.03135305009533084
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7772020725388601,
      "acc_stderr": 0.03003114797764154,
      "acc_norm": 0.7772020725388601,
      "acc_norm_stderr": 0.03003114797764154
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.5025641025641026,
      "acc_stderr": 0.025350672979412188,
      "acc_norm": 0.5025641025641026,
      "acc_norm_stderr": 0.025350672979412188
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085622,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085622
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5756302521008403,
      "acc_stderr": 0.032104790510157764,
      "acc_norm": 0.5756302521008403,
      "acc_norm_stderr": 0.032104790510157764
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.31125827814569534,
      "acc_stderr": 0.03780445850526733,
      "acc_norm": 0.31125827814569534,
      "acc_norm_stderr": 0.03780445850526733
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7577981651376147,
      "acc_stderr": 0.018368176306598615,
      "acc_norm": 0.7577981651376147,
      "acc_norm_stderr": 0.018368176306598615
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.375,
      "acc_stderr": 0.033016908987210894,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.033016908987210894
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.7303921568627451,
      "acc_stderr": 0.031145570659486782,
      "acc_norm": 0.7303921568627451,
      "acc_norm_stderr": 0.031145570659486782
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7510548523206751,
      "acc_stderr": 0.028146970599422644,
      "acc_norm": 0.7510548523206751,
      "acc_norm_stderr": 0.028146970599422644
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6188340807174888,
      "acc_stderr": 0.03259625118416827,
      "acc_norm": 0.6188340807174888,
      "acc_norm_stderr": 0.03259625118416827
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.648854961832061,
      "acc_stderr": 0.04186445163013751,
      "acc_norm": 0.648854961832061,
      "acc_norm_stderr": 0.04186445163013751
    },
    "hendrycksTest-international_law": {
      "acc": 0.7768595041322314,
      "acc_stderr": 0.03800754475228732,
      "acc_norm": 0.7768595041322314,
      "acc_norm_stderr": 0.03800754475228732
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.7314814814814815,
      "acc_stderr": 0.042844679680521934,
      "acc_norm": 0.7314814814814815,
      "acc_norm_stderr": 0.042844679680521934
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.656441717791411,
      "acc_stderr": 0.037311335196738925,
      "acc_norm": 0.656441717791411,
      "acc_norm_stderr": 0.037311335196738925
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.04697113923010212,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.04697113923010212
    },
    "hendrycksTest-management": {
      "acc": 0.6990291262135923,
      "acc_stderr": 0.04541609446503948,
      "acc_norm": 0.6990291262135923,
      "acc_norm_stderr": 0.04541609446503948
    },
    "hendrycksTest-marketing": {
      "acc": 0.8418803418803419,
      "acc_stderr": 0.023902325549560396,
      "acc_norm": 0.8418803418803419,
      "acc_norm_stderr": 0.023902325549560396
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.58,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7637292464878672,
      "acc_stderr": 0.01519047371703751,
      "acc_norm": 0.7637292464878672,
      "acc_norm_stderr": 0.01519047371703751
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.661849710982659,
      "acc_stderr": 0.02546977014940017,
      "acc_norm": 0.661849710982659,
      "acc_norm_stderr": 0.02546977014940017
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.28268156424581004,
      "acc_stderr": 0.01506038173001809,
      "acc_norm": 0.28268156424581004,
      "acc_norm_stderr": 0.01506038173001809
    },
    "hendrycksTest-nutrition": {
      "acc": 0.630718954248366,
      "acc_stderr": 0.027634176689602656,
      "acc_norm": 0.630718954248366,
      "acc_norm_stderr": 0.027634176689602656
    },
    "hendrycksTest-philosophy": {
      "acc": 0.662379421221865,
      "acc_stderr": 0.02685882587948854,
      "acc_norm": 0.662379421221865,
      "acc_norm_stderr": 0.02685882587948854
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6358024691358025,
      "acc_stderr": 0.02677492989972234,
      "acc_norm": 0.6358024691358025,
      "acc_norm_stderr": 0.02677492989972234
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.4148936170212766,
      "acc_stderr": 0.029392236584612503,
      "acc_norm": 0.4148936170212766,
      "acc_norm_stderr": 0.029392236584612503
    },
    "hendrycksTest-professional_law": {
      "acc": 0.4276401564537158,
      "acc_stderr": 0.012635799922765846,
      "acc_norm": 0.4276401564537158,
      "acc_norm_stderr": 0.012635799922765846
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.5661764705882353,
      "acc_stderr": 0.030105636570016633,
      "acc_norm": 0.5661764705882353,
      "acc_norm_stderr": 0.030105636570016633
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.020102583895887184,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.020102583895887184
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6454545454545455,
      "acc_stderr": 0.04582004841505417,
      "acc_norm": 0.6454545454545455,
      "acc_norm_stderr": 0.04582004841505417
    },
    "hendrycksTest-security_studies": {
      "acc": 0.7061224489795919,
      "acc_stderr": 0.029162738410249772,
      "acc_norm": 0.7061224489795919,
      "acc_norm_stderr": 0.029162738410249772
    },
    "hendrycksTest-sociology": {
      "acc": 0.8059701492537313,
      "acc_stderr": 0.027962677604768907,
      "acc_norm": 0.8059701492537313,
      "acc_norm_stderr": 0.027962677604768907
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.79,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.79,
      "acc_norm_stderr": 0.040936018074033256
    },
    "hendrycksTest-virology": {
      "acc": 0.46987951807228917,
      "acc_stderr": 0.03885425420866766,
      "acc_norm": 0.46987951807228917,
      "acc_norm_stderr": 0.03885425420866766
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7777777777777778,
      "acc_stderr": 0.03188578017686398,
      "acc_norm": 0.7777777777777778,
      "acc_norm_stderr": 0.03188578017686398
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/xverse_13b,dtype='bfloat16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:0",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "5:48:17.231700",
    "model_name": "xverse_13b"
  }
}
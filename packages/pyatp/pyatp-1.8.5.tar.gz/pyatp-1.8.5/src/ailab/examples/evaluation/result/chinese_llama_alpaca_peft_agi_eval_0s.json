{
  "results": {
    "agi_eval-aqua-rat": {
      "acc": 0.20078740157480315,
      "acc_stderr": 0.02518483615410782
    },
    "agi_eval-gaokao-biology": {
      "acc": 0.24285714285714285,
      "acc_stderr": 0.02966137041396582
    },
    "agi_eval-gaokao-chemistry": {
      "acc": 0.27053140096618356,
      "acc_stderr": 0.030951274112885212
    },
    "agi_eval-gaokao-chinese": {
      "acc": 0.2601626016260163,
      "acc_stderr": 0.028028995361669366
    },
    "agi_eval-gaokao-english": {
      "acc": 0.2973856209150327,
      "acc_stderr": 0.026173908506718576
    },
    "agi_eval-gaokao-geography": {
      "acc": 0.20100502512562815,
      "acc_stderr": 0.028480190262234934
    },
    "agi_eval-gaokao-history": {
      "acc": 0.251063829787234,
      "acc_stderr": 0.02834696377716246
    },
    "agi_eval-gaokao-mathqa": {
      "acc": 0.2564102564102564,
      "acc_stderr": 0.023339974098276813
    },
    "agi_eval-logiqa-en": {
      "acc": 0.23195084485407066,
      "acc_stderr": 0.016555252497925894
    },
    "agi_eval-logiqa-zh": {
      "acc": 0.23655913978494625,
      "acc_stderr": 0.016668667667174196
    },
    "agi_eval-lsat-ar": {
      "acc": 0.1956521739130435,
      "acc_stderr": 0.026214799709819592
    },
    "agi_eval-lsat-lr": {
      "acc": 0.17254901960784313,
      "acc_stderr": 0.016748213724077515
    },
    "agi_eval-lsat-rc": {
      "acc": 0.17843866171003717,
      "acc_stderr": 0.023388215054216312
    },
    "agi_eval-sat-en": {
      "acc": 0.3300970873786408,
      "acc_stderr": 0.03284353151466849
    },
    "agi_eval-sat-en-without-passage": {
      "acc": 0.2961165048543689,
      "acc_stderr": 0.031886346983271176
    },
    "agi_eval-sat-math": {
      "acc": 0.20909090909090908,
      "acc_stderr": 0.02747949844564045
    }
  },
  "versions": {
    "agi_eval-aqua-rat": 0,
    "agi_eval-gaokao-biology": 0,
    "agi_eval-gaokao-chemistry": 0,
    "agi_eval-gaokao-chinese": 0,
    "agi_eval-gaokao-english": 0,
    "agi_eval-gaokao-geography": 0,
    "agi_eval-gaokao-history": 0,
    "agi_eval-gaokao-mathqa": 0,
    "agi_eval-logiqa-en": 0,
    "agi_eval-logiqa-zh": 0,
    "agi_eval-lsat-ar": 0,
    "agi_eval-lsat-lr": 0,
    "agi_eval-lsat-rc": 0,
    "agi_eval-sat-en": 0,
    "agi_eval-sat-en-without-passage": 0,
    "agi_eval-sat-math": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama-7b-hf,load_in_8bit=True,dtype='float16',tokenizer=/home/sdk_token/chinese_llama_alpaca_tokenizer,use_accelerate=False,peft=/home/finetuned_models/my_chinese_llama_alpaca_model",
    "num_fewshot": 0,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:56:16.109566",
    "model_name": "chinese_llama_alpaca"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3254437869822485,
      "acc_stderr": 0.03614867847292204,
      "acc_norm": 0.3254437869822485,
      "acc_norm_stderr": 0.03614867847292204
    },
    "Cmmlu-anatomy": {
      "acc": 0.2635135135135135,
      "acc_stderr": 0.036335000433819875,
      "acc_norm": 0.2635135135135135,
      "acc_norm_stderr": 0.036335000433819875
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2804878048780488,
      "acc_stderr": 0.03518700228801578,
      "acc_norm": 0.2804878048780488,
      "acc_norm_stderr": 0.03518700228801578
    },
    "Cmmlu-arts": {
      "acc": 0.3375,
      "acc_stderr": 0.03749999999999997,
      "acc_norm": 0.3375,
      "acc_norm_stderr": 0.03749999999999997
    },
    "Cmmlu-astronomy": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.033175059300091805,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.033175059300091805
    },
    "Cmmlu-business_ethics": {
      "acc": 0.2966507177033493,
      "acc_stderr": 0.03167207801693405,
      "acc_norm": 0.2966507177033493,
      "acc_norm_stderr": 0.03167207801693405
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.2125,
      "acc_stderr": 0.03244189290245473,
      "acc_norm": 0.2125,
      "acc_norm_stderr": 0.03244189290245473
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.3282442748091603,
      "acc_stderr": 0.04118438565806298,
      "acc_norm": 0.3282442748091603,
      "acc_norm_stderr": 0.04118438565806298
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.3088235294117647,
      "acc_stderr": 0.03976333292288875,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.03976333292288875
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.3364485981308411,
      "acc_stderr": 0.04589271111471628,
      "acc_norm": 0.3364485981308411,
      "acc_norm_stderr": 0.04589271111471628
    },
    "Cmmlu-chinese_history": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.024539600216850282,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.024539600216850282
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.031980016601150726,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.031980016601150726
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.2681564245810056,
      "acc_stderr": 0.03320421630673713,
      "acc_norm": 0.2681564245810056,
      "acc_norm_stderr": 0.03320421630673713
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.25738396624472576,
      "acc_stderr": 0.028458820991460285,
      "acc_norm": 0.25738396624472576,
      "acc_norm_stderr": 0.028458820991460285
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2169811320754717,
      "acc_stderr": 0.040225592469367126,
      "acc_norm": 0.2169811320754717,
      "acc_norm_stderr": 0.040225592469367126
    },
    "Cmmlu-college_education": {
      "acc": 0.29906542056074764,
      "acc_stderr": 0.044470182376718334,
      "acc_norm": 0.29906542056074764,
      "acc_norm_stderr": 0.044470182376718334
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.22641509433962265,
      "acc_stderr": 0.040842473153370994,
      "acc_norm": 0.22641509433962265,
      "acc_norm_stderr": 0.040842473153370994
    },
    "Cmmlu-college_law": {
      "acc": 0.25,
      "acc_stderr": 0.04186091791394607,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04186091791394607
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2761904761904762,
      "acc_stderr": 0.04384295586918883,
      "acc_norm": 0.2761904761904762,
      "acc_norm_stderr": 0.04384295586918883
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.27358490566037735,
      "acc_stderr": 0.04350546818999061,
      "acc_norm": 0.27358490566037735,
      "acc_norm_stderr": 0.04350546818999061
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2893772893772894,
      "acc_stderr": 0.02749586023452527,
      "acc_norm": 0.2893772893772894,
      "acc_norm_stderr": 0.02749586023452527
    },
    "Cmmlu-computer_science": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.03149328104507955,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.03149328104507955
    },
    "Cmmlu-computer_security": {
      "acc": 0.25146198830409355,
      "acc_stderr": 0.033275044238468436,
      "acc_norm": 0.25146198830409355,
      "acc_norm_stderr": 0.033275044238468436
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.29931972789115646,
      "acc_stderr": 0.0379010453091039,
      "acc_norm": 0.29931972789115646,
      "acc_norm_stderr": 0.0379010453091039
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2805755395683453,
      "acc_stderr": 0.03824529014900685,
      "acc_norm": 0.2805755395683453,
      "acc_norm_stderr": 0.03824529014900685
    },
    "Cmmlu-economics": {
      "acc": 0.27672955974842767,
      "acc_stderr": 0.03559177035707935,
      "acc_norm": 0.27672955974842767,
      "acc_norm_stderr": 0.03559177035707935
    },
    "Cmmlu-education": {
      "acc": 0.31901840490797545,
      "acc_stderr": 0.03661997551073836,
      "acc_norm": 0.31901840490797545,
      "acc_norm_stderr": 0.03661997551073836
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.2616279069767442,
      "acc_stderr": 0.03361101403890494,
      "acc_norm": 0.2616279069767442,
      "acc_norm_stderr": 0.03361101403890494
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.02775179241879093,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.02775179241879093
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.30808080808080807,
      "acc_stderr": 0.03289477330098617,
      "acc_norm": 0.30808080808080807,
      "acc_norm_stderr": 0.03289477330098617
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.02934457250063435,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.02934457250063435
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.25217391304347825,
      "acc_stderr": 0.028696745294493353,
      "acc_norm": 0.25217391304347825,
      "acc_norm_stderr": 0.028696745294493353
    },
    "Cmmlu-ethnology": {
      "acc": 0.24444444444444444,
      "acc_stderr": 0.037125378336148665,
      "acc_norm": 0.24444444444444444,
      "acc_norm_stderr": 0.037125378336148665
    },
    "Cmmlu-food_science": {
      "acc": 0.32167832167832167,
      "acc_stderr": 0.03919986517659164,
      "acc_norm": 0.32167832167832167,
      "acc_norm_stderr": 0.03919986517659164
    },
    "Cmmlu-genetics": {
      "acc": 0.29545454545454547,
      "acc_stderr": 0.03448901746724544,
      "acc_norm": 0.29545454545454547,
      "acc_norm_stderr": 0.03448901746724544
    },
    "Cmmlu-global_facts": {
      "acc": 0.2483221476510067,
      "acc_stderr": 0.035513440416974316,
      "acc_norm": 0.2483221476510067,
      "acc_norm_stderr": 0.035513440416974316
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.23076923076923078,
      "acc_stderr": 0.03250593287417369,
      "acc_norm": 0.23076923076923078,
      "acc_norm_stderr": 0.03250593287417369
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.036614333604107194,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.036614333604107194
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.22033898305084745,
      "acc_stderr": 0.0383182484922332,
      "acc_norm": 0.22033898305084745,
      "acc_norm_stderr": 0.0383182484922332
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364997,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364997
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2636363636363636,
      "acc_stderr": 0.04220224692971987,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.2517482517482518,
      "acc_stderr": 0.036421927837417045,
      "acc_norm": 0.2517482517482518,
      "acc_norm_stderr": 0.036421927837417045
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.04104947269903394,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.04104947269903394
    },
    "Cmmlu-international_law": {
      "acc": 0.2972972972972973,
      "acc_stderr": 0.03369553691877718,
      "acc_norm": 0.2972972972972973,
      "acc_norm_stderr": 0.03369553691877718
    },
    "Cmmlu-journalism": {
      "acc": 0.28488372093023256,
      "acc_stderr": 0.0345162887625062,
      "acc_norm": 0.28488372093023256,
      "acc_norm_stderr": 0.0345162887625062
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.2725060827250608,
      "acc_stderr": 0.021989272196105025,
      "acc_norm": 0.2725060827250608,
      "acc_norm_stderr": 0.021989272196105025
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.3598130841121495,
      "acc_stderr": 0.03288531991318828,
      "acc_norm": 0.3598130841121495,
      "acc_norm_stderr": 0.03288531991318828
    },
    "Cmmlu-logical": {
      "acc": 0.34146341463414637,
      "acc_stderr": 0.042932099563790314,
      "acc_norm": 0.34146341463414637,
      "acc_norm_stderr": 0.042932099563790314
    },
    "Cmmlu-machine_learning": {
      "acc": 0.32786885245901637,
      "acc_stderr": 0.04267606874299955,
      "acc_norm": 0.32786885245901637,
      "acc_norm_stderr": 0.04267606874299955
    },
    "Cmmlu-management": {
      "acc": 0.28095238095238095,
      "acc_stderr": 0.031090094469344617,
      "acc_norm": 0.28095238095238095,
      "acc_norm_stderr": 0.031090094469344617
    },
    "Cmmlu-marketing": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.03387720998298804,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.03387720998298804
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.3386243386243386,
      "acc_stderr": 0.03451471285997054,
      "acc_norm": 0.3386243386243386,
      "acc_norm_stderr": 0.03451471285997054
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.2413793103448276,
      "acc_stderr": 0.0399037253226882,
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.0399037253226882
    },
    "Cmmlu-nutrition": {
      "acc": 0.2482758620689655,
      "acc_stderr": 0.03600105692727772,
      "acc_norm": 0.2482758620689655,
      "acc_norm_stderr": 0.03600105692727772
    },
    "Cmmlu-philosophy": {
      "acc": 0.34285714285714286,
      "acc_stderr": 0.04654465622977448,
      "acc_norm": 0.34285714285714286,
      "acc_norm_stderr": 0.04654465622977448
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.03424737867752742,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.03424737867752742
    },
    "Cmmlu-professional_law": {
      "acc": 0.27488151658767773,
      "acc_stderr": 0.030808291124780333,
      "acc_norm": 0.27488151658767773,
      "acc_norm_stderr": 0.030808291124780333
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.023099237430720333,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.023099237430720333
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.34051724137931033,
      "acc_stderr": 0.03117922285925479,
      "acc_norm": 0.34051724137931033,
      "acc_norm_stderr": 0.03117922285925479
    },
    "Cmmlu-public_relations": {
      "acc": 0.26436781609195403,
      "acc_stderr": 0.03352830517660786,
      "acc_norm": 0.26436781609195403,
      "acc_norm_stderr": 0.03352830517660786
    },
    "Cmmlu-security_study": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.03915450630414251,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.03915450630414251
    },
    "Cmmlu-sociology": {
      "acc": 0.26548672566371684,
      "acc_stderr": 0.02943946890825876,
      "acc_norm": 0.26548672566371684,
      "acc_norm_stderr": 0.02943946890825876
    },
    "Cmmlu-sports_science": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.035886248000917095,
      "acc_norm": 0.30303030303030304,
      "acc_norm_stderr": 0.035886248000917095
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2702702702702703,
      "acc_stderr": 0.03273943999002354,
      "acc_norm": 0.2702702702702703,
      "acc_norm_stderr": 0.03273943999002354
    },
    "Cmmlu-virology": {
      "acc": 0.28994082840236685,
      "acc_stderr": 0.03500638924911012,
      "acc_norm": 0.28994082840236685,
      "acc_norm_stderr": 0.03500638924911012
    },
    "Cmmlu-world_history": {
      "acc": 0.2484472049689441,
      "acc_stderr": 0.03416149068322981,
      "acc_norm": 0.2484472049689441,
      "acc_norm_stderr": 0.03416149068322981
    },
    "Cmmlu-world_religions": {
      "acc": 0.28125,
      "acc_stderr": 0.03565632932250201,
      "acc_norm": 0.28125,
      "acc_norm_stderr": 0.03565632932250201
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/moss-moon-003-base,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:18:33.586350",
    "model_name": "moss_moon_003_base"
  }
}
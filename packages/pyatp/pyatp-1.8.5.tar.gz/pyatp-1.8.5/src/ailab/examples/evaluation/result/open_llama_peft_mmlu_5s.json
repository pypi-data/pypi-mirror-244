{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.31,
      "acc_stderr": 0.046482319871173156,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.046482319871173156
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.039725528847851375
    },
    "hendrycksTest-astronomy": {
      "acc": 0.2565789473684211,
      "acc_stderr": 0.0355418036802569,
      "acc_norm": 0.2565789473684211,
      "acc_norm_stderr": 0.0355418036802569
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.26,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.32075471698113206,
      "acc_stderr": 0.028727502957880263,
      "acc_norm": 0.32075471698113206,
      "acc_norm_stderr": 0.028727502957880263
    },
    "hendrycksTest-college_biology": {
      "acc": 0.25,
      "acc_stderr": 0.03621034121889507,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03621034121889507
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909283,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909283
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2947976878612717,
      "acc_stderr": 0.034765996075164785,
      "acc_norm": 0.2947976878612717,
      "acc_norm_stderr": 0.034765996075164785
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237655,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237655
    },
    "hendrycksTest-computer_security": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252605,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.30638297872340425,
      "acc_stderr": 0.03013590647851756,
      "acc_norm": 0.30638297872340425,
      "acc_norm_stderr": 0.03013590647851756
    },
    "hendrycksTest-econometrics": {
      "acc": 0.24561403508771928,
      "acc_stderr": 0.04049339297748142,
      "acc_norm": 0.24561403508771928,
      "acc_norm_stderr": 0.04049339297748142
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.30344827586206896,
      "acc_stderr": 0.038312260488503336,
      "acc_norm": 0.30344827586206896,
      "acc_norm_stderr": 0.038312260488503336
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2751322751322751,
      "acc_stderr": 0.02300008685906865,
      "acc_norm": 0.2751322751322751,
      "acc_norm_stderr": 0.02300008685906865
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.20634920634920634,
      "acc_stderr": 0.03619604524124252,
      "acc_norm": 0.20634920634920634,
      "acc_norm_stderr": 0.03619604524124252
    },
    "hendrycksTest-global_facts": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.2903225806451613,
      "acc_stderr": 0.02582210611941589,
      "acc_norm": 0.2903225806451613,
      "acc_norm_stderr": 0.02582210611941589
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.28078817733990147,
      "acc_stderr": 0.0316185633535861,
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.0316185633535861
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.03546563019624337,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.03546563019624337
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.03427308652999934,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03427308652999934
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.38341968911917096,
      "acc_stderr": 0.03508984236295341,
      "acc_norm": 0.38341968911917096,
      "acc_norm_stderr": 0.03508984236295341
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.36153846153846153,
      "acc_stderr": 0.024359581465396987,
      "acc_norm": 0.36153846153846153,
      "acc_norm_stderr": 0.024359581465396987
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.026962424325073828,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.026962424325073828
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.3277310924369748,
      "acc_stderr": 0.03048991141767323,
      "acc_norm": 0.3277310924369748,
      "acc_norm_stderr": 0.03048991141767323
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.31788079470198677,
      "acc_stderr": 0.038020397601079024,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.038020397601079024
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.30642201834862387,
      "acc_stderr": 0.019765517220458523,
      "acc_norm": 0.30642201834862387,
      "acc_norm_stderr": 0.019765517220458523
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.44907407407407407,
      "acc_stderr": 0.03392238405321617,
      "acc_norm": 0.44907407407407407,
      "acc_norm_stderr": 0.03392238405321617
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.30392156862745096,
      "acc_stderr": 0.032282103870378935,
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.032282103870378935
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.2742616033755274,
      "acc_stderr": 0.02904133351059802,
      "acc_norm": 0.2742616033755274,
      "acc_norm_stderr": 0.02904133351059802
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2645739910313901,
      "acc_stderr": 0.02960510321703834,
      "acc_norm": 0.2645739910313901,
      "acc_norm_stderr": 0.02960510321703834
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.2824427480916031,
      "acc_stderr": 0.03948406125768362,
      "acc_norm": 0.2824427480916031,
      "acc_norm_stderr": 0.03948406125768362
    },
    "hendrycksTest-international_law": {
      "acc": 0.4132231404958678,
      "acc_stderr": 0.04495087843548408,
      "acc_norm": 0.4132231404958678,
      "acc_norm_stderr": 0.04495087843548408
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.043300437496507416,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.043300437496507416
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.27607361963190186,
      "acc_stderr": 0.03512385283705051,
      "acc_norm": 0.27607361963190186,
      "acc_norm_stderr": 0.03512385283705051
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.20535714285714285,
      "acc_stderr": 0.03834241021419074,
      "acc_norm": 0.20535714285714285,
      "acc_norm_stderr": 0.03834241021419074
    },
    "hendrycksTest-management": {
      "acc": 0.2621359223300971,
      "acc_stderr": 0.04354631077260595,
      "acc_norm": 0.2621359223300971,
      "acc_norm_stderr": 0.04354631077260595
    },
    "hendrycksTest-marketing": {
      "acc": 0.33760683760683763,
      "acc_stderr": 0.030980296992618558,
      "acc_norm": 0.33760683760683763,
      "acc_norm_stderr": 0.030980296992618558
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384741,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384741
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.31800766283524906,
      "acc_stderr": 0.01665348627561539,
      "acc_norm": 0.31800766283524906,
      "acc_norm_stderr": 0.01665348627561539
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.30346820809248554,
      "acc_stderr": 0.024752411960917212,
      "acc_norm": 0.30346820809248554,
      "acc_norm_stderr": 0.024752411960917212
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808835
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2973856209150327,
      "acc_stderr": 0.02617390850671858,
      "acc_norm": 0.2973856209150327,
      "acc_norm_stderr": 0.02617390850671858
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3504823151125402,
      "acc_stderr": 0.02709865262130175,
      "acc_norm": 0.3504823151125402,
      "acc_norm_stderr": 0.02709865262130175
    },
    "hendrycksTest-prehistory": {
      "acc": 0.30246913580246915,
      "acc_stderr": 0.025557653981868055,
      "acc_norm": 0.30246913580246915,
      "acc_norm_stderr": 0.025557653981868055
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2730496453900709,
      "acc_stderr": 0.026577860943307854,
      "acc_norm": 0.2730496453900709,
      "acc_norm_stderr": 0.026577860943307854
    },
    "hendrycksTest-professional_law": {
      "acc": 0.27444589308996087,
      "acc_stderr": 0.011397043163078154,
      "acc_norm": 0.27444589308996087,
      "acc_norm_stderr": 0.011397043163078154
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.029520095697687758,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.029520095697687758
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2581699346405229,
      "acc_stderr": 0.017704531653250075,
      "acc_norm": 0.2581699346405229,
      "acc_norm_stderr": 0.017704531653250075
    },
    "hendrycksTest-public_relations": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.041723430387053825,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.041723430387053825
    },
    "hendrycksTest-security_studies": {
      "acc": 0.39591836734693875,
      "acc_stderr": 0.03130802899065686,
      "acc_norm": 0.39591836734693875,
      "acc_norm_stderr": 0.03130802899065686
    },
    "hendrycksTest-sociology": {
      "acc": 0.373134328358209,
      "acc_stderr": 0.034198326081760065,
      "acc_norm": 0.373134328358209,
      "acc_norm_stderr": 0.034198326081760065
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-virology": {
      "acc": 0.3674698795180723,
      "acc_stderr": 0.03753267402120575,
      "acc_norm": 0.3674698795180723,
      "acc_norm_stderr": 0.03753267402120575
    },
    "hendrycksTest-world_religions": {
      "acc": 0.30409356725146197,
      "acc_stderr": 0.03528211258245232,
      "acc_norm": 0.30409356725146197,
      "acc_norm_stderr": 0.03528211258245232
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/open_llama_7b',trust_remote_code=True,use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_open_llama_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:57:28.112817",
    "model_name": "open_llama"
  }
}
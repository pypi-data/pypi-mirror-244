{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.47928994082840237,
      "acc_stderr": 0.03854273242663733,
      "acc_norm": 0.47928994082840237,
      "acc_norm_stderr": 0.03854273242663733
    },
    "Cmmlu-anatomy": {
      "acc": 0.40540540540540543,
      "acc_stderr": 0.040494550122399664,
      "acc_norm": 0.40540540540540543,
      "acc_norm_stderr": 0.040494550122399664
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.32926829268292684,
      "acc_stderr": 0.03680913164548253,
      "acc_norm": 0.32926829268292684,
      "acc_norm_stderr": 0.03680913164548253
    },
    "Cmmlu-arts": {
      "acc": 0.76875,
      "acc_stderr": 0.03343758265727744,
      "acc_norm": 0.76875,
      "acc_norm_stderr": 0.03343758265727744
    },
    "Cmmlu-astronomy": {
      "acc": 0.3696969696969697,
      "acc_stderr": 0.03769430314512567,
      "acc_norm": 0.3696969696969697,
      "acc_norm_stderr": 0.03769430314512567
    },
    "Cmmlu-business_ethics": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.03452520569603411,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.03452520569603411
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.5,
      "acc_stderr": 0.03965257928590721,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.03965257928590721
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.7099236641221374,
      "acc_stderr": 0.03980066246467766,
      "acc_norm": 0.7099236641221374,
      "acc_norm_stderr": 0.03980066246467766
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.5514705882352942,
      "acc_stderr": 0.04280453234454161,
      "acc_norm": 0.5514705882352942,
      "acc_norm_stderr": 0.04280453234454161
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5233644859813084,
      "acc_stderr": 0.048511241723296745,
      "acc_norm": 0.5233644859813084,
      "acc_norm_stderr": 0.048511241723296745
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6037151702786377,
      "acc_stderr": 0.027257864752235458,
      "acc_norm": 0.6037151702786377,
      "acc_norm_stderr": 0.027257864752235458
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.46568627450980393,
      "acc_stderr": 0.035010383276358976,
      "acc_norm": 0.46568627450980393,
      "acc_norm_stderr": 0.035010383276358976
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.664804469273743,
      "acc_stderr": 0.035382301081428424,
      "acc_norm": 0.664804469273743,
      "acc_norm_stderr": 0.035382301081428424
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.510548523206751,
      "acc_stderr": 0.032539983791662855,
      "acc_norm": 0.510548523206751,
      "acc_norm_stderr": 0.032539983791662855
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.3113207547169811,
      "acc_stderr": 0.0451874553177075,
      "acc_norm": 0.3113207547169811,
      "acc_norm_stderr": 0.0451874553177075
    },
    "Cmmlu-college_education": {
      "acc": 0.6542056074766355,
      "acc_stderr": 0.04619693596622578,
      "acc_norm": 0.6542056074766355,
      "acc_norm_stderr": 0.04619693596622578
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.44339622641509435,
      "acc_stderr": 0.04848131822975479,
      "acc_norm": 0.44339622641509435,
      "acc_norm_stderr": 0.04848131822975479
    },
    "Cmmlu-college_law": {
      "acc": 0.4722222222222222,
      "acc_stderr": 0.048262172941398944,
      "acc_norm": 0.4722222222222222,
      "acc_norm_stderr": 0.048262172941398944
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3142857142857143,
      "acc_stderr": 0.045521571818039494,
      "acc_norm": 0.3142857142857143,
      "acc_norm_stderr": 0.045521571818039494
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.047732492983673595,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.047732492983673595
    },
    "Cmmlu-college_medicine": {
      "acc": 0.5274725274725275,
      "acc_stderr": 0.030271155718081682,
      "acc_norm": 0.5274725274725275,
      "acc_norm_stderr": 0.030271155718081682
    },
    "Cmmlu-computer_science": {
      "acc": 0.5343137254901961,
      "acc_stderr": 0.03501038327635897,
      "acc_norm": 0.5343137254901961,
      "acc_norm_stderr": 0.03501038327635897
    },
    "Cmmlu-computer_security": {
      "acc": 0.5906432748538012,
      "acc_stderr": 0.037712831076265434,
      "acc_norm": 0.5906432748538012,
      "acc_norm_stderr": 0.037712831076265434
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.47619047619047616,
      "acc_stderr": 0.04133335136970882,
      "acc_norm": 0.47619047619047616,
      "acc_norm_stderr": 0.04133335136970882
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.43884892086330934,
      "acc_stderr": 0.042243303690285554,
      "acc_norm": 0.43884892086330934,
      "acc_norm_stderr": 0.042243303690285554
    },
    "Cmmlu-economics": {
      "acc": 0.5345911949685535,
      "acc_stderr": 0.039682557564722526,
      "acc_norm": 0.5345911949685535,
      "acc_norm_stderr": 0.039682557564722526
    },
    "Cmmlu-education": {
      "acc": 0.588957055214724,
      "acc_stderr": 0.038656978537853624,
      "acc_norm": 0.588957055214724,
      "acc_norm_stderr": 0.038656978537853624
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.5465116279069767,
      "acc_stderr": 0.03807016210250966,
      "acc_norm": 0.5465116279069767,
      "acc_norm_stderr": 0.03807016210250966
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.5,
      "acc_stderr": 0.031559720154890156,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.031559720154890156
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.5050505050505051,
      "acc_stderr": 0.035621707606254015,
      "acc_norm": 0.5050505050505051,
      "acc_norm_stderr": 0.035621707606254015
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.7142857142857143,
      "acc_stderr": 0.029344572500634335,
      "acc_norm": 0.7142857142857143,
      "acc_norm_stderr": 0.029344572500634335
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.2956521739130435,
      "acc_stderr": 0.030155489768916178,
      "acc_norm": 0.2956521739130435,
      "acc_norm_stderr": 0.030155489768916178
    },
    "Cmmlu-ethnology": {
      "acc": 0.5037037037037037,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.5037037037037037,
      "acc_norm_stderr": 0.04319223625811331
    },
    "Cmmlu-food_science": {
      "acc": 0.5664335664335665,
      "acc_stderr": 0.04158705287172622,
      "acc_norm": 0.5664335664335665,
      "acc_norm_stderr": 0.04158705287172622
    },
    "Cmmlu-genetics": {
      "acc": 0.3977272727272727,
      "acc_stderr": 0.03699731953659028,
      "acc_norm": 0.3977272727272727,
      "acc_norm_stderr": 0.03699731953659028
    },
    "Cmmlu-global_facts": {
      "acc": 0.5234899328859061,
      "acc_stderr": 0.04105436598675506,
      "acc_norm": 0.5234899328859061,
      "acc_norm_stderr": 0.04105436598675506
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.4437869822485207,
      "acc_stderr": 0.038331270387187345,
      "acc_norm": 0.4437869822485207,
      "acc_norm_stderr": 0.038331270387187345
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.3787878787878788,
      "acc_stderr": 0.042382088042043194,
      "acc_norm": 0.3787878787878788,
      "acc_norm_stderr": 0.042382088042043194
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.576271186440678,
      "acc_stderr": 0.04568404181144862,
      "acc_norm": 0.576271186440678,
      "acc_norm_stderr": 0.04568404181144862
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2865853658536585,
      "acc_stderr": 0.03541638332993504,
      "acc_norm": 0.2865853658536585,
      "acc_norm_stderr": 0.03541638332993504
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.04554619617541054,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.04554619617541054
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5384615384615384,
      "acc_stderr": 0.041834744477373405,
      "acc_norm": 0.5384615384615384,
      "acc_norm_stderr": 0.041834744477373405
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.5476190476190477,
      "acc_stderr": 0.044518079590553275,
      "acc_norm": 0.5476190476190477,
      "acc_norm_stderr": 0.044518079590553275
    },
    "Cmmlu-international_law": {
      "acc": 0.5027027027027027,
      "acc_stderr": 0.03685995053239417,
      "acc_norm": 0.5027027027027027,
      "acc_norm_stderr": 0.03685995053239417
    },
    "Cmmlu-journalism": {
      "acc": 0.5930232558139535,
      "acc_stderr": 0.03756839173779933,
      "acc_norm": 0.5930232558139535,
      "acc_norm_stderr": 0.03756839173779933
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5401459854014599,
      "acc_stderr": 0.02461351501919943,
      "acc_norm": 0.5401459854014599,
      "acc_norm_stderr": 0.02461351501919943
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8738317757009346,
      "acc_stderr": 0.0227509276988054,
      "acc_norm": 0.8738317757009346,
      "acc_norm_stderr": 0.0227509276988054
    },
    "Cmmlu-logical": {
      "acc": 0.4634146341463415,
      "acc_stderr": 0.0451465292863279,
      "acc_norm": 0.4634146341463415,
      "acc_norm_stderr": 0.0451465292863279
    },
    "Cmmlu-machine_learning": {
      "acc": 0.39344262295081966,
      "acc_stderr": 0.04441032615723205,
      "acc_norm": 0.39344262295081966,
      "acc_norm_stderr": 0.04441032615723205
    },
    "Cmmlu-management": {
      "acc": 0.6523809523809524,
      "acc_stderr": 0.032940430891650836,
      "acc_norm": 0.6523809523809524,
      "acc_norm_stderr": 0.032940430891650836
    },
    "Cmmlu-marketing": {
      "acc": 0.5833333333333334,
      "acc_stderr": 0.036849047011740986,
      "acc_norm": 0.5833333333333334,
      "acc_norm_stderr": 0.036849047011740986
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.656084656084656,
      "acc_stderr": 0.034643901257432906,
      "acc_norm": 0.656084656084656,
      "acc_norm_stderr": 0.034643901257432906
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.39655172413793105,
      "acc_stderr": 0.04561640191490673,
      "acc_norm": 0.39655172413793105,
      "acc_norm_stderr": 0.04561640191490673
    },
    "Cmmlu-nutrition": {
      "acc": 0.5241379310344828,
      "acc_stderr": 0.0416180850350153,
      "acc_norm": 0.5241379310344828,
      "acc_norm_stderr": 0.0416180850350153
    },
    "Cmmlu-philosophy": {
      "acc": 0.580952380952381,
      "acc_stderr": 0.04838216375282531,
      "acc_norm": 0.580952380952381,
      "acc_norm_stderr": 0.04838216375282531
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.6514285714285715,
      "acc_stderr": 0.036124735035030504,
      "acc_norm": 0.6514285714285715,
      "acc_norm_stderr": 0.036124735035030504
    },
    "Cmmlu-professional_law": {
      "acc": 0.46919431279620855,
      "acc_stderr": 0.03443772914613259,
      "acc_norm": 0.46919431279620855,
      "acc_norm_stderr": 0.03443772914613259
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.42819148936170215,
      "acc_stderr": 0.025552223010919438,
      "acc_norm": 0.42819148936170215,
      "acc_norm_stderr": 0.025552223010919438
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5905172413793104,
      "acc_stderr": 0.032354009701728824,
      "acc_norm": 0.5905172413793104,
      "acc_norm_stderr": 0.032354009701728824
    },
    "Cmmlu-public_relations": {
      "acc": 0.5517241379310345,
      "acc_stderr": 0.037810343077949025,
      "acc_norm": 0.5517241379310345,
      "acc_norm_stderr": 0.037810343077949025
    },
    "Cmmlu-security_study": {
      "acc": 0.6148148148148148,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.6148148148148148,
      "acc_norm_stderr": 0.042039210401562783
    },
    "Cmmlu-sociology": {
      "acc": 0.5575221238938053,
      "acc_stderr": 0.033112012272335754,
      "acc_norm": 0.5575221238938053,
      "acc_norm_stderr": 0.033112012272335754
    },
    "Cmmlu-sports_science": {
      "acc": 0.5696969696969697,
      "acc_stderr": 0.038662259628790774,
      "acc_norm": 0.5696969696969697,
      "acc_norm_stderr": 0.038662259628790774
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.6108108108108108,
      "acc_stderr": 0.03594386960243731,
      "acc_norm": 0.6108108108108108,
      "acc_norm_stderr": 0.03594386960243731
    },
    "Cmmlu-virology": {
      "acc": 0.591715976331361,
      "acc_stderr": 0.03792129848885541,
      "acc_norm": 0.591715976331361,
      "acc_norm_stderr": 0.03792129848885541
    },
    "Cmmlu-world_history": {
      "acc": 0.5527950310559007,
      "acc_stderr": 0.0393074964777559,
      "acc_norm": 0.5527950310559007,
      "acc_norm_stderr": 0.0393074964777559
    },
    "Cmmlu-world_religions": {
      "acc": 0.68125,
      "acc_stderr": 0.036955560385363254,
      "acc_norm": 0.68125,
      "acc_norm_stderr": 0.036955560385363254
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/xverse_13b,dtype='bfloat16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_xverse_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:45:16.324467",
    "model_name": "xverse_13b"
  }
}
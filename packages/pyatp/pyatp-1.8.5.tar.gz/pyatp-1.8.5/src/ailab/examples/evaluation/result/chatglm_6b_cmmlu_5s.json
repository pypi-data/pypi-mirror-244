{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3905325443786982,
      "acc_stderr": 0.03763996705629265,
      "acc_norm": 0.3905325443786982,
      "acc_norm_stderr": 0.03763996705629265
    },
    "Cmmlu-anatomy": {
      "acc": 0.25675675675675674,
      "acc_stderr": 0.036030290036472144,
      "acc_norm": 0.25675675675675674,
      "acc_norm_stderr": 0.036030290036472144
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.29878048780487804,
      "acc_stderr": 0.03585166336909661,
      "acc_norm": 0.29878048780487804,
      "acc_norm_stderr": 0.03585166336909661
    },
    "Cmmlu-arts": {
      "acc": 0.39375,
      "acc_stderr": 0.038746956666858325,
      "acc_norm": 0.39375,
      "acc_norm_stderr": 0.038746956666858325
    },
    "Cmmlu-astronomy": {
      "acc": 0.21818181818181817,
      "acc_stderr": 0.03225078108306289,
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03225078108306289
    },
    "Cmmlu-business_ethics": {
      "acc": 0.37799043062200954,
      "acc_stderr": 0.03362074000611289,
      "acc_norm": 0.37799043062200954,
      "acc_norm_stderr": 0.03362074000611289
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.36875,
      "acc_stderr": 0.03826204233503227,
      "acc_norm": 0.36875,
      "acc_norm_stderr": 0.03826204233503227
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.45038167938931295,
      "acc_stderr": 0.04363643698524779,
      "acc_norm": 0.45038167938931295,
      "acc_norm_stderr": 0.04363643698524779
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.3602941176470588,
      "acc_stderr": 0.041319197084091215,
      "acc_norm": 0.3602941176470588,
      "acc_norm_stderr": 0.041319197084091215
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.3925233644859813,
      "acc_stderr": 0.04742907046004222,
      "acc_norm": 0.3925233644859813,
      "acc_norm_stderr": 0.04742907046004222
    },
    "Cmmlu-chinese_history": {
      "acc": 0.4179566563467492,
      "acc_stderr": 0.02748624103922622,
      "acc_norm": 0.4179566563467492,
      "acc_norm_stderr": 0.02748624103922622
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.28921568627450983,
      "acc_stderr": 0.031822318676475524,
      "acc_norm": 0.28921568627450983,
      "acc_norm_stderr": 0.031822318676475524
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.40782122905027934,
      "acc_stderr": 0.03683420752157822,
      "acc_norm": 0.40782122905027934,
      "acc_norm_stderr": 0.03683420752157822
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.25738396624472576,
      "acc_stderr": 0.0284588209914603,
      "acc_norm": 0.25738396624472576,
      "acc_norm_stderr": 0.0284588209914603
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.27358490566037735,
      "acc_stderr": 0.0435054681899906,
      "acc_norm": 0.27358490566037735,
      "acc_norm_stderr": 0.0435054681899906
    },
    "Cmmlu-college_education": {
      "acc": 0.42990654205607476,
      "acc_stderr": 0.048084723494299535,
      "acc_norm": 0.42990654205607476,
      "acc_norm_stderr": 0.048084723494299535
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.044801270921106716,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.044801270921106716
    },
    "Cmmlu-college_law": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.04489931073591312,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.04489931073591312
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.044298119496145844,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.044298119496145844
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.29245283018867924,
      "acc_stderr": 0.04439263906199628,
      "acc_norm": 0.29245283018867924,
      "acc_norm_stderr": 0.04439263906199628
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2893772893772894,
      "acc_stderr": 0.02749586023452527,
      "acc_norm": 0.2893772893772894,
      "acc_norm_stderr": 0.02749586023452527
    },
    "Cmmlu-computer_science": {
      "acc": 0.4068627450980392,
      "acc_stderr": 0.03447891136353382,
      "acc_norm": 0.4068627450980392,
      "acc_norm_stderr": 0.03447891136353382
    },
    "Cmmlu-computer_security": {
      "acc": 0.3391812865497076,
      "acc_stderr": 0.03631053496488905,
      "acc_norm": 0.3391812865497076,
      "acc_norm_stderr": 0.03631053496488905
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2925170068027211,
      "acc_stderr": 0.03764931984085171,
      "acc_norm": 0.2925170068027211,
      "acc_norm_stderr": 0.03764931984085171
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2302158273381295,
      "acc_stderr": 0.03583542294357026,
      "acc_norm": 0.2302158273381295,
      "acc_norm_stderr": 0.03583542294357026
    },
    "Cmmlu-economics": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.03856271073542805,
      "acc_norm": 0.37735849056603776,
      "acc_norm_stderr": 0.03856271073542805
    },
    "Cmmlu-education": {
      "acc": 0.3987730061349693,
      "acc_stderr": 0.03847021420456023,
      "acc_norm": 0.3987730061349693,
      "acc_norm_stderr": 0.03847021420456023
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3081395348837209,
      "acc_stderr": 0.03530895898152282,
      "acc_norm": 0.3081395348837209,
      "acc_norm_stderr": 0.03530895898152282
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.31746031746031744,
      "acc_stderr": 0.02938135465203212,
      "acc_norm": 0.31746031746031744,
      "acc_norm_stderr": 0.02938135465203212
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03358618145732524,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03358618145732524
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.42436974789915966,
      "acc_stderr": 0.03210479051015776,
      "acc_norm": 0.42436974789915966,
      "acc_norm_stderr": 0.03210479051015776
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.26956521739130435,
      "acc_stderr": 0.029322764228949527,
      "acc_norm": 0.26956521739130435,
      "acc_norm_stderr": 0.029322764228949527
    },
    "Cmmlu-ethnology": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.03972552884785139,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.03972552884785139
    },
    "Cmmlu-food_science": {
      "acc": 0.38461538461538464,
      "acc_stderr": 0.04082653018725189,
      "acc_norm": 0.38461538461538464,
      "acc_norm_stderr": 0.04082653018725189
    },
    "Cmmlu-genetics": {
      "acc": 0.3068181818181818,
      "acc_stderr": 0.03486142240553238,
      "acc_norm": 0.3068181818181818,
      "acc_norm_stderr": 0.03486142240553238
    },
    "Cmmlu-global_facts": {
      "acc": 0.3221476510067114,
      "acc_stderr": 0.038411757592369186,
      "acc_norm": 0.3221476510067114,
      "acc_norm_stderr": 0.038411757592369186
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.2958579881656805,
      "acc_stderr": 0.035214144124964784,
      "acc_norm": 0.2958579881656805,
      "acc_norm_stderr": 0.035214144124964784
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.26515151515151514,
      "acc_stderr": 0.03856650735812558,
      "acc_norm": 0.26515151515151514,
      "acc_norm_stderr": 0.03856650735812558
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3644067796610169,
      "acc_stderr": 0.0444928188384252,
      "acc_norm": 0.3644067796610169,
      "acc_norm_stderr": 0.0444928188384252
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.035637888362588285,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.035637888362588285
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.04013964554072774,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.04013964554072774
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.3776223776223776,
      "acc_stderr": 0.040682878492098076,
      "acc_norm": 0.3776223776223776,
      "acc_norm_stderr": 0.040682878492098076
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.31746031746031744,
      "acc_stderr": 0.04163453031302859,
      "acc_norm": 0.31746031746031744,
      "acc_norm_stderr": 0.04163453031302859
    },
    "Cmmlu-international_law": {
      "acc": 0.2864864864864865,
      "acc_stderr": 0.03333068663336699,
      "acc_norm": 0.2864864864864865,
      "acc_norm_stderr": 0.03333068663336699
    },
    "Cmmlu-journalism": {
      "acc": 0.37209302325581395,
      "acc_stderr": 0.036963693685536064,
      "acc_norm": 0.37209302325581395,
      "acc_norm_stderr": 0.036963693685536064
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.3381995133819951,
      "acc_stderr": 0.02336458663469535,
      "acc_norm": 0.3381995133819951,
      "acc_norm_stderr": 0.02336458663469535
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.5514018691588785,
      "acc_stderr": 0.03407791733638944,
      "acc_norm": 0.5514018691588785,
      "acc_norm_stderr": 0.03407791733638944
    },
    "Cmmlu-logical": {
      "acc": 0.3902439024390244,
      "acc_stderr": 0.04416377855732609,
      "acc_norm": 0.3902439024390244,
      "acc_norm_stderr": 0.04416377855732609
    },
    "Cmmlu-machine_learning": {
      "acc": 0.28688524590163933,
      "acc_stderr": 0.04111886635267182,
      "acc_norm": 0.28688524590163933,
      "acc_norm_stderr": 0.04111886635267182
    },
    "Cmmlu-management": {
      "acc": 0.3619047619047619,
      "acc_stderr": 0.033240439515935034,
      "acc_norm": 0.3619047619047619,
      "acc_norm_stderr": 0.033240439515935034
    },
    "Cmmlu-marketing": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.036849047011741,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.036849047011741
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4656084656084656,
      "acc_stderr": 0.036379882677500544,
      "acc_norm": 0.4656084656084656,
      "acc_norm_stderr": 0.036379882677500544
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3448275862068966,
      "acc_stderr": 0.044323074959803505,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.044323074959803505
    },
    "Cmmlu-nutrition": {
      "acc": 0.3586206896551724,
      "acc_stderr": 0.039966295748767186,
      "acc_norm": 0.3586206896551724,
      "acc_norm_stderr": 0.039966295748767186
    },
    "Cmmlu-philosophy": {
      "acc": 0.3904761904761905,
      "acc_stderr": 0.047838322981141455,
      "acc_norm": 0.3904761904761905,
      "acc_norm_stderr": 0.047838322981141455
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.35428571428571426,
      "acc_stderr": 0.036259548638451475,
      "acc_norm": 0.35428571428571426,
      "acc_norm_stderr": 0.036259548638451475
    },
    "Cmmlu-professional_law": {
      "acc": 0.3127962085308057,
      "acc_stderr": 0.031993655655275954,
      "acc_norm": 0.3127962085308057,
      "acc_norm_stderr": 0.031993655655275954
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.023099237430720336,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.023099237430720336
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.3922413793103448,
      "acc_stderr": 0.032124492663102744,
      "acc_norm": 0.3922413793103448,
      "acc_norm_stderr": 0.032124492663102744
    },
    "Cmmlu-public_relations": {
      "acc": 0.3850574712643678,
      "acc_stderr": 0.0369961890779064,
      "acc_norm": 0.3850574712643678,
      "acc_norm_stderr": 0.0369961890779064
    },
    "Cmmlu-security_study": {
      "acc": 0.37777777777777777,
      "acc_stderr": 0.04188307537595853,
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.04188307537595853
    },
    "Cmmlu-sociology": {
      "acc": 0.37610619469026546,
      "acc_stderr": 0.03229381261727144,
      "acc_norm": 0.37610619469026546,
      "acc_norm_stderr": 0.03229381261727144
    },
    "Cmmlu-sports_science": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.03793713171165634,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.03793713171165634
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.31351351351351353,
      "acc_stderr": 0.03420071750756409,
      "acc_norm": 0.31351351351351353,
      "acc_norm_stderr": 0.03420071750756409
    },
    "Cmmlu-virology": {
      "acc": 0.3905325443786982,
      "acc_stderr": 0.037639967056292634,
      "acc_norm": 0.3905325443786982,
      "acc_norm_stderr": 0.037639967056292634
    },
    "Cmmlu-world_history": {
      "acc": 0.3416149068322981,
      "acc_stderr": 0.03749284617282493,
      "acc_norm": 0.3416149068322981,
      "acc_norm_stderr": 0.03749284617282493
    },
    "Cmmlu-world_religions": {
      "acc": 0.4125,
      "acc_stderr": 0.03904067786683381,
      "acc_norm": 0.4125,
      "acc_norm_stderr": 0.03904067786683381
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained=/home/sdk_models/chatglm-6b,add_special_tokens=True,dtype='float16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:02:11.694566",
    "model_name": "chatglm_6b"
  }
}
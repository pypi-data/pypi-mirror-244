{
  "results": {
    "agi_eval-aqua-rat": {
      "acc": 0.1889763779527559,
      "acc_stderr": 0.02461275630319305
    },
    "agi_eval-gaokao-biology": {
      "acc": 0.2523809523809524,
      "acc_stderr": 0.030046599156031494
    },
    "agi_eval-gaokao-chemistry": {
      "acc": 0.22705314009661837,
      "acc_stderr": 0.029188042144307678
    },
    "agi_eval-gaokao-chinese": {
      "acc": 0.23983739837398374,
      "acc_stderr": 0.027279013441612703
    },
    "agi_eval-gaokao-english": {
      "acc": 0.2875816993464052,
      "acc_stderr": 0.02591780611714716
    },
    "agi_eval-gaokao-geography": {
      "acc": 0.2562814070351759,
      "acc_stderr": 0.031026320485108786
    },
    "agi_eval-gaokao-history": {
      "acc": 0.23404255319148937,
      "acc_stderr": 0.02767845257821238
    },
    "agi_eval-gaokao-mathqa": {
      "acc": 0.26495726495726496,
      "acc_stderr": 0.02358903575232897
    },
    "agi_eval-logiqa-en": {
      "acc": 0.21044546850998463,
      "acc_stderr": 0.015988369488888748
    },
    "agi_eval-logiqa-zh": {
      "acc": 0.21044546850998463,
      "acc_stderr": 0.01598836948888876
    },
    "agi_eval-lsat-ar": {
      "acc": 0.1956521739130435,
      "acc_stderr": 0.02621479970981959
    },
    "agi_eval-lsat-lr": {
      "acc": 0.24705882352941178,
      "acc_stderr": 0.019117091440867727
    },
    "agi_eval-lsat-rc": {
      "acc": 0.22676579925650558,
      "acc_stderr": 0.0255786080999064
    },
    "agi_eval-sat-en": {
      "acc": 0.27184466019417475,
      "acc_stderr": 0.031073880563247475
    },
    "agi_eval-sat-en-without-passage": {
      "acc": 0.24757281553398058,
      "acc_stderr": 0.030144409872297512
    },
    "agi_eval-sat-math": {
      "acc": 0.33636363636363636,
      "acc_stderr": 0.03192622349349311
    }
  },
  "versions": {
    "agi_eval-aqua-rat": 0,
    "agi_eval-gaokao-biology": 0,
    "agi_eval-gaokao-chemistry": 0,
    "agi_eval-gaokao-chinese": 0,
    "agi_eval-gaokao-english": 0,
    "agi_eval-gaokao-geography": 0,
    "agi_eval-gaokao-history": 0,
    "agi_eval-gaokao-mathqa": 0,
    "agi_eval-logiqa-en": 0,
    "agi_eval-logiqa-zh": 0,
    "agi_eval-lsat-ar": 0,
    "agi_eval-lsat-lr": 0,
    "agi_eval-lsat-rc": 0,
    "agi_eval-sat-en": 0,
    "agi_eval-sat-en-without-passage": 0,
    "agi_eval-sat-math": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/falcon_7b,dtype='float16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_falcon_model",
    "num_fewshot": 0,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:28:41.516431",
    "model_name": "falcon_7b"
  }
}
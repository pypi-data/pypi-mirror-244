{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.2958579881656805,
      "acc_stderr": 0.03521414412496478,
      "acc_norm": 0.2958579881656805,
      "acc_norm_stderr": 0.03521414412496478
    },
    "Cmmlu-anatomy": {
      "acc": 0.25675675675675674,
      "acc_stderr": 0.036030290036472144,
      "acc_norm": 0.25675675675675674,
      "acc_norm_stderr": 0.036030290036472144
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364998,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364998
    },
    "Cmmlu-arts": {
      "acc": 0.31875,
      "acc_stderr": 0.036955560385363254,
      "acc_norm": 0.31875,
      "acc_norm_stderr": 0.036955560385363254
    },
    "Cmmlu-astronomy": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.036810508691615486,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.036810508691615486
    },
    "Cmmlu-business_ethics": {
      "acc": 0.40669856459330145,
      "acc_stderr": 0.03405982026516628,
      "acc_norm": 0.40669856459330145,
      "acc_norm_stderr": 0.03405982026516628
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.20625,
      "acc_stderr": 0.03208782538184615,
      "acc_norm": 0.20625,
      "acc_norm_stderr": 0.03208782538184615
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.37404580152671757,
      "acc_stderr": 0.04243869242230523,
      "acc_norm": 0.37404580152671757,
      "acc_norm_stderr": 0.04243869242230523
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.039215686274509755,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.039215686274509755
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.308411214953271,
      "acc_stderr": 0.04485760883316698,
      "acc_norm": 0.308411214953271,
      "acc_norm_stderr": 0.04485760883316698
    },
    "Cmmlu-chinese_history": {
      "acc": 0.3065015479876161,
      "acc_stderr": 0.02569278296500778,
      "acc_norm": 0.3065015479876161,
      "acc_norm_stderr": 0.02569278296500778
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.2696078431372549,
      "acc_stderr": 0.031145570659486782,
      "acc_norm": 0.2696078431372549,
      "acc_norm_stderr": 0.031145570659486782
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.33519553072625696,
      "acc_stderr": 0.035382301081428424,
      "acc_norm": 0.33519553072625696,
      "acc_norm_stderr": 0.035382301081428424
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.32489451476793246,
      "acc_stderr": 0.030486039389105303,
      "acc_norm": 0.32489451476793246,
      "acc_norm_stderr": 0.030486039389105303
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.24528301886792453,
      "acc_stderr": 0.04198857662371222,
      "acc_norm": 0.24528301886792453,
      "acc_norm_stderr": 0.04198857662371222
    },
    "Cmmlu-college_education": {
      "acc": 0.42990654205607476,
      "acc_stderr": 0.04808472349429952,
      "acc_norm": 0.42990654205607476,
      "acc_norm_stderr": 0.04808472349429952
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.42452830188679247,
      "acc_stderr": 0.0482359303724347,
      "acc_norm": 0.42452830188679247,
      "acc_norm_stderr": 0.0482359303724347
    },
    "Cmmlu-college_law": {
      "acc": 0.26851851851851855,
      "acc_stderr": 0.04284467968052192,
      "acc_norm": 0.26851851851851855,
      "acc_norm_stderr": 0.04284467968052192
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.29523809523809524,
      "acc_stderr": 0.044729159560441434,
      "acc_norm": 0.29523809523809524,
      "acc_norm_stderr": 0.044729159560441434
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.36792452830188677,
      "acc_stderr": 0.04706187110761454,
      "acc_norm": 0.36792452830188677,
      "acc_norm_stderr": 0.04706187110761454
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2893772893772894,
      "acc_stderr": 0.027495860234525278,
      "acc_norm": 0.2893772893772894,
      "acc_norm_stderr": 0.027495860234525278
    },
    "Cmmlu-computer_science": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.03354092437591519,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.03354092437591519
    },
    "Cmmlu-computer_security": {
      "acc": 0.3216374269005848,
      "acc_stderr": 0.03582529442573122,
      "acc_norm": 0.3216374269005848,
      "acc_norm_stderr": 0.03582529442573122
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2925170068027211,
      "acc_stderr": 0.0376493198408517,
      "acc_norm": 0.2925170068027211,
      "acc_norm_stderr": 0.0376493198408517
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.33093525179856115,
      "acc_stderr": 0.04005585872539581,
      "acc_norm": 0.33093525179856115,
      "acc_norm_stderr": 0.04005585872539581
    },
    "Cmmlu-economics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03750293003086745,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03750293003086745
    },
    "Cmmlu-education": {
      "acc": 0.3312883435582822,
      "acc_stderr": 0.03697983910025588,
      "acc_norm": 0.3312883435582822,
      "acc_norm_stderr": 0.03697983910025588
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3430232558139535,
      "acc_stderr": 0.036302683175748356,
      "acc_norm": 0.3430232558139535,
      "acc_norm_stderr": 0.036302683175748356
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.25,
      "acc_stderr": 0.027331519390462633,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.027331519390462633
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.35858585858585856,
      "acc_stderr": 0.03416903640391521,
      "acc_norm": 0.35858585858585856,
      "acc_norm_stderr": 0.03416903640391521
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.44537815126050423,
      "acc_stderr": 0.032284106267163895,
      "acc_norm": 0.44537815126050423,
      "acc_norm_stderr": 0.032284106267163895
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.2608695652173913,
      "acc_stderr": 0.02901713355938126,
      "acc_norm": 0.2608695652173913,
      "acc_norm_stderr": 0.02901713355938126
    },
    "Cmmlu-ethnology": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.039725528847851375
    },
    "Cmmlu-food_science": {
      "acc": 0.3146853146853147,
      "acc_stderr": 0.03897077881510412,
      "acc_norm": 0.3146853146853147,
      "acc_norm_stderr": 0.03897077881510412
    },
    "Cmmlu-genetics": {
      "acc": 0.2897727272727273,
      "acc_stderr": 0.03429323080239875,
      "acc_norm": 0.2897727272727273,
      "acc_norm_stderr": 0.03429323080239875
    },
    "Cmmlu-global_facts": {
      "acc": 0.28187919463087246,
      "acc_stderr": 0.036982767559851,
      "acc_norm": 0.28187919463087246,
      "acc_norm_stderr": 0.036982767559851
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.23076923076923078,
      "acc_stderr": 0.032505932874173686,
      "acc_norm": 0.23076923076923078,
      "acc_norm_stderr": 0.032505932874173686
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.03661433360410717,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.03661433360410717
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3050847457627119,
      "acc_stderr": 0.04256799926288002,
      "acc_norm": 0.3050847457627119,
      "acc_norm_stderr": 0.04256799926288002
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.23170731707317074,
      "acc_stderr": 0.033047561588107864,
      "acc_norm": 0.23170731707317074,
      "acc_norm_stderr": 0.033047561588107864
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.041723430387053825,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.041723430387053825
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03737392962695623,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03737392962695623
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04285714285714281,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04285714285714281
    },
    "Cmmlu-international_law": {
      "acc": 0.34594594594594597,
      "acc_stderr": 0.03506727605846201,
      "acc_norm": 0.34594594594594597,
      "acc_norm_stderr": 0.03506727605846201
    },
    "Cmmlu-journalism": {
      "acc": 0.32558139534883723,
      "acc_stderr": 0.03583410038767278,
      "acc_norm": 0.32558139534883723,
      "acc_norm_stderr": 0.03583410038767278
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.3381995133819951,
      "acc_stderr": 0.023364586634695362,
      "acc_norm": 0.3381995133819951,
      "acc_norm_stderr": 0.023364586634695362
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.5280373831775701,
      "acc_stderr": 0.03420553075179498,
      "acc_norm": 0.5280373831775701,
      "acc_norm_stderr": 0.03420553075179498
    },
    "Cmmlu-logical": {
      "acc": 0.3902439024390244,
      "acc_stderr": 0.04416377855732609,
      "acc_norm": 0.3902439024390244,
      "acc_norm_stderr": 0.04416377855732609
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2540983606557377,
      "acc_stderr": 0.03957756102798663,
      "acc_norm": 0.2540983606557377,
      "acc_norm_stderr": 0.03957756102798663
    },
    "Cmmlu-management": {
      "acc": 0.3523809523809524,
      "acc_stderr": 0.03304401999334816,
      "acc_norm": 0.3523809523809524,
      "acc_norm_stderr": 0.03304401999334816
    },
    "Cmmlu-marketing": {
      "acc": 0.37777777777777777,
      "acc_stderr": 0.03623801889425292,
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.03623801889425292
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.43386243386243384,
      "acc_stderr": 0.036145820389423475,
      "acc_norm": 0.43386243386243384,
      "acc_norm_stderr": 0.036145820389423475
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3103448275862069,
      "acc_stderr": 0.043140913253187904,
      "acc_norm": 0.3103448275862069,
      "acc_norm_stderr": 0.043140913253187904
    },
    "Cmmlu-nutrition": {
      "acc": 0.30344827586206896,
      "acc_stderr": 0.038312260488503336,
      "acc_norm": 0.30344827586206896,
      "acc_norm_stderr": 0.038312260488503336
    },
    "Cmmlu-philosophy": {
      "acc": 0.3523809523809524,
      "acc_stderr": 0.046843501394377526,
      "acc_norm": 0.3523809523809524,
      "acc_norm_stderr": 0.046843501394377526
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.29714285714285715,
      "acc_stderr": 0.034645078898843704,
      "acc_norm": 0.29714285714285715,
      "acc_norm_stderr": 0.034645078898843704
    },
    "Cmmlu-professional_law": {
      "acc": 0.3033175355450237,
      "acc_stderr": 0.03172170716716874,
      "acc_norm": 0.3033175355450237,
      "acc_norm_stderr": 0.03172170716716874
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.28191489361702127,
      "acc_stderr": 0.023234393263661238,
      "acc_norm": 0.28191489361702127,
      "acc_norm_stderr": 0.023234393263661238
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.375,
      "acc_stderr": 0.0318529494648516,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.0318529494648516
    },
    "Cmmlu-public_relations": {
      "acc": 0.40229885057471265,
      "acc_stderr": 0.03728150104591907,
      "acc_norm": 0.40229885057471265,
      "acc_norm_stderr": 0.03728150104591907
    },
    "Cmmlu-security_study": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04072314811876837,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04072314811876837
    },
    "Cmmlu-sociology": {
      "acc": 0.36283185840707965,
      "acc_stderr": 0.032054460665973096,
      "acc_norm": 0.36283185840707965,
      "acc_norm_stderr": 0.032054460665973096
    },
    "Cmmlu-sports_science": {
      "acc": 0.3575757575757576,
      "acc_stderr": 0.037425970438065864,
      "acc_norm": 0.3575757575757576,
      "acc_norm_stderr": 0.037425970438065864
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.24324324324324326,
      "acc_stderr": 0.03162930395697948,
      "acc_norm": 0.24324324324324326,
      "acc_norm_stderr": 0.03162930395697948
    },
    "Cmmlu-virology": {
      "acc": 0.378698224852071,
      "acc_stderr": 0.037423404262347686,
      "acc_norm": 0.378698224852071,
      "acc_norm_stderr": 0.037423404262347686
    },
    "Cmmlu-world_history": {
      "acc": 0.36645962732919257,
      "acc_stderr": 0.03809256561874491,
      "acc_norm": 0.36645962732919257,
      "acc_norm_stderr": 0.03809256561874491
    },
    "Cmmlu-world_religions": {
      "acc": 0.425,
      "acc_stderr": 0.0392039498715957,
      "acc_norm": 0.425,
      "acc_norm_stderr": 0.0392039498715957
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama2-7b-hf,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:23:07.170459",
    "model_name": "llama2_7b"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3905325443786982,
      "acc_stderr": 0.037639967056292634,
      "acc_norm": 0.3905325443786982,
      "acc_norm_stderr": 0.037639967056292634
    },
    "Cmmlu-anatomy": {
      "acc": 0.19594594594594594,
      "acc_stderr": 0.032737996420279705,
      "acc_norm": 0.19594594594594594,
      "acc_norm_stderr": 0.032737996420279705
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.22560975609756098,
      "acc_stderr": 0.032738974545663414,
      "acc_norm": 0.22560975609756098,
      "acc_norm_stderr": 0.032738974545663414
    },
    "Cmmlu-arts": {
      "acc": 0.3375,
      "acc_stderr": 0.03749999999999997,
      "acc_norm": 0.3375,
      "acc_norm_stderr": 0.03749999999999997
    },
    "Cmmlu-astronomy": {
      "acc": 0.2787878787878788,
      "acc_stderr": 0.03501438706296781,
      "acc_norm": 0.2787878787878788,
      "acc_norm_stderr": 0.03501438706296781
    },
    "Cmmlu-business_ethics": {
      "acc": 0.41148325358851673,
      "acc_stderr": 0.034121163182778434,
      "acc_norm": 0.41148325358851673,
      "acc_norm_stderr": 0.034121163182778434
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.33125,
      "acc_stderr": 0.03732598513993524,
      "acc_norm": 0.33125,
      "acc_norm_stderr": 0.03732598513993524
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5114503816793893,
      "acc_stderr": 0.043841400240780176,
      "acc_norm": 0.5114503816793893,
      "acc_norm_stderr": 0.043841400240780176
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.375,
      "acc_stderr": 0.041666666666666664,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.041666666666666664
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.3364485981308411,
      "acc_stderr": 0.04589271111471628,
      "acc_norm": 0.3364485981308411,
      "acc_norm_stderr": 0.04589271111471628
    },
    "Cmmlu-chinese_history": {
      "acc": 0.39009287925696595,
      "acc_stderr": 0.027182408040188046,
      "acc_norm": 0.39009287925696595,
      "acc_norm_stderr": 0.027182408040188046
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.031493281045079556,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.031493281045079556
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.4748603351955307,
      "acc_stderr": 0.03742918386493423,
      "acc_norm": 0.4748603351955307,
      "acc_norm_stderr": 0.03742918386493423
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.2911392405063291,
      "acc_stderr": 0.029571601065753374,
      "acc_norm": 0.2911392405063291,
      "acc_norm_stderr": 0.029571601065753374
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.24528301886792453,
      "acc_stderr": 0.04198857662371223,
      "acc_norm": 0.24528301886792453,
      "acc_norm_stderr": 0.04198857662371223
    },
    "Cmmlu-college_education": {
      "acc": 0.32710280373831774,
      "acc_stderr": 0.04556837693674773,
      "acc_norm": 0.32710280373831774,
      "acc_norm_stderr": 0.04556837693674773
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3867924528301887,
      "acc_stderr": 0.04752784159123843,
      "acc_norm": 0.3867924528301887,
      "acc_norm_stderr": 0.04752784159123843
    },
    "Cmmlu-college_law": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04557239513497751,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04557239513497751
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.04176466758604902,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.04176466758604902
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.25471698113207547,
      "acc_stderr": 0.0425201622376331,
      "acc_norm": 0.25471698113207547,
      "acc_norm_stderr": 0.0425201622376331
    },
    "Cmmlu-college_medicine": {
      "acc": 0.34065934065934067,
      "acc_stderr": 0.028736285365734002,
      "acc_norm": 0.34065934065934067,
      "acc_norm_stderr": 0.028736285365734002
    },
    "Cmmlu-computer_science": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.03441190023482465,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.03441190023482465
    },
    "Cmmlu-computer_security": {
      "acc": 0.391812865497076,
      "acc_stderr": 0.037439798259263996,
      "acc_norm": 0.391812865497076,
      "acc_norm_stderr": 0.037439798259263996
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.32653061224489793,
      "acc_stderr": 0.03881007243853121,
      "acc_norm": 0.32653061224489793,
      "acc_norm_stderr": 0.03881007243853121
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.302158273381295,
      "acc_stderr": 0.03908914479291562,
      "acc_norm": 0.302158273381295,
      "acc_norm_stderr": 0.03908914479291562
    },
    "Cmmlu-economics": {
      "acc": 0.3522012578616352,
      "acc_stderr": 0.038000294121306503,
      "acc_norm": 0.3522012578616352,
      "acc_norm_stderr": 0.038000294121306503
    },
    "Cmmlu-education": {
      "acc": 0.39263803680981596,
      "acc_stderr": 0.03836740907831029,
      "acc_norm": 0.39263803680981596,
      "acc_norm_stderr": 0.03836740907831029
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.36627906976744184,
      "acc_stderr": 0.03684317268101586,
      "acc_norm": 0.36627906976744184,
      "acc_norm_stderr": 0.03684317268101586
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.26587301587301587,
      "acc_stderr": 0.02788597694851165,
      "acc_norm": 0.26587301587301587,
      "acc_norm_stderr": 0.02788597694851165
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.033184773338453315,
      "acc_norm": 0.3181818181818182,
      "acc_norm_stderr": 0.033184773338453315
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.47478991596638653,
      "acc_stderr": 0.03243718055137411,
      "acc_norm": 0.47478991596638653,
      "acc_norm_stderr": 0.03243718055137411
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.26521739130434785,
      "acc_stderr": 0.02917176407847258,
      "acc_norm": 0.26521739130434785,
      "acc_norm_stderr": 0.02917176407847258
    },
    "Cmmlu-ethnology": {
      "acc": 0.37037037037037035,
      "acc_stderr": 0.04171654161354543,
      "acc_norm": 0.37037037037037035,
      "acc_norm_stderr": 0.04171654161354543
    },
    "Cmmlu-food_science": {
      "acc": 0.4125874125874126,
      "acc_stderr": 0.04131287692392343,
      "acc_norm": 0.4125874125874126,
      "acc_norm_stderr": 0.04131287692392343
    },
    "Cmmlu-genetics": {
      "acc": 0.3068181818181818,
      "acc_stderr": 0.03486142240553238,
      "acc_norm": 0.3068181818181818,
      "acc_norm_stderr": 0.03486142240553238
    },
    "Cmmlu-global_facts": {
      "acc": 0.3087248322147651,
      "acc_stderr": 0.037973480272130815,
      "acc_norm": 0.3087248322147651,
      "acc_norm_stderr": 0.037973480272130815
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.22485207100591717,
      "acc_stderr": 0.03220965704514523,
      "acc_norm": 0.22485207100591717,
      "acc_norm_stderr": 0.03220965704514523
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04118680421434766,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04118680421434766
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3728813559322034,
      "acc_stderr": 0.044706148865825766,
      "acc_norm": 0.3728813559322034,
      "acc_norm_stderr": 0.044706148865825766
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2682926829268293,
      "acc_stderr": 0.03470398212814533,
      "acc_norm": 0.2682926829268293,
      "acc_norm_stderr": 0.03470398212814533
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2818181818181818,
      "acc_stderr": 0.0430911870994646,
      "acc_norm": 0.2818181818181818,
      "acc_norm_stderr": 0.0430911870994646
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.43356643356643354,
      "acc_stderr": 0.04158705287172621,
      "acc_norm": 0.43356643356643354,
      "acc_norm_stderr": 0.04158705287172621
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.04360314860077459,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.04360314860077459
    },
    "Cmmlu-international_law": {
      "acc": 0.35135135135135137,
      "acc_stderr": 0.03519384049793635,
      "acc_norm": 0.35135135135135137,
      "acc_norm_stderr": 0.03519384049793635
    },
    "Cmmlu-journalism": {
      "acc": 0.4127906976744186,
      "acc_stderr": 0.03764985943823199,
      "acc_norm": 0.4127906976744186,
      "acc_norm_stderr": 0.03764985943823199
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.39416058394160586,
      "acc_stderr": 0.02413367336864483,
      "acc_norm": 0.39416058394160586,
      "acc_norm_stderr": 0.02413367336864483
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.5280373831775701,
      "acc_stderr": 0.034205530751794964,
      "acc_norm": 0.5280373831775701,
      "acc_norm_stderr": 0.034205530751794964
    },
    "Cmmlu-logical": {
      "acc": 0.3008130081300813,
      "acc_stderr": 0.04152073768551428,
      "acc_norm": 0.3008130081300813,
      "acc_norm_stderr": 0.04152073768551428
    },
    "Cmmlu-machine_learning": {
      "acc": 0.3442622950819672,
      "acc_stderr": 0.04319337331204006,
      "acc_norm": 0.3442622950819672,
      "acc_norm_stderr": 0.04319337331204006
    },
    "Cmmlu-management": {
      "acc": 0.38095238095238093,
      "acc_stderr": 0.033591100467499885,
      "acc_norm": 0.38095238095238093,
      "acc_norm_stderr": 0.033591100467499885
    },
    "Cmmlu-marketing": {
      "acc": 0.46111111111111114,
      "acc_stderr": 0.03725854514270211,
      "acc_norm": 0.46111111111111114,
      "acc_norm_stderr": 0.03725854514270211
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4126984126984127,
      "acc_stderr": 0.035906085602154886,
      "acc_norm": 0.4126984126984127,
      "acc_norm_stderr": 0.035906085602154886
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.25862068965517243,
      "acc_stderr": 0.040832215386495764,
      "acc_norm": 0.25862068965517243,
      "acc_norm_stderr": 0.040832215386495764
    },
    "Cmmlu-nutrition": {
      "acc": 0.3586206896551724,
      "acc_stderr": 0.039966295748767186,
      "acc_norm": 0.3586206896551724,
      "acc_norm_stderr": 0.039966295748767186
    },
    "Cmmlu-philosophy": {
      "acc": 0.37142857142857144,
      "acc_stderr": 0.04738035414793429,
      "acc_norm": 0.37142857142857144,
      "acc_norm_stderr": 0.04738035414793429
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.44571428571428573,
      "acc_stderr": 0.03768083305144797,
      "acc_norm": 0.44571428571428573,
      "acc_norm_stderr": 0.03768083305144797
    },
    "Cmmlu-professional_law": {
      "acc": 0.27488151658767773,
      "acc_stderr": 0.03080829112478033,
      "acc_norm": 0.27488151658767773,
      "acc_norm_stderr": 0.03080829112478033
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.28191489361702127,
      "acc_stderr": 0.02323439326366123,
      "acc_norm": 0.28191489361702127,
      "acc_norm_stderr": 0.02323439326366123
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.4051724137931034,
      "acc_stderr": 0.03230051859810265,
      "acc_norm": 0.4051724137931034,
      "acc_norm_stderr": 0.03230051859810265
    },
    "Cmmlu-public_relations": {
      "acc": 0.3505747126436782,
      "acc_stderr": 0.03627703962615275,
      "acc_norm": 0.3505747126436782,
      "acc_norm_stderr": 0.03627703962615275
    },
    "Cmmlu-security_study": {
      "acc": 0.4962962962962963,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.4962962962962963,
      "acc_norm_stderr": 0.04319223625811331
    },
    "Cmmlu-sociology": {
      "acc": 0.37610619469026546,
      "acc_stderr": 0.03229381261727144,
      "acc_norm": 0.37610619469026546,
      "acc_norm_stderr": 0.03229381261727144
    },
    "Cmmlu-sports_science": {
      "acc": 0.3878787878787879,
      "acc_stderr": 0.038049136539710114,
      "acc_norm": 0.3878787878787879,
      "acc_norm_stderr": 0.038049136539710114
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.33513513513513515,
      "acc_stderr": 0.03479907984892718,
      "acc_norm": 0.33513513513513515,
      "acc_norm_stderr": 0.03479907984892718
    },
    "Cmmlu-virology": {
      "acc": 0.4378698224852071,
      "acc_stderr": 0.038276861175393674,
      "acc_norm": 0.4378698224852071,
      "acc_norm_stderr": 0.038276861175393674
    },
    "Cmmlu-world_history": {
      "acc": 0.40372670807453415,
      "acc_stderr": 0.03878880744346832,
      "acc_norm": 0.40372670807453415,
      "acc_norm_stderr": 0.03878880744346832
    },
    "Cmmlu-world_religions": {
      "acc": 0.3875,
      "acc_stderr": 0.03863583812241406,
      "acc_norm": 0.3875,
      "acc_norm_stderr": 0.03863583812241406
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/belle_7b_2m,dtype='bfloat16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:37:49.613945",
    "model_name": "belle_7b_2m"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.27218934911242604,
      "acc_stderr": 0.03433919627548533,
      "acc_norm": 0.27218934911242604,
      "acc_norm_stderr": 0.03433919627548533
    },
    "Cmmlu-anatomy": {
      "acc": 0.27702702702702703,
      "acc_stderr": 0.036911647897386525,
      "acc_norm": 0.27702702702702703,
      "acc_norm_stderr": 0.036911647897386525
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.03563788836258828,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.03563788836258828
    },
    "Cmmlu-arts": {
      "acc": 0.25625,
      "acc_stderr": 0.0346215784586514,
      "acc_norm": 0.25625,
      "acc_norm_stderr": 0.0346215784586514
    },
    "Cmmlu-astronomy": {
      "acc": 0.22424242424242424,
      "acc_stderr": 0.03256866661681102,
      "acc_norm": 0.22424242424242424,
      "acc_norm_stderr": 0.03256866661681102
    },
    "Cmmlu-business_ethics": {
      "acc": 0.24401913875598086,
      "acc_stderr": 0.029780753228706103,
      "acc_norm": 0.24401913875598086,
      "acc_norm_stderr": 0.029780753228706103
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.2375,
      "acc_stderr": 0.033748398517792225,
      "acc_norm": 0.2375,
      "acc_norm_stderr": 0.033748398517792225
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.24427480916030533,
      "acc_stderr": 0.03768335959728744,
      "acc_norm": 0.24427480916030533,
      "acc_norm_stderr": 0.03768335959728744
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.0365078171078927,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.0365078171078927
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.2336448598130841,
      "acc_stderr": 0.041099848424639984,
      "acc_norm": 0.2336448598130841,
      "acc_norm_stderr": 0.041099848424639984
    },
    "Cmmlu-chinese_history": {
      "acc": 0.21981424148606812,
      "acc_stderr": 0.023078043444172087,
      "acc_norm": 0.21981424148606812,
      "acc_norm_stderr": 0.023078043444172087
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.030587591351604257,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.030587591351604257
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.20670391061452514,
      "acc_stderr": 0.030351628795046437,
      "acc_norm": 0.20670391061452514,
      "acc_norm_stderr": 0.030351628795046437
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.23628691983122363,
      "acc_stderr": 0.027652153144159267,
      "acc_norm": 0.23628691983122363,
      "acc_norm_stderr": 0.027652153144159267
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.20754716981132076,
      "acc_stderr": 0.039577692383779325,
      "acc_norm": 0.20754716981132076,
      "acc_norm_stderr": 0.039577692383779325
    },
    "Cmmlu-college_education": {
      "acc": 0.205607476635514,
      "acc_stderr": 0.039254015800704825,
      "acc_norm": 0.205607476635514,
      "acc_norm_stderr": 0.039254015800704825
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.29245283018867924,
      "acc_stderr": 0.04439263906199628,
      "acc_norm": 0.29245283018867924,
      "acc_norm_stderr": 0.04439263906199628
    },
    "Cmmlu-college_law": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.04236511258094633,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.04236511258094633
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.20952380952380953,
      "acc_stderr": 0.03990657150993187,
      "acc_norm": 0.20952380952380953,
      "acc_norm_stderr": 0.03990657150993187
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.2358490566037736,
      "acc_stderr": 0.04142972007800375,
      "acc_norm": 0.2358490566037736,
      "acc_norm_stderr": 0.04142972007800375
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2490842490842491,
      "acc_stderr": 0.02622311550050611,
      "acc_norm": 0.2490842490842491,
      "acc_norm_stderr": 0.02622311550050611
    },
    "Cmmlu-computer_science": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.031980016601150726,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.031980016601150726
    },
    "Cmmlu-computer_security": {
      "acc": 0.2807017543859649,
      "acc_stderr": 0.034462962170884265,
      "acc_norm": 0.2807017543859649,
      "acc_norm_stderr": 0.034462962170884265
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2585034013605442,
      "acc_stderr": 0.03623358323071023,
      "acc_norm": 0.2585034013605442,
      "acc_norm_stderr": 0.03623358323071023
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2805755395683453,
      "acc_stderr": 0.03824529014900686,
      "acc_norm": 0.2805755395683453,
      "acc_norm_stderr": 0.03824529014900686
    },
    "Cmmlu-economics": {
      "acc": 0.27044025157232704,
      "acc_stderr": 0.03533764101912229,
      "acc_norm": 0.27044025157232704,
      "acc_norm_stderr": 0.03533764101912229
    },
    "Cmmlu-education": {
      "acc": 0.2392638036809816,
      "acc_stderr": 0.03351953879521268,
      "acc_norm": 0.2392638036809816,
      "acc_norm_stderr": 0.03351953879521268
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.2558139534883721,
      "acc_stderr": 0.03336605189761064,
      "acc_norm": 0.2558139534883721,
      "acc_norm_stderr": 0.03336605189761064
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.02624125778712509,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.02624125778712509
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.029126522834586815,
      "acc_norm": 0.21212121212121213,
      "acc_norm_stderr": 0.029126522834586815
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.25630252100840334,
      "acc_stderr": 0.02835962087053395,
      "acc_norm": 0.25630252100840334,
      "acc_norm_stderr": 0.02835962087053395
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.02725685083881996,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.02725685083881996
    },
    "Cmmlu-ethnology": {
      "acc": 0.2962962962962963,
      "acc_stderr": 0.039446241625011175,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.039446241625011175
    },
    "Cmmlu-food_science": {
      "acc": 0.3006993006993007,
      "acc_stderr": 0.03848167949490064,
      "acc_norm": 0.3006993006993007,
      "acc_norm_stderr": 0.03848167949490064
    },
    "Cmmlu-genetics": {
      "acc": 0.25,
      "acc_stderr": 0.032732683535398856,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.032732683535398856
    },
    "Cmmlu-global_facts": {
      "acc": 0.28187919463087246,
      "acc_stderr": 0.036982767559851,
      "acc_norm": 0.28187919463087246,
      "acc_norm_stderr": 0.036982767559851
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.2485207100591716,
      "acc_stderr": 0.033341501981019636,
      "acc_norm": 0.2485207100591716,
      "acc_norm_stderr": 0.033341501981019636
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.25757575757575757,
      "acc_stderr": 0.03820699814849796,
      "acc_norm": 0.25757575757575757,
      "acc_norm_stderr": 0.03820699814849796
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2796610169491525,
      "acc_stderr": 0.04149459161011112,
      "acc_norm": 0.2796610169491525,
      "acc_norm_stderr": 0.04149459161011112
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.21951219512195122,
      "acc_stderr": 0.03242041613395385,
      "acc_norm": 0.21951219512195122,
      "acc_norm_stderr": 0.03242041613395385
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3,
      "acc_stderr": 0.04389311454644286,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.04389311454644286
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03737392962695624,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03737392962695624
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.04134913018303316,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.04134913018303316
    },
    "Cmmlu-international_law": {
      "acc": 0.31891891891891894,
      "acc_stderr": 0.034358218597411866,
      "acc_norm": 0.31891891891891894,
      "acc_norm_stderr": 0.034358218597411866
    },
    "Cmmlu-journalism": {
      "acc": 0.29069767441860467,
      "acc_stderr": 0.034724693044775996,
      "acc_norm": 0.29069767441860467,
      "acc_norm_stderr": 0.034724693044775996
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.2725060827250608,
      "acc_stderr": 0.02198927219610504,
      "acc_norm": 0.2725060827250608,
      "acc_norm_stderr": 0.02198927219610504
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.308411214953271,
      "acc_stderr": 0.03164457376920025,
      "acc_norm": 0.308411214953271,
      "acc_norm_stderr": 0.03164457376920025
    },
    "Cmmlu-logical": {
      "acc": 0.25203252032520324,
      "acc_stderr": 0.039308795268239924,
      "acc_norm": 0.25203252032520324,
      "acc_norm_stderr": 0.039308795268239924
    },
    "Cmmlu-machine_learning": {
      "acc": 0.29508196721311475,
      "acc_stderr": 0.04146178164901211,
      "acc_norm": 0.29508196721311475,
      "acc_norm_stderr": 0.04146178164901211
    },
    "Cmmlu-management": {
      "acc": 0.29523809523809524,
      "acc_stderr": 0.031552535545053995,
      "acc_norm": 0.29523809523809524,
      "acc_norm_stderr": 0.031552535545053995
    },
    "Cmmlu-marketing": {
      "acc": 0.3,
      "acc_stderr": 0.03425177889602087,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.03425177889602087
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.30687830687830686,
      "acc_stderr": 0.03363635410184865,
      "acc_norm": 0.30687830687830686,
      "acc_norm_stderr": 0.03363635410184865
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.25,
      "acc_stderr": 0.04037864265436242,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04037864265436242
    },
    "Cmmlu-nutrition": {
      "acc": 0.2896551724137931,
      "acc_stderr": 0.037800192304380135,
      "acc_norm": 0.2896551724137931,
      "acc_norm_stderr": 0.037800192304380135
    },
    "Cmmlu-philosophy": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.043362909039199406,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.043362909039199406
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.29714285714285715,
      "acc_stderr": 0.03464507889884372,
      "acc_norm": 0.29714285714285715,
      "acc_norm_stderr": 0.03464507889884372
    },
    "Cmmlu-professional_law": {
      "acc": 0.24170616113744076,
      "acc_stderr": 0.029542889951620098,
      "acc_norm": 0.24170616113744076,
      "acc_norm_stderr": 0.029542889951620098
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2712765957446808,
      "acc_stderr": 0.022960000252372683,
      "acc_norm": 0.2712765957446808,
      "acc_norm_stderr": 0.022960000252372683
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.2543103448275862,
      "acc_stderr": 0.028652009240399654,
      "acc_norm": 0.2543103448275862,
      "acc_norm_stderr": 0.028652009240399654
    },
    "Cmmlu-public_relations": {
      "acc": 0.25287356321839083,
      "acc_stderr": 0.0330465186437516,
      "acc_norm": 0.25287356321839083,
      "acc_norm_stderr": 0.0330465186437516
    },
    "Cmmlu-security_study": {
      "acc": 0.22962962962962963,
      "acc_stderr": 0.03633384414073463,
      "acc_norm": 0.22962962962962963,
      "acc_norm_stderr": 0.03633384414073463
    },
    "Cmmlu-sociology": {
      "acc": 0.26991150442477874,
      "acc_stderr": 0.02959423999541741,
      "acc_norm": 0.26991150442477874,
      "acc_norm_stderr": 0.02959423999541741
    },
    "Cmmlu-sports_science": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.0347769116216366,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.0347769116216366
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.21081081081081082,
      "acc_stderr": 0.030069630502647618,
      "acc_norm": 0.21081081081081082,
      "acc_norm_stderr": 0.030069630502647618
    },
    "Cmmlu-virology": {
      "acc": 0.2485207100591716,
      "acc_stderr": 0.033341501981019615,
      "acc_norm": 0.2485207100591716,
      "acc_norm_stderr": 0.033341501981019615
    },
    "Cmmlu-world_history": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.03260869565217389,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.03260869565217389
    },
    "Cmmlu-world_religions": {
      "acc": 0.25625,
      "acc_stderr": 0.03462157845865141,
      "acc_norm": 0.25625,
      "acc_norm_stderr": 0.03462157845865141
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama-7b-hf,load_in_8bit=True,dtype='float16',tokenizer=/home/sdk_token/chinese_llama_alpaca_tokenizer,use_accelerate=False,peft=/home/finetuned_models/my_chinese_llama_alpaca_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:05:01.434382",
    "model_name": "chinese_llama_alpaca"
  }
}
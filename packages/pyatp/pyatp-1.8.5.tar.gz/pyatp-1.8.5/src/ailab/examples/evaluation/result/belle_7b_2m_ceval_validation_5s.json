{
  "results": {
    "Ceval-valid-accountant": {
      "acc": 0.3673469387755102,
      "acc_stderr": 0.06958255967849926,
      "acc_norm": 0.3673469387755102,
      "acc_norm_stderr": 0.06958255967849926
    },
    "Ceval-valid-advanced_mathematics": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.11369720523522558,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.11369720523522558
    },
    "Ceval-valid-art_studies": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.08124094920275461,
      "acc_norm": 0.30303030303030304,
      "acc_norm_stderr": 0.08124094920275461
    },
    "Ceval-valid-basic_medicine": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-business_administration": {
      "acc": 0.42424242424242425,
      "acc_stderr": 0.08736789844447573,
      "acc_norm": 0.42424242424242425,
      "acc_norm_stderr": 0.08736789844447573
    },
    "Ceval-valid-chinese_language_and_literature": {
      "acc": 0.30434782608695654,
      "acc_stderr": 0.09810018692482894,
      "acc_norm": 0.30434782608695654,
      "acc_norm_stderr": 0.09810018692482894
    },
    "Ceval-valid-civil_servant": {
      "acc": 0.3191489361702128,
      "acc_stderr": 0.0687296045180637,
      "acc_norm": 0.3191489361702128,
      "acc_norm_stderr": 0.0687296045180637
    },
    "Ceval-valid-clinical_medicine": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.0971859061499725,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.0971859061499725
    },
    "Ceval-valid-college_chemistry": {
      "acc": 0.25,
      "acc_stderr": 0.09028938981432691,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.09028938981432691
    },
    "Ceval-valid-college_economics": {
      "acc": 0.4727272727272727,
      "acc_stderr": 0.06794008776080906,
      "acc_norm": 0.4727272727272727,
      "acc_norm_stderr": 0.06794008776080906
    },
    "Ceval-valid-college_physics": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.09609167675529229,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.09609167675529229
    },
    "Ceval-valid-college_programming": {
      "acc": 0.35135135135135137,
      "acc_stderr": 0.0795654132101608,
      "acc_norm": 0.35135135135135137,
      "acc_norm_stderr": 0.0795654132101608
    },
    "Ceval-valid-computer_architecture": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.09523809523809523,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.09523809523809523
    },
    "Ceval-valid-computer_network": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-discrete_mathematics": {
      "acc": 0.3125,
      "acc_stderr": 0.11967838846954226,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.11967838846954226
    },
    "Ceval-valid-education_science": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.09398415777506855,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.09398415777506855
    },
    "Ceval-valid-electrical_engineer": {
      "acc": 0.32432432432432434,
      "acc_stderr": 0.07802030664724674,
      "acc_norm": 0.32432432432432434,
      "acc_norm_stderr": 0.07802030664724674
    },
    "Ceval-valid-environmental_impact_assessment_engineer": {
      "acc": 0.1935483870967742,
      "acc_stderr": 0.07213122508063839,
      "acc_norm": 0.1935483870967742,
      "acc_norm_stderr": 0.07213122508063839
    },
    "Ceval-valid-fire_engineer": {
      "acc": 0.25806451612903225,
      "acc_stderr": 0.07988892740217941,
      "acc_norm": 0.25806451612903225,
      "acc_norm_stderr": 0.07988892740217941
    },
    "Ceval-valid-high_school_biology": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.11369720523522558,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.11369720523522558
    },
    "Ceval-valid-high_school_chemistry": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.11369720523522557,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.11369720523522557
    },
    "Ceval-valid-high_school_chinese": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-high_school_geography": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-high_school_history": {
      "acc": 0.45,
      "acc_stderr": 0.11413288653790232,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.11413288653790232
    },
    "Ceval-valid-high_school_mathematics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.11433239009500591,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.11433239009500591
    },
    "Ceval-valid-high_school_physics": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.11768778828946262,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.11768778828946262
    },
    "Ceval-valid-high_school_politics": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-ideological_and_moral_cultivation": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.11768778828946262,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.11768778828946262
    },
    "Ceval-valid-law": {
      "acc": 0.16666666666666666,
      "acc_stderr": 0.07770873402002614,
      "acc_norm": 0.16666666666666666,
      "acc_norm_stderr": 0.07770873402002614
    },
    "Ceval-valid-legal_professional": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.0879391124952055,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.0879391124952055
    },
    "Ceval-valid-logic": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.1049727762162956,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.1049727762162956
    },
    "Ceval-valid-mao_zedong_thought": {
      "acc": 0.375,
      "acc_stderr": 0.10094660663590604,
      "acc_norm": 0.375,
      "acc_norm_stderr": 0.10094660663590604
    },
    "Ceval-valid-marxism": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.10956136839295433,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.10956136839295433
    },
    "Ceval-valid-metrology_engineer": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.0982946374365981,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.0982946374365981
    },
    "Ceval-valid-middle_school_biology": {
      "acc": 0.47619047619047616,
      "acc_stderr": 0.11167656571008164,
      "acc_norm": 0.47619047619047616,
      "acc_norm_stderr": 0.11167656571008164
    },
    "Ceval-valid-middle_school_chemistry": {
      "acc": 0.4,
      "acc_stderr": 0.11239029738980327,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.11239029738980327
    },
    "Ceval-valid-middle_school_geography": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.14213381090374033,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.14213381090374033
    },
    "Ceval-valid-middle_school_history": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.10497277621629558,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.10497277621629558
    },
    "Ceval-valid-middle_school_mathematics": {
      "acc": 0.10526315789473684,
      "acc_stderr": 0.07233518641434492,
      "acc_norm": 0.10526315789473684,
      "acc_norm_stderr": 0.07233518641434492
    },
    "Ceval-valid-middle_school_physics": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.10956136839295434,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.10956136839295434
    },
    "Ceval-valid-middle_school_politics": {
      "acc": 0.38095238095238093,
      "acc_stderr": 0.10858813572372741,
      "acc_norm": 0.38095238095238093,
      "acc_norm_stderr": 0.10858813572372741
    },
    "Ceval-valid-modern_chinese_history": {
      "acc": 0.391304347826087,
      "acc_stderr": 0.10405096111532161,
      "acc_norm": 0.391304347826087,
      "acc_norm_stderr": 0.10405096111532161
    },
    "Ceval-valid-operating_system": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-physician": {
      "acc": 0.3469387755102041,
      "acc_stderr": 0.06870411522695292,
      "acc_norm": 0.3469387755102041,
      "acc_norm_stderr": 0.06870411522695292
    },
    "Ceval-valid-plant_protection": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.10865714630312667,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.10865714630312667
    },
    "Ceval-valid-probability_and_statistics": {
      "acc": 0.1111111111111111,
      "acc_stderr": 0.07622159339667062,
      "acc_norm": 0.1111111111111111,
      "acc_norm_stderr": 0.07622159339667062
    },
    "Ceval-valid-professional_tour_guide": {
      "acc": 0.3793103448275862,
      "acc_stderr": 0.09169709590633637,
      "acc_norm": 0.3793103448275862,
      "acc_norm_stderr": 0.09169709590633637
    },
    "Ceval-valid-sports_science": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.1136972052352256,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.1136972052352256
    },
    "Ceval-valid-tax_accountant": {
      "acc": 0.2653061224489796,
      "acc_stderr": 0.06372446937141223,
      "acc_norm": 0.2653061224489796,
      "acc_norm_stderr": 0.06372446937141223
    },
    "Ceval-valid-teacher_qualification": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.07593355178041428,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.07593355178041428
    },
    "Ceval-valid-urban_and_rural_planner": {
      "acc": 0.5217391304347826,
      "acc_stderr": 0.07446511639805871,
      "acc_norm": 0.5217391304347826,
      "acc_norm_stderr": 0.07446511639805871
    },
    "Ceval-valid-veterinary_medicine": {
      "acc": 0.34782608695652173,
      "acc_stderr": 0.10154334054280735,
      "acc_norm": 0.34782608695652173,
      "acc_norm_stderr": 0.10154334054280735
    }
  },
  "versions": {
    "Ceval-valid-accountant": 1,
    "Ceval-valid-advanced_mathematics": 1,
    "Ceval-valid-art_studies": 1,
    "Ceval-valid-basic_medicine": 1,
    "Ceval-valid-business_administration": 1,
    "Ceval-valid-chinese_language_and_literature": 1,
    "Ceval-valid-civil_servant": 1,
    "Ceval-valid-clinical_medicine": 1,
    "Ceval-valid-college_chemistry": 1,
    "Ceval-valid-college_economics": 1,
    "Ceval-valid-college_physics": 1,
    "Ceval-valid-college_programming": 1,
    "Ceval-valid-computer_architecture": 1,
    "Ceval-valid-computer_network": 1,
    "Ceval-valid-discrete_mathematics": 1,
    "Ceval-valid-education_science": 1,
    "Ceval-valid-electrical_engineer": 1,
    "Ceval-valid-environmental_impact_assessment_engineer": 1,
    "Ceval-valid-fire_engineer": 1,
    "Ceval-valid-high_school_biology": 1,
    "Ceval-valid-high_school_chemistry": 1,
    "Ceval-valid-high_school_chinese": 1,
    "Ceval-valid-high_school_geography": 1,
    "Ceval-valid-high_school_history": 1,
    "Ceval-valid-high_school_mathematics": 1,
    "Ceval-valid-high_school_physics": 1,
    "Ceval-valid-high_school_politics": 1,
    "Ceval-valid-ideological_and_moral_cultivation": 1,
    "Ceval-valid-law": 1,
    "Ceval-valid-legal_professional": 1,
    "Ceval-valid-logic": 1,
    "Ceval-valid-mao_zedong_thought": 1,
    "Ceval-valid-marxism": 1,
    "Ceval-valid-metrology_engineer": 1,
    "Ceval-valid-middle_school_biology": 1,
    "Ceval-valid-middle_school_chemistry": 1,
    "Ceval-valid-middle_school_geography": 1,
    "Ceval-valid-middle_school_history": 1,
    "Ceval-valid-middle_school_mathematics": 1,
    "Ceval-valid-middle_school_physics": 1,
    "Ceval-valid-middle_school_politics": 1,
    "Ceval-valid-modern_chinese_history": 1,
    "Ceval-valid-operating_system": 1,
    "Ceval-valid-physician": 1,
    "Ceval-valid-plant_protection": 1,
    "Ceval-valid-probability_and_statistics": 1,
    "Ceval-valid-professional_tour_guide": 1,
    "Ceval-valid-sports_science": 1,
    "Ceval-valid-tax_accountant": 1,
    "Ceval-valid-teacher_qualification": 1,
    "Ceval-valid-urban_and_rural_planner": 1,
    "Ceval-valid-veterinary_medicine": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/belle_7b_2m,dtype='bfloat16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:18:16.890464",
    "model_name": "belle_7b_2m"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.33727810650887574,
      "acc_stderr": 0.03647582250277504,
      "acc_norm": 0.33727810650887574,
      "acc_norm_stderr": 0.03647582250277504
    },
    "Cmmlu-anatomy": {
      "acc": 0.25675675675675674,
      "acc_stderr": 0.036030290036472144,
      "acc_norm": 0.25675675675675674,
      "acc_norm_stderr": 0.036030290036472144
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.27439024390243905,
      "acc_stderr": 0.03494959016177541,
      "acc_norm": 0.27439024390243905,
      "acc_norm_stderr": 0.03494959016177541
    },
    "Cmmlu-arts": {
      "acc": 0.46875,
      "acc_stderr": 0.039575057062617526,
      "acc_norm": 0.46875,
      "acc_norm_stderr": 0.039575057062617526
    },
    "Cmmlu-astronomy": {
      "acc": 0.3212121212121212,
      "acc_stderr": 0.0364620496325381,
      "acc_norm": 0.3212121212121212,
      "acc_norm_stderr": 0.0364620496325381
    },
    "Cmmlu-business_ethics": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.03423387555726265,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.03423387555726265
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.33125,
      "acc_stderr": 0.03732598513993524,
      "acc_norm": 0.33125,
      "acc_norm_stderr": 0.03732598513993524
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5801526717557252,
      "acc_stderr": 0.043285772152629715,
      "acc_norm": 0.5801526717557252,
      "acc_norm_stderr": 0.043285772152629715
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.041129758751770676,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.041129758751770676
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.38317757009345793,
      "acc_stderr": 0.04722013080771233,
      "acc_norm": 0.38317757009345793,
      "acc_norm_stderr": 0.04722013080771233
    },
    "Cmmlu-chinese_history": {
      "acc": 0.43343653250773995,
      "acc_stderr": 0.02761589383909775,
      "acc_norm": 0.43343653250773995,
      "acc_norm_stderr": 0.02761589383909775
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.29901960784313725,
      "acc_stderr": 0.03213325717373617,
      "acc_norm": 0.29901960784313725,
      "acc_norm_stderr": 0.03213325717373617
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.4581005586592179,
      "acc_stderr": 0.037344767605407,
      "acc_norm": 0.4581005586592179,
      "acc_norm_stderr": 0.037344767605407
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.379746835443038,
      "acc_stderr": 0.03159188752965849,
      "acc_norm": 0.379746835443038,
      "acc_norm_stderr": 0.03159188752965849
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.20754716981132076,
      "acc_stderr": 0.039577692383779325,
      "acc_norm": 0.20754716981132076,
      "acc_norm_stderr": 0.039577692383779325
    },
    "Cmmlu-college_education": {
      "acc": 0.411214953271028,
      "acc_stderr": 0.04779251692801369,
      "acc_norm": 0.411214953271028,
      "acc_norm_stderr": 0.04779251692801369
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3867924528301887,
      "acc_stderr": 0.04752784159123842,
      "acc_norm": 0.3867924528301887,
      "acc_norm_stderr": 0.04752784159123842
    },
    "Cmmlu-college_law": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.041331194402438376,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.041331194402438376
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.20952380952380953,
      "acc_stderr": 0.039906571509931876,
      "acc_norm": 0.20952380952380953,
      "acc_norm_stderr": 0.039906571509931876
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.04730439022852895,
      "acc_norm": 0.37735849056603776,
      "acc_norm_stderr": 0.04730439022852895
    },
    "Cmmlu-college_medicine": {
      "acc": 0.32967032967032966,
      "acc_stderr": 0.028503599063602895,
      "acc_norm": 0.32967032967032966,
      "acc_norm_stderr": 0.028503599063602895
    },
    "Cmmlu-computer_science": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.03441190023482465,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.03441190023482465
    },
    "Cmmlu-computer_security": {
      "acc": 0.43859649122807015,
      "acc_stderr": 0.038057975055904594,
      "acc_norm": 0.43859649122807015,
      "acc_norm_stderr": 0.038057975055904594
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.4013605442176871,
      "acc_stderr": 0.0405670641901363,
      "acc_norm": 0.4013605442176871,
      "acc_norm_stderr": 0.0405670641901363
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.31654676258992803,
      "acc_stderr": 0.03959440284735793,
      "acc_norm": 0.31654676258992803,
      "acc_norm_stderr": 0.03959440284735793
    },
    "Cmmlu-economics": {
      "acc": 0.36477987421383645,
      "acc_stderr": 0.03829561213441045,
      "acc_norm": 0.36477987421383645,
      "acc_norm_stderr": 0.03829561213441045
    },
    "Cmmlu-education": {
      "acc": 0.4049079754601227,
      "acc_stderr": 0.038566721635489125,
      "acc_norm": 0.4049079754601227,
      "acc_norm_stderr": 0.038566721635489125
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.37209302325581395,
      "acc_stderr": 0.03696369368553606,
      "acc_norm": 0.37209302325581395,
      "acc_norm_stderr": 0.03696369368553606
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.3531746031746032,
      "acc_stderr": 0.030168339307987273,
      "acc_norm": 0.3531746031746032,
      "acc_norm_stderr": 0.030168339307987273
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.3484848484848485,
      "acc_stderr": 0.033948539651564025,
      "acc_norm": 0.3484848484848485,
      "acc_norm_stderr": 0.033948539651564025
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.4957983193277311,
      "acc_stderr": 0.0324773433444811,
      "acc_norm": 0.4957983193277311,
      "acc_norm_stderr": 0.0324773433444811
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.3130434782608696,
      "acc_stderr": 0.030644265367425528,
      "acc_norm": 0.3130434782608696,
      "acc_norm_stderr": 0.030644265367425528
    },
    "Cmmlu-ethnology": {
      "acc": 0.35555555555555557,
      "acc_stderr": 0.04135176749720386,
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.04135176749720386
    },
    "Cmmlu-food_science": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.041785323616083794,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.041785323616083794
    },
    "Cmmlu-genetics": {
      "acc": 0.3522727272727273,
      "acc_stderr": 0.03610909287087525,
      "acc_norm": 0.3522727272727273,
      "acc_norm_stderr": 0.03610909287087525
    },
    "Cmmlu-global_facts": {
      "acc": 0.40268456375838924,
      "acc_stderr": 0.040313778231912095,
      "acc_norm": 0.40268456375838924,
      "acc_norm_stderr": 0.040313778231912095
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.28994082840236685,
      "acc_stderr": 0.03500638924911012,
      "acc_norm": 0.28994082840236685,
      "acc_norm_stderr": 0.03500638924911012
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.26515151515151514,
      "acc_stderr": 0.03856650735812559,
      "acc_norm": 0.26515151515151514,
      "acc_norm_stderr": 0.03856650735812559
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3389830508474576,
      "acc_stderr": 0.04376252368595952,
      "acc_norm": 0.3389830508474576,
      "acc_norm_stderr": 0.04376252368595952
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.25,
      "acc_stderr": 0.03391617237346009,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03391617237346009
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2818181818181818,
      "acc_stderr": 0.043091187099464585,
      "acc_norm": 0.2818181818181818,
      "acc_norm_stderr": 0.043091187099464585
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.3916083916083916,
      "acc_stderr": 0.04096127157727561,
      "acc_norm": 0.3916083916083916,
      "acc_norm_stderr": 0.04096127157727561
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.4126984126984127,
      "acc_stderr": 0.04403438954768176,
      "acc_norm": 0.4126984126984127,
      "acc_norm_stderr": 0.04403438954768176
    },
    "Cmmlu-international_law": {
      "acc": 0.3891891891891892,
      "acc_stderr": 0.03594386960243731,
      "acc_norm": 0.3891891891891892,
      "acc_norm_stderr": 0.03594386960243731
    },
    "Cmmlu-journalism": {
      "acc": 0.4186046511627907,
      "acc_stderr": 0.03772591189087504,
      "acc_norm": 0.4186046511627907,
      "acc_norm_stderr": 0.03772591189087504
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.40875912408759124,
      "acc_stderr": 0.024278621659089537,
      "acc_norm": 0.40875912408759124,
      "acc_norm_stderr": 0.024278621659089537
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.6542056074766355,
      "acc_stderr": 0.032589395336056405,
      "acc_norm": 0.6542056074766355,
      "acc_norm_stderr": 0.032589395336056405
    },
    "Cmmlu-logical": {
      "acc": 0.3252032520325203,
      "acc_stderr": 0.04241153733573297,
      "acc_norm": 0.3252032520325203,
      "acc_norm_stderr": 0.04241153733573297
    },
    "Cmmlu-machine_learning": {
      "acc": 0.28688524590163933,
      "acc_stderr": 0.041118866352671826,
      "acc_norm": 0.28688524590163933,
      "acc_norm_stderr": 0.041118866352671826
    },
    "Cmmlu-management": {
      "acc": 0.3761904761904762,
      "acc_stderr": 0.033508636451125215,
      "acc_norm": 0.3761904761904762,
      "acc_norm_stderr": 0.033508636451125215
    },
    "Cmmlu-marketing": {
      "acc": 0.4,
      "acc_stderr": 0.03661669186184115,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.03661669186184115
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4603174603174603,
      "acc_stderr": 0.03635121936293256,
      "acc_norm": 0.4603174603174603,
      "acc_norm_stderr": 0.03635121936293256
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.33620689655172414,
      "acc_stderr": 0.044052519965701635,
      "acc_norm": 0.33620689655172414,
      "acc_norm_stderr": 0.044052519965701635
    },
    "Cmmlu-nutrition": {
      "acc": 0.3931034482758621,
      "acc_stderr": 0.0407032901370707,
      "acc_norm": 0.3931034482758621,
      "acc_norm_stderr": 0.0407032901370707
    },
    "Cmmlu-philosophy": {
      "acc": 0.4095238095238095,
      "acc_stderr": 0.04821965555951261,
      "acc_norm": 0.4095238095238095,
      "acc_norm_stderr": 0.04821965555951261
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.3485714285714286,
      "acc_stderr": 0.036124735035030504,
      "acc_norm": 0.3485714285714286,
      "acc_norm_stderr": 0.036124735035030504
    },
    "Cmmlu-professional_law": {
      "acc": 0.3080568720379147,
      "acc_stderr": 0.03185965022587206,
      "acc_norm": 0.3080568720379147,
      "acc_norm_stderr": 0.03185965022587206
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.29521276595744683,
      "acc_stderr": 0.023554882793068655,
      "acc_norm": 0.29521276595744683,
      "acc_norm_stderr": 0.023554882793068655
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.38362068965517243,
      "acc_stderr": 0.03199403733163038,
      "acc_norm": 0.38362068965517243,
      "acc_norm_stderr": 0.03199403733163038
    },
    "Cmmlu-public_relations": {
      "acc": 0.4367816091954023,
      "acc_stderr": 0.0377092186843057,
      "acc_norm": 0.4367816091954023,
      "acc_norm_stderr": 0.0377092186843057
    },
    "Cmmlu-security_study": {
      "acc": 0.4888888888888889,
      "acc_stderr": 0.04318275491977976,
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.04318275491977976
    },
    "Cmmlu-sociology": {
      "acc": 0.3805309734513274,
      "acc_stderr": 0.032367827078100764,
      "acc_norm": 0.3805309734513274,
      "acc_norm_stderr": 0.032367827078100764
    },
    "Cmmlu-sports_science": {
      "acc": 0.42424242424242425,
      "acc_stderr": 0.03859268142070262,
      "acc_norm": 0.42424242424242425,
      "acc_norm_stderr": 0.03859268142070262
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3081081081081081,
      "acc_stderr": 0.03403782277834384,
      "acc_norm": 0.3081081081081081,
      "acc_norm_stderr": 0.03403782277834384
    },
    "Cmmlu-virology": {
      "acc": 0.41420118343195267,
      "acc_stderr": 0.03800364668244123,
      "acc_norm": 0.41420118343195267,
      "acc_norm_stderr": 0.03800364668244123
    },
    "Cmmlu-world_history": {
      "acc": 0.40993788819875776,
      "acc_stderr": 0.03888193796754317,
      "acc_norm": 0.40993788819875776,
      "acc_norm_stderr": 0.03888193796754317
    },
    "Cmmlu-world_religions": {
      "acc": 0.44375,
      "acc_stderr": 0.039400853796259426,
      "acc_norm": 0.44375,
      "acc_norm_stderr": 0.039400853796259426
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/ziya_llama_13b/Ziya-LLaMA-13B,load_in_8bit=True,dtype='float16',use_accelerate=False,peft=/home/finetuned_models/my_ziya_llama13b/final",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:44:25.643421",
    "model_name": "ziya_llama_13b"
  }
}
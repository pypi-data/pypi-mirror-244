{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.34911242603550297,
      "acc_stderr": 0.03677739827593945,
      "acc_norm": 0.34911242603550297,
      "acc_norm_stderr": 0.03677739827593945
    },
    "Cmmlu-anatomy": {
      "acc": 0.27702702702702703,
      "acc_stderr": 0.036911647897386525,
      "acc_norm": 0.27702702702702703,
      "acc_norm_stderr": 0.036911647897386525
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.22560975609756098,
      "acc_stderr": 0.032738974545663414,
      "acc_norm": 0.22560975609756098,
      "acc_norm_stderr": 0.032738974545663414
    },
    "Cmmlu-arts": {
      "acc": 0.40625,
      "acc_stderr": 0.03894932504400619,
      "acc_norm": 0.40625,
      "acc_norm_stderr": 0.03894932504400619
    },
    "Cmmlu-astronomy": {
      "acc": 0.24242424242424243,
      "acc_stderr": 0.03346409881055953,
      "acc_norm": 0.24242424242424243,
      "acc_norm_stderr": 0.03346409881055953
    },
    "Cmmlu-business_ethics": {
      "acc": 0.35406698564593303,
      "acc_stderr": 0.03315925698294869,
      "acc_norm": 0.35406698564593303,
      "acc_norm_stderr": 0.03315925698294869
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.24375,
      "acc_stderr": 0.034049163262375844,
      "acc_norm": 0.24375,
      "acc_norm_stderr": 0.034049163262375844
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.4198473282442748,
      "acc_stderr": 0.04328577215262972,
      "acc_norm": 0.4198473282442748,
      "acc_norm_stderr": 0.04328577215262972
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.34558823529411764,
      "acc_stderr": 0.04092966025145302,
      "acc_norm": 0.34558823529411764,
      "acc_norm_stderr": 0.04092966025145302
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.2897196261682243,
      "acc_stderr": 0.0440606533474851,
      "acc_norm": 0.2897196261682243,
      "acc_norm_stderr": 0.0440606533474851
    },
    "Cmmlu-chinese_history": {
      "acc": 0.33436532507739936,
      "acc_stderr": 0.02629060919555796,
      "acc_norm": 0.33436532507739936,
      "acc_norm_stderr": 0.02629060919555796
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.3235294117647059,
      "acc_stderr": 0.03283472056108567,
      "acc_norm": 0.3235294117647059,
      "acc_norm_stderr": 0.03283472056108567
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.4022346368715084,
      "acc_stderr": 0.03675319551744534,
      "acc_norm": 0.4022346368715084,
      "acc_norm_stderr": 0.03675319551744534
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.28270042194092826,
      "acc_stderr": 0.029312814153955934,
      "acc_norm": 0.28270042194092826,
      "acc_norm_stderr": 0.029312814153955934
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2169811320754717,
      "acc_stderr": 0.040225592469367126,
      "acc_norm": 0.2169811320754717,
      "acc_norm_stderr": 0.040225592469367126
    },
    "Cmmlu-college_education": {
      "acc": 0.42990654205607476,
      "acc_stderr": 0.048084723494299535,
      "acc_norm": 0.42990654205607476,
      "acc_norm_stderr": 0.048084723494299535
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3490566037735849,
      "acc_stderr": 0.046518413265290263,
      "acc_norm": 0.3490566037735849,
      "acc_norm_stderr": 0.046518413265290263
    },
    "Cmmlu-college_law": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.04489931073591312,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.04489931073591312
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.04176466758604903,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.04176466758604903
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.29245283018867924,
      "acc_stderr": 0.044392639061996274,
      "acc_norm": 0.29245283018867924,
      "acc_norm_stderr": 0.044392639061996274
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2893772893772894,
      "acc_stderr": 0.027495860234525275,
      "acc_norm": 0.2893772893772894,
      "acc_norm_stderr": 0.027495860234525275
    },
    "Cmmlu-computer_science": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.031980016601150726,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.031980016601150726
    },
    "Cmmlu-computer_security": {
      "acc": 0.30994152046783624,
      "acc_stderr": 0.035469769593931624,
      "acc_norm": 0.30994152046783624,
      "acc_norm_stderr": 0.035469769593931624
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.272108843537415,
      "acc_stderr": 0.036832239154550236,
      "acc_norm": 0.272108843537415,
      "acc_norm_stderr": 0.036832239154550236
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2949640287769784,
      "acc_stderr": 0.038819561267357056,
      "acc_norm": 0.2949640287769784,
      "acc_norm_stderr": 0.038819561267357056
    },
    "Cmmlu-economics": {
      "acc": 0.27672955974842767,
      "acc_stderr": 0.03559177035707934,
      "acc_norm": 0.27672955974842767,
      "acc_norm_stderr": 0.03559177035707934
    },
    "Cmmlu-education": {
      "acc": 0.4049079754601227,
      "acc_stderr": 0.038566721635489125,
      "acc_norm": 0.4049079754601227,
      "acc_norm_stderr": 0.038566721635489125
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3372093023255814,
      "acc_stderr": 0.03615263198871636,
      "acc_norm": 0.3372093023255814,
      "acc_norm_stderr": 0.03615263198871636
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.25396825396825395,
      "acc_stderr": 0.027474608338697408,
      "acc_norm": 0.25396825396825395,
      "acc_norm_stderr": 0.027474608338697408
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.3787878787878788,
      "acc_stderr": 0.03456088731993747,
      "acc_norm": 0.3787878787878788,
      "acc_norm_stderr": 0.03456088731993747
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.3949579831932773,
      "acc_stderr": 0.031753678460966245,
      "acc_norm": 0.3949579831932773,
      "acc_norm_stderr": 0.031753678460966245
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.2565217391304348,
      "acc_stderr": 0.028858814315305646,
      "acc_norm": 0.2565217391304348,
      "acc_norm_stderr": 0.028858814315305646
    },
    "Cmmlu-ethnology": {
      "acc": 0.2962962962962963,
      "acc_stderr": 0.03944624162501116,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.03944624162501116
    },
    "Cmmlu-food_science": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.04036845779880778,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.04036845779880778
    },
    "Cmmlu-genetics": {
      "acc": 0.30113636363636365,
      "acc_stderr": 0.03467837977202437,
      "acc_norm": 0.30113636363636365,
      "acc_norm_stderr": 0.03467837977202437
    },
    "Cmmlu-global_facts": {
      "acc": 0.31543624161073824,
      "acc_stderr": 0.03819723167141383,
      "acc_norm": 0.31543624161073824,
      "acc_norm_stderr": 0.03819723167141383
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.28994082840236685,
      "acc_stderr": 0.03500638924911012,
      "acc_norm": 0.28994082840236685,
      "acc_norm_stderr": 0.03500638924911012
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.26515151515151514,
      "acc_stderr": 0.038566507358125585,
      "acc_norm": 0.26515151515151514,
      "acc_norm_stderr": 0.038566507358125585
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2966101694915254,
      "acc_stderr": 0.04222776832233627,
      "acc_norm": 0.2966101694915254,
      "acc_norm_stderr": 0.04222776832233627
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364997,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364997
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.04172343038705383,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.04172343038705383
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.23776223776223776,
      "acc_stderr": 0.0357250214181557,
      "acc_norm": 0.23776223776223776,
      "acc_norm_stderr": 0.0357250214181557
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.0436031486007746,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.0436031486007746
    },
    "Cmmlu-international_law": {
      "acc": 0.2810810810810811,
      "acc_stderr": 0.03313956873549873,
      "acc_norm": 0.2810810810810811,
      "acc_norm_stderr": 0.03313956873549873
    },
    "Cmmlu-journalism": {
      "acc": 0.36046511627906974,
      "acc_stderr": 0.036716872822364986,
      "acc_norm": 0.36046511627906974,
      "acc_norm_stderr": 0.036716872822364986
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.30900243309002434,
      "acc_stderr": 0.02282061164153643,
      "acc_norm": 0.30900243309002434,
      "acc_norm_stderr": 0.02282061164153643
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.45794392523364486,
      "acc_stderr": 0.034138030131983854,
      "acc_norm": 0.45794392523364486,
      "acc_norm_stderr": 0.034138030131983854
    },
    "Cmmlu-logical": {
      "acc": 0.3252032520325203,
      "acc_stderr": 0.042411537335732975,
      "acc_norm": 0.3252032520325203,
      "acc_norm_stderr": 0.042411537335732975
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2786885245901639,
      "acc_stderr": 0.04075944659069252,
      "acc_norm": 0.2786885245901639,
      "acc_norm_stderr": 0.04075944659069252
    },
    "Cmmlu-management": {
      "acc": 0.4,
      "acc_stderr": 0.03388694968349424,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.03388694968349424
    },
    "Cmmlu-marketing": {
      "acc": 0.3277777777777778,
      "acc_stderr": 0.03508485373860691,
      "acc_norm": 0.3277777777777778,
      "acc_norm_stderr": 0.03508485373860691
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4021164021164021,
      "acc_stderr": 0.03576064052818363,
      "acc_norm": 0.4021164021164021,
      "acc_norm_stderr": 0.03576064052818363
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.041678081808441514,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.041678081808441514
    },
    "Cmmlu-nutrition": {
      "acc": 0.36551724137931035,
      "acc_stderr": 0.04013124195424385,
      "acc_norm": 0.36551724137931035,
      "acc_norm_stderr": 0.04013124195424385
    },
    "Cmmlu-philosophy": {
      "acc": 0.38095238095238093,
      "acc_stderr": 0.04761904761904764,
      "acc_norm": 0.38095238095238093,
      "acc_norm_stderr": 0.04761904761904764
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.3485714285714286,
      "acc_stderr": 0.036124735035030504,
      "acc_norm": 0.3485714285714286,
      "acc_norm_stderr": 0.036124735035030504
    },
    "Cmmlu-professional_law": {
      "acc": 0.24644549763033174,
      "acc_stderr": 0.029737751726596835,
      "acc_norm": 0.24644549763033174,
      "acc_norm_stderr": 0.029737751726596835
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.30319148936170215,
      "acc_stderr": 0.02373556600773539,
      "acc_norm": 0.30319148936170215,
      "acc_norm_stderr": 0.02373556600773539
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.3922413793103448,
      "acc_stderr": 0.032124492663102744,
      "acc_norm": 0.3922413793103448,
      "acc_norm_stderr": 0.032124492663102744
    },
    "Cmmlu-public_relations": {
      "acc": 0.3160919540229885,
      "acc_stderr": 0.035349438976908586,
      "acc_norm": 0.3160919540229885,
      "acc_norm_stderr": 0.035349438976908586
    },
    "Cmmlu-security_study": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04072314811876837,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04072314811876837
    },
    "Cmmlu-sociology": {
      "acc": 0.3584070796460177,
      "acc_stderr": 0.03196883516493523,
      "acc_norm": 0.3584070796460177,
      "acc_norm_stderr": 0.03196883516493523
    },
    "Cmmlu-sports_science": {
      "acc": 0.3090909090909091,
      "acc_stderr": 0.036085410115739666,
      "acc_norm": 0.3090909090909091,
      "acc_norm_stderr": 0.036085410115739666
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2918918918918919,
      "acc_stderr": 0.03351597731741764,
      "acc_norm": 0.2918918918918919,
      "acc_norm_stderr": 0.03351597731741764
    },
    "Cmmlu-virology": {
      "acc": 0.3136094674556213,
      "acc_stderr": 0.035795265164562266,
      "acc_norm": 0.3136094674556213,
      "acc_norm_stderr": 0.035795265164562266
    },
    "Cmmlu-world_history": {
      "acc": 0.2732919254658385,
      "acc_stderr": 0.035231683977370906,
      "acc_norm": 0.2732919254658385,
      "acc_norm_stderr": 0.035231683977370906
    },
    "Cmmlu-world_religions": {
      "acc": 0.3625,
      "acc_stderr": 0.038123743406448904,
      "acc_norm": 0.3625,
      "acc_norm_stderr": 0.038123743406448904
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/baichuan_7b,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_baichuan_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "4:34:11.124392",
    "model_name": "baichuan_7b"
  }
}
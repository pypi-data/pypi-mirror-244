{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816506,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816506
    },
    "hendrycksTest-anatomy": {
      "acc": 0.34814814814814815,
      "acc_stderr": 0.041153246103369526,
      "acc_norm": 0.34814814814814815,
      "acc_norm_stderr": 0.041153246103369526
    },
    "hendrycksTest-astronomy": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.04063302731486671,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.04063302731486671
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.58,
      "acc_stderr": 0.04960449637488584,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.04960449637488584
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4981132075471698,
      "acc_stderr": 0.03077265364207567,
      "acc_norm": 0.4981132075471698,
      "acc_norm_stderr": 0.03077265364207567
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04155319955593146,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04155319955593146
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4508670520231214,
      "acc_stderr": 0.03794012674697028,
      "acc_norm": 0.4508670520231214,
      "acc_norm_stderr": 0.03794012674697028
    },
    "hendrycksTest-college_physics": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.04533838195929775,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.04533838195929775
    },
    "hendrycksTest-computer_security": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4297872340425532,
      "acc_stderr": 0.03236214467715564,
      "acc_norm": 0.4297872340425532,
      "acc_norm_stderr": 0.03236214467715564
    },
    "hendrycksTest-econometrics": {
      "acc": 0.21929824561403508,
      "acc_stderr": 0.03892431106518754,
      "acc_norm": 0.21929824561403508,
      "acc_norm_stderr": 0.03892431106518754
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.04144311810878151,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.04144311810878151
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.31216931216931215,
      "acc_stderr": 0.023865206836972602,
      "acc_norm": 0.31216931216931215,
      "acc_norm_stderr": 0.023865206836972602
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.3412698412698413,
      "acc_stderr": 0.04240799327574925,
      "acc_norm": 0.3412698412698413,
      "acc_norm_stderr": 0.04240799327574925
    },
    "hendrycksTest-global_facts": {
      "acc": 0.16,
      "acc_stderr": 0.03684529491774708,
      "acc_norm": 0.16,
      "acc_norm_stderr": 0.03684529491774708
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5096774193548387,
      "acc_stderr": 0.02843867799890955,
      "acc_norm": 0.5096774193548387,
      "acc_norm_stderr": 0.02843867799890955
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.39408866995073893,
      "acc_stderr": 0.03438157967036544,
      "acc_norm": 0.39408866995073893,
      "acc_norm_stderr": 0.03438157967036544
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.44,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.03401506715249039,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.03401506715249039
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5404040404040404,
      "acc_stderr": 0.035507024651313425,
      "acc_norm": 0.5404040404040404,
      "acc_norm_stderr": 0.035507024651313425
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6010362694300518,
      "acc_stderr": 0.03533999094065696,
      "acc_norm": 0.6010362694300518,
      "acc_norm_stderr": 0.03533999094065696
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4307692307692308,
      "acc_stderr": 0.02510682066053975,
      "acc_norm": 0.4307692307692308,
      "acc_norm_stderr": 0.02510682066053975
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085626,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085626
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4369747899159664,
      "acc_stderr": 0.032219436365661956,
      "acc_norm": 0.4369747899159664,
      "acc_norm_stderr": 0.032219436365661956
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.31788079470198677,
      "acc_stderr": 0.03802039760107903,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.03802039760107903
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5688073394495413,
      "acc_stderr": 0.02123336503031956,
      "acc_norm": 0.5688073394495413,
      "acc_norm_stderr": 0.02123336503031956
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3287037037037037,
      "acc_stderr": 0.032036140846700596,
      "acc_norm": 0.3287037037037037,
      "acc_norm_stderr": 0.032036140846700596
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.029771775228145638,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.029771775228145638
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.03068582059661081,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.03068582059661081
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4484304932735426,
      "acc_stderr": 0.033378837362550984,
      "acc_norm": 0.4484304932735426,
      "acc_norm_stderr": 0.033378837362550984
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4732824427480916,
      "acc_stderr": 0.04379024936553894,
      "acc_norm": 0.4732824427480916,
      "acc_norm_stderr": 0.04379024936553894
    },
    "hendrycksTest-international_law": {
      "acc": 0.628099173553719,
      "acc_stderr": 0.044120158066245044,
      "acc_norm": 0.628099173553719,
      "acc_norm_stderr": 0.044120158066245044
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5648148148148148,
      "acc_stderr": 0.04792898170907061,
      "acc_norm": 0.5648148148148148,
      "acc_norm_stderr": 0.04792898170907061
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.4294478527607362,
      "acc_stderr": 0.03889066619112722,
      "acc_norm": 0.4294478527607362,
      "acc_norm_stderr": 0.03889066619112722
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.32142857142857145,
      "acc_stderr": 0.0443280405529152,
      "acc_norm": 0.32142857142857145,
      "acc_norm_stderr": 0.0443280405529152
    },
    "hendrycksTest-management": {
      "acc": 0.6116504854368932,
      "acc_stderr": 0.048257293373563895,
      "acc_norm": 0.6116504854368932,
      "acc_norm_stderr": 0.048257293373563895
    },
    "hendrycksTest-marketing": {
      "acc": 0.6709401709401709,
      "acc_stderr": 0.03078232157768817,
      "acc_norm": 0.6709401709401709,
      "acc_norm_stderr": 0.03078232157768817
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956913,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956913
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.5772669220945083,
      "acc_stderr": 0.01766518035195406,
      "acc_norm": 0.5772669220945083,
      "acc_norm_stderr": 0.01766518035195406
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.4682080924855491,
      "acc_stderr": 0.026864624366756656,
      "acc_norm": 0.4682080924855491,
      "acc_norm_stderr": 0.026864624366756656
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2659217877094972,
      "acc_stderr": 0.014776765066438885,
      "acc_norm": 0.2659217877094972,
      "acc_norm_stderr": 0.014776765066438885
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5130718954248366,
      "acc_stderr": 0.028620130800700246,
      "acc_norm": 0.5130718954248366,
      "acc_norm_stderr": 0.028620130800700246
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5176848874598071,
      "acc_stderr": 0.02838032284907713,
      "acc_norm": 0.5176848874598071,
      "acc_norm_stderr": 0.02838032284907713
    },
    "hendrycksTest-prehistory": {
      "acc": 0.4876543209876543,
      "acc_stderr": 0.027812262269327235,
      "acc_norm": 0.4876543209876543,
      "acc_norm_stderr": 0.027812262269327235
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3404255319148936,
      "acc_stderr": 0.028267657482650137,
      "acc_norm": 0.3404255319148936,
      "acc_norm_stderr": 0.028267657482650137
    },
    "hendrycksTest-professional_law": {
      "acc": 0.34810951760104303,
      "acc_stderr": 0.012166738993698205,
      "acc_norm": 0.34810951760104303,
      "acc_norm_stderr": 0.012166738993698205
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.3860294117647059,
      "acc_stderr": 0.029573269134411127,
      "acc_norm": 0.3860294117647059,
      "acc_norm_stderr": 0.029573269134411127
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.019910377463105932,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.019910377463105932
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5818181818181818,
      "acc_stderr": 0.04724577405731572,
      "acc_norm": 0.5818181818181818,
      "acc_norm_stderr": 0.04724577405731572
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5836734693877551,
      "acc_stderr": 0.031557828165561644,
      "acc_norm": 0.5836734693877551,
      "acc_norm_stderr": 0.031557828165561644
    },
    "hendrycksTest-sociology": {
      "acc": 0.6119402985074627,
      "acc_stderr": 0.03445789964362749,
      "acc_norm": 0.6119402985074627,
      "acc_norm_stderr": 0.03445789964362749
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.64,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.64,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-virology": {
      "acc": 0.39156626506024095,
      "acc_stderr": 0.03799857454479637,
      "acc_norm": 0.39156626506024095,
      "acc_norm_stderr": 0.03799857454479637
    },
    "hendrycksTest-world_religions": {
      "acc": 0.49122807017543857,
      "acc_stderr": 0.03834234744164993,
      "acc_norm": 0.49122807017543857,
      "acc_norm_stderr": 0.03834234744164993
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained='/home/sdk_models/chatglm2_6b',add_special_tokens=True,trust_remote_code=True,dtype='float16',use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_chatglm2_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:54:05.493926",
    "model_name": "chatglm2_6b"
  }
}
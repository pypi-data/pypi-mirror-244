{
  "results": {
    "Ceval-valid-accountant": {
      "acc": 0.46938775510204084,
      "acc_stderr": 0.07203339654607953,
      "acc_norm": 0.46938775510204084,
      "acc_norm_stderr": 0.07203339654607953
    },
    "Ceval-valid-advanced_mathematics": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-art_studies": {
      "acc": 0.696969696969697,
      "acc_stderr": 0.08124094920275463,
      "acc_norm": 0.696969696969697,
      "acc_norm_stderr": 0.08124094920275463
    },
    "Ceval-valid-basic_medicine": {
      "acc": 0.7894736842105263,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.7894736842105263,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-business_administration": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.08802234877744129,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.08802234877744129
    },
    "Ceval-valid-chinese_language_and_literature": {
      "acc": 0.5217391304347826,
      "acc_stderr": 0.10649955403405124,
      "acc_norm": 0.5217391304347826,
      "acc_norm_stderr": 0.10649955403405124
    },
    "Ceval-valid-civil_servant": {
      "acc": 0.46808510638297873,
      "acc_stderr": 0.07357064625618347,
      "acc_norm": 0.46808510638297873,
      "acc_norm_stderr": 0.07357064625618347
    },
    "Ceval-valid-clinical_medicine": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.10865714630312667,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.10865714630312667
    },
    "Ceval-valid-college_chemistry": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.10279899245732686,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.10279899245732686
    },
    "Ceval-valid-college_economics": {
      "acc": 0.5272727272727272,
      "acc_stderr": 0.06794008776080905,
      "acc_norm": 0.5272727272727272,
      "acc_norm_stderr": 0.06794008776080905
    },
    "Ceval-valid-college_physics": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.11768778828946262,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.11768778828946262
    },
    "Ceval-valid-college_programming": {
      "acc": 0.6216216216216216,
      "acc_stderr": 0.08083044344561426,
      "acc_norm": 0.6216216216216216,
      "acc_norm_stderr": 0.08083044344561426
    },
    "Ceval-valid-computer_architecture": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.11065666703449763,
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.11065666703449763
    },
    "Ceval-valid-computer_network": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-discrete_mathematics": {
      "acc": 0.25,
      "acc_stderr": 0.11180339887498948,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.11180339887498948
    },
    "Ceval-valid-education_science": {
      "acc": 0.6551724137931034,
      "acc_stderr": 0.08982552969857373,
      "acc_norm": 0.6551724137931034,
      "acc_norm_stderr": 0.08982552969857373
    },
    "Ceval-valid-electrical_engineer": {
      "acc": 0.2972972972972973,
      "acc_stderr": 0.07617808344724214,
      "acc_norm": 0.2972972972972973,
      "acc_norm_stderr": 0.07617808344724214
    },
    "Ceval-valid-environmental_impact_assessment_engineer": {
      "acc": 0.6774193548387096,
      "acc_stderr": 0.08534681648595453,
      "acc_norm": 0.6774193548387096,
      "acc_norm_stderr": 0.08534681648595453
    },
    "Ceval-valid-fire_engineer": {
      "acc": 0.5483870967741935,
      "acc_stderr": 0.09085862440549507,
      "acc_norm": 0.5483870967741935,
      "acc_norm_stderr": 0.09085862440549507
    },
    "Ceval-valid-high_school_biology": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.10956136839295433,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.10956136839295433
    },
    "Ceval-valid-high_school_chemistry": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.11768778828946262,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.11768778828946262
    },
    "Ceval-valid-high_school_chinese": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-high_school_geography": {
      "acc": 0.7368421052631579,
      "acc_stderr": 0.10379087338771255,
      "acc_norm": 0.7368421052631579,
      "acc_norm_stderr": 0.10379087338771255
    },
    "Ceval-valid-high_school_history": {
      "acc": 0.75,
      "acc_stderr": 0.09933992677987828,
      "acc_norm": 0.75,
      "acc_norm_stderr": 0.09933992677987828
    },
    "Ceval-valid-high_school_mathematics": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.10083169033033672,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.10083169033033672
    },
    "Ceval-valid-high_school_physics": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-high_school_politics": {
      "acc": 0.7894736842105263,
      "acc_stderr": 0.09609167675529229,
      "acc_norm": 0.7894736842105263,
      "acc_norm_stderr": 0.09609167675529229
    },
    "Ceval-valid-ideological_and_moral_cultivation": {
      "acc": 0.9473684210526315,
      "acc_stderr": 0.052631578947368404,
      "acc_norm": 0.9473684210526315,
      "acc_norm_stderr": 0.052631578947368404
    },
    "Ceval-valid-law": {
      "acc": 0.625,
      "acc_stderr": 0.10094660663590604,
      "acc_norm": 0.625,
      "acc_norm_stderr": 0.10094660663590604
    },
    "Ceval-valid-legal_professional": {
      "acc": 0.6956521739130435,
      "acc_stderr": 0.09810018692482896,
      "acc_norm": 0.6956521739130435,
      "acc_norm_stderr": 0.09810018692482896
    },
    "Ceval-valid-logic": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.10729033533674223,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.10729033533674223
    },
    "Ceval-valid-mao_zedong_thought": {
      "acc": 0.7916666666666666,
      "acc_stderr": 0.08468112965594378,
      "acc_norm": 0.7916666666666666,
      "acc_norm_stderr": 0.08468112965594378
    },
    "Ceval-valid-marxism": {
      "acc": 0.7368421052631579,
      "acc_stderr": 0.10379087338771255,
      "acc_norm": 0.7368421052631579,
      "acc_norm_stderr": 0.10379087338771255
    },
    "Ceval-valid-metrology_engineer": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.09829463743659808,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.09829463743659808
    },
    "Ceval-valid-middle_school_biology": {
      "acc": 0.8571428571428571,
      "acc_stderr": 0.07824607964359516,
      "acc_norm": 0.8571428571428571,
      "acc_norm_stderr": 0.07824607964359516
    },
    "Ceval-valid-middle_school_chemistry": {
      "acc": 0.85,
      "acc_stderr": 0.08191780219091253,
      "acc_norm": 0.85,
      "acc_norm_stderr": 0.08191780219091253
    },
    "Ceval-valid-middle_school_geography": {
      "acc": 0.5,
      "acc_stderr": 0.15075567228888181,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.15075567228888181
    },
    "Ceval-valid-middle_school_history": {
      "acc": 0.8636363636363636,
      "acc_stderr": 0.0748867700952649,
      "acc_norm": 0.8636363636363636,
      "acc_norm_stderr": 0.0748867700952649
    },
    "Ceval-valid-middle_school_mathematics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-middle_school_physics": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-middle_school_politics": {
      "acc": 0.8095238095238095,
      "acc_stderr": 0.08780518530755131,
      "acc_norm": 0.8095238095238095,
      "acc_norm_stderr": 0.08780518530755131
    },
    "Ceval-valid-modern_chinese_history": {
      "acc": 0.6956521739130435,
      "acc_stderr": 0.09810018692482894,
      "acc_norm": 0.6956521739130435,
      "acc_norm_stderr": 0.09810018692482894
    },
    "Ceval-valid-operating_system": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-physician": {
      "acc": 0.5918367346938775,
      "acc_stderr": 0.07094099868916398,
      "acc_norm": 0.5918367346938775,
      "acc_norm_stderr": 0.07094099868916398
    },
    "Ceval-valid-plant_protection": {
      "acc": 0.8181818181818182,
      "acc_stderr": 0.08416546361568648,
      "acc_norm": 0.8181818181818182,
      "acc_norm_stderr": 0.08416546361568648
    },
    "Ceval-valid-probability_and_statistics": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.1086324845659782,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.1086324845659782
    },
    "Ceval-valid-professional_tour_guide": {
      "acc": 0.7931034482758621,
      "acc_stderr": 0.07655305550699533,
      "acc_norm": 0.7931034482758621,
      "acc_norm_stderr": 0.07655305550699533
    },
    "Ceval-valid-sports_science": {
      "acc": 0.5789473684210527,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.5789473684210527,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-tax_accountant": {
      "acc": 0.5306122448979592,
      "acc_stderr": 0.07203339654607951,
      "acc_norm": 0.5306122448979592,
      "acc_norm_stderr": 0.07203339654607951
    },
    "Ceval-valid-teacher_qualification": {
      "acc": 0.7954545454545454,
      "acc_stderr": 0.06151320742474889,
      "acc_norm": 0.7954545454545454,
      "acc_norm_stderr": 0.06151320742474889
    },
    "Ceval-valid-urban_and_rural_planner": {
      "acc": 0.7608695652173914,
      "acc_stderr": 0.06358669845936321,
      "acc_norm": 0.7608695652173914,
      "acc_norm_stderr": 0.06358669845936321
    },
    "Ceval-valid-veterinary_medicine": {
      "acc": 0.5652173913043478,
      "acc_stderr": 0.10568965974008646,
      "acc_norm": 0.5652173913043478,
      "acc_norm_stderr": 0.10568965974008646
    }
  },
  "versions": {
    "Ceval-valid-accountant": 1,
    "Ceval-valid-advanced_mathematics": 1,
    "Ceval-valid-art_studies": 1,
    "Ceval-valid-basic_medicine": 1,
    "Ceval-valid-business_administration": 1,
    "Ceval-valid-chinese_language_and_literature": 1,
    "Ceval-valid-civil_servant": 1,
    "Ceval-valid-clinical_medicine": 1,
    "Ceval-valid-college_chemistry": 1,
    "Ceval-valid-college_economics": 1,
    "Ceval-valid-college_physics": 1,
    "Ceval-valid-college_programming": 1,
    "Ceval-valid-computer_architecture": 1,
    "Ceval-valid-computer_network": 1,
    "Ceval-valid-discrete_mathematics": 1,
    "Ceval-valid-education_science": 1,
    "Ceval-valid-electrical_engineer": 1,
    "Ceval-valid-environmental_impact_assessment_engineer": 1,
    "Ceval-valid-fire_engineer": 1,
    "Ceval-valid-high_school_biology": 1,
    "Ceval-valid-high_school_chemistry": 1,
    "Ceval-valid-high_school_chinese": 1,
    "Ceval-valid-high_school_geography": 1,
    "Ceval-valid-high_school_history": 1,
    "Ceval-valid-high_school_mathematics": 1,
    "Ceval-valid-high_school_physics": 1,
    "Ceval-valid-high_school_politics": 1,
    "Ceval-valid-ideological_and_moral_cultivation": 1,
    "Ceval-valid-law": 1,
    "Ceval-valid-legal_professional": 1,
    "Ceval-valid-logic": 1,
    "Ceval-valid-mao_zedong_thought": 1,
    "Ceval-valid-marxism": 1,
    "Ceval-valid-metrology_engineer": 1,
    "Ceval-valid-middle_school_biology": 1,
    "Ceval-valid-middle_school_chemistry": 1,
    "Ceval-valid-middle_school_geography": 1,
    "Ceval-valid-middle_school_history": 1,
    "Ceval-valid-middle_school_mathematics": 1,
    "Ceval-valid-middle_school_physics": 1,
    "Ceval-valid-middle_school_politics": 1,
    "Ceval-valid-modern_chinese_history": 1,
    "Ceval-valid-operating_system": 1,
    "Ceval-valid-physician": 1,
    "Ceval-valid-plant_protection": 1,
    "Ceval-valid-probability_and_statistics": 1,
    "Ceval-valid-professional_tour_guide": 1,
    "Ceval-valid-sports_science": 1,
    "Ceval-valid-tax_accountant": 1,
    "Ceval-valid-teacher_qualification": 1,
    "Ceval-valid-urban_and_rural_planner": 1,
    "Ceval-valid-veterinary_medicine": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/xverse_13b,dtype='bfloat16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:21:00.975867",
    "model_name": "xverse_13b"
  }
}
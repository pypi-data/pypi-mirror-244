{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.4970414201183432,
      "acc_stderr": 0.038575162160962455,
      "acc_norm": 0.4970414201183432,
      "acc_norm_stderr": 0.038575162160962455
    },
    "Cmmlu-anatomy": {
      "acc": 0.3918918918918919,
      "acc_stderr": 0.040263810066822794,
      "acc_norm": 0.3918918918918919,
      "acc_norm_stderr": 0.040263810066822794
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.3475609756097561,
      "acc_stderr": 0.03729852575607224,
      "acc_norm": 0.3475609756097561,
      "acc_norm_stderr": 0.03729852575607224
    },
    "Cmmlu-arts": {
      "acc": 0.8,
      "acc_stderr": 0.03172206342872575,
      "acc_norm": 0.8,
      "acc_norm_stderr": 0.03172206342872575
    },
    "Cmmlu-astronomy": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.037131580674819135,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.037131580674819135
    },
    "Cmmlu-business_ethics": {
      "acc": 0.569377990430622,
      "acc_stderr": 0.03433339751358729,
      "acc_norm": 0.569377990430622,
      "acc_norm_stderr": 0.03433339751358729
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.5125,
      "acc_stderr": 0.03964018591811396,
      "acc_norm": 0.5125,
      "acc_norm_stderr": 0.03964018591811396
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.732824427480916,
      "acc_stderr": 0.03880848301082394,
      "acc_norm": 0.732824427480916,
      "acc_norm_stderr": 0.03880848301082394
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.5661764705882353,
      "acc_stderr": 0.04265457074552585,
      "acc_norm": 0.5661764705882353,
      "acc_norm_stderr": 0.04265457074552585
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5607476635514018,
      "acc_stderr": 0.048204529006379074,
      "acc_norm": 0.5607476635514018,
      "acc_norm_stderr": 0.048204529006379074
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6346749226006192,
      "acc_stderr": 0.026834127317462826,
      "acc_norm": 0.6346749226006192,
      "acc_norm_stderr": 0.026834127317462826
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.553921568627451,
      "acc_stderr": 0.03488845451304974,
      "acc_norm": 0.553921568627451,
      "acc_norm_stderr": 0.03488845451304974
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.7541899441340782,
      "acc_stderr": 0.032272320235413,
      "acc_norm": 0.7541899441340782,
      "acc_norm_stderr": 0.032272320235413
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4430379746835443,
      "acc_stderr": 0.03233532777533484,
      "acc_norm": 0.4430379746835443,
      "acc_norm_stderr": 0.03233532777533484
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.25471698113207547,
      "acc_stderr": 0.042520162237633115,
      "acc_norm": 0.25471698113207547,
      "acc_norm_stderr": 0.042520162237633115
    },
    "Cmmlu-college_education": {
      "acc": 0.6822429906542056,
      "acc_stderr": 0.0452235007738203,
      "acc_norm": 0.6822429906542056,
      "acc_norm_stderr": 0.0452235007738203
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.4339622641509434,
      "acc_stderr": 0.0483675429782382,
      "acc_norm": 0.4339622641509434,
      "acc_norm_stderr": 0.0483675429782382
    },
    "Cmmlu-college_law": {
      "acc": 0.48148148148148145,
      "acc_stderr": 0.04830366024635331,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.04830366024635331
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3142857142857143,
      "acc_stderr": 0.045521571818039494,
      "acc_norm": 0.3142857142857143,
      "acc_norm_stderr": 0.045521571818039494
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.4528301886792453,
      "acc_stderr": 0.048577381460134975,
      "acc_norm": 0.4528301886792453,
      "acc_norm_stderr": 0.048577381460134975
    },
    "Cmmlu-college_medicine": {
      "acc": 0.46153846153846156,
      "acc_stderr": 0.030227124922822136,
      "acc_norm": 0.46153846153846156,
      "acc_norm_stderr": 0.030227124922822136
    },
    "Cmmlu-computer_science": {
      "acc": 0.5686274509803921,
      "acc_stderr": 0.03476099060501637,
      "acc_norm": 0.5686274509803921,
      "acc_norm_stderr": 0.03476099060501637
    },
    "Cmmlu-computer_security": {
      "acc": 0.6198830409356725,
      "acc_stderr": 0.037229657413855394,
      "acc_norm": 0.6198830409356725,
      "acc_norm_stderr": 0.037229657413855394
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.47619047619047616,
      "acc_stderr": 0.041333351369708816,
      "acc_norm": 0.47619047619047616,
      "acc_norm_stderr": 0.041333351369708816
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.4244604316546763,
      "acc_stderr": 0.04207427642081081,
      "acc_norm": 0.4244604316546763,
      "acc_norm_stderr": 0.04207427642081081
    },
    "Cmmlu-economics": {
      "acc": 0.5911949685534591,
      "acc_stderr": 0.03911064131397898,
      "acc_norm": 0.5911949685534591,
      "acc_norm_stderr": 0.03911064131397898
    },
    "Cmmlu-education": {
      "acc": 0.6871165644171779,
      "acc_stderr": 0.03642914578292404,
      "acc_norm": 0.6871165644171779,
      "acc_norm_stderr": 0.03642914578292404
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.5465116279069767,
      "acc_stderr": 0.03807016210250966,
      "acc_norm": 0.5465116279069767,
      "acc_norm_stderr": 0.03807016210250966
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.5277777777777778,
      "acc_stderr": 0.0315109792452557,
      "acc_norm": 0.5277777777777778,
      "acc_norm_stderr": 0.0315109792452557
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.6818181818181818,
      "acc_stderr": 0.03318477333845331,
      "acc_norm": 0.6818181818181818,
      "acc_norm_stderr": 0.03318477333845331
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.7142857142857143,
      "acc_stderr": 0.029344572500634342,
      "acc_norm": 0.7142857142857143,
      "acc_norm_stderr": 0.029344572500634342
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.36086956521739133,
      "acc_stderr": 0.03173599626649655,
      "acc_norm": 0.36086956521739133,
      "acc_norm_stderr": 0.03173599626649655
    },
    "Cmmlu-ethnology": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.04316378599511326,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.04316378599511326
    },
    "Cmmlu-food_science": {
      "acc": 0.5384615384615384,
      "acc_stderr": 0.041834744477373405,
      "acc_norm": 0.5384615384615384,
      "acc_norm_stderr": 0.041834744477373405
    },
    "Cmmlu-genetics": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.03763993960049208,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.03763993960049208
    },
    "Cmmlu-global_facts": {
      "acc": 0.610738255033557,
      "acc_stderr": 0.0400790636583561,
      "acc_norm": 0.610738255033557,
      "acc_norm_stderr": 0.0400790636583561
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.44970414201183434,
      "acc_stderr": 0.03838017272948937,
      "acc_norm": 0.44970414201183434,
      "acc_norm_stderr": 0.03838017272948937
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.4015151515151515,
      "acc_stderr": 0.042829391226308106,
      "acc_norm": 0.4015151515151515,
      "acc_norm_stderr": 0.042829391226308106
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.635593220338983,
      "acc_stderr": 0.04449281883842521,
      "acc_norm": 0.635593220338983,
      "acc_norm_stderr": 0.04449281883842521
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364997,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364997
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.047093069786618966,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.047093069786618966
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5104895104895105,
      "acc_stderr": 0.04194983340040537,
      "acc_norm": 0.5104895104895105,
      "acc_norm_stderr": 0.04194983340040537
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.044444444444444495,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.044444444444444495
    },
    "Cmmlu-international_law": {
      "acc": 0.4486486486486487,
      "acc_stderr": 0.03666557432850435,
      "acc_norm": 0.4486486486486487,
      "acc_norm_stderr": 0.03666557432850435
    },
    "Cmmlu-journalism": {
      "acc": 0.5930232558139535,
      "acc_stderr": 0.03756839173779933,
      "acc_norm": 0.5930232558139535,
      "acc_norm_stderr": 0.03756839173779933
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5815085158150851,
      "acc_stderr": 0.0243629247438038,
      "acc_norm": 0.5815085158150851,
      "acc_norm_stderr": 0.0243629247438038
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8551401869158879,
      "acc_stderr": 0.024115863483127636,
      "acc_norm": 0.8551401869158879,
      "acc_norm_stderr": 0.024115863483127636
    },
    "Cmmlu-logical": {
      "acc": 0.5203252032520326,
      "acc_stderr": 0.04523045598338889,
      "acc_norm": 0.5203252032520326,
      "acc_norm_stderr": 0.04523045598338889
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4918032786885246,
      "acc_stderr": 0.04544843720410404,
      "acc_norm": 0.4918032786885246,
      "acc_norm_stderr": 0.04544843720410404
    },
    "Cmmlu-management": {
      "acc": 0.6904761904761905,
      "acc_stderr": 0.03197777494209474,
      "acc_norm": 0.6904761904761905,
      "acc_norm_stderr": 0.03197777494209474
    },
    "Cmmlu-marketing": {
      "acc": 0.6222222222222222,
      "acc_stderr": 0.03623801889425291,
      "acc_norm": 0.6222222222222222,
      "acc_norm_stderr": 0.03623801889425291
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.7037037037037037,
      "acc_stderr": 0.033302673930836024,
      "acc_norm": 0.7037037037037037,
      "acc_norm_stderr": 0.033302673930836024
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.046375088285891965,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.046375088285891965
    },
    "Cmmlu-nutrition": {
      "acc": 0.5862068965517241,
      "acc_stderr": 0.041042692118062316,
      "acc_norm": 0.5862068965517241,
      "acc_norm_stderr": 0.041042692118062316
    },
    "Cmmlu-philosophy": {
      "acc": 0.6761904761904762,
      "acc_stderr": 0.045884147180674746,
      "acc_norm": 0.6761904761904762,
      "acc_norm_stderr": 0.045884147180674746
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.6514285714285715,
      "acc_stderr": 0.0361247350350305,
      "acc_norm": 0.6514285714285715,
      "acc_norm_stderr": 0.0361247350350305
    },
    "Cmmlu-professional_law": {
      "acc": 0.4028436018957346,
      "acc_stderr": 0.03384563236276238,
      "acc_norm": 0.4028436018957346,
      "acc_norm_stderr": 0.03384563236276238
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.4122340425531915,
      "acc_stderr": 0.025419002692916994,
      "acc_norm": 0.4122340425531915,
      "acc_norm_stderr": 0.025419002692916994
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.6120689655172413,
      "acc_stderr": 0.032060585704337626,
      "acc_norm": 0.6120689655172413,
      "acc_norm_stderr": 0.032060585704337626
    },
    "Cmmlu-public_relations": {
      "acc": 0.6149425287356322,
      "acc_stderr": 0.03699618907790639,
      "acc_norm": 0.6149425287356322,
      "acc_norm_stderr": 0.03699618907790639
    },
    "Cmmlu-security_study": {
      "acc": 0.7037037037037037,
      "acc_stderr": 0.03944624162501116,
      "acc_norm": 0.7037037037037037,
      "acc_norm_stderr": 0.03944624162501116
    },
    "Cmmlu-sociology": {
      "acc": 0.5973451327433629,
      "acc_stderr": 0.03269549239276309,
      "acc_norm": 0.5973451327433629,
      "acc_norm_stderr": 0.03269549239276309
    },
    "Cmmlu-sports_science": {
      "acc": 0.6181818181818182,
      "acc_stderr": 0.03793713171165635,
      "acc_norm": 0.6181818181818182,
      "acc_norm_stderr": 0.03793713171165635
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.5297297297297298,
      "acc_stderr": 0.036795272555679256,
      "acc_norm": 0.5297297297297298,
      "acc_norm_stderr": 0.036795272555679256
    },
    "Cmmlu-virology": {
      "acc": 0.5857988165680473,
      "acc_stderr": 0.03800364668244123,
      "acc_norm": 0.5857988165680473,
      "acc_norm_stderr": 0.03800364668244123
    },
    "Cmmlu-world_history": {
      "acc": 0.6832298136645962,
      "acc_stderr": 0.03677863131157454,
      "acc_norm": 0.6832298136645962,
      "acc_norm_stderr": 0.03677863131157454
    },
    "Cmmlu-world_religions": {
      "acc": 0.73125,
      "acc_stderr": 0.035156741348767645,
      "acc_norm": 0.73125,
      "acc_norm_stderr": 0.035156741348767645
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/baichuan_13b,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:06:05.198769",
    "model_name": "baichuan_13b"
  }
}
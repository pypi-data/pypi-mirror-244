{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4962962962962963,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.4962962962962963,
      "acc_norm_stderr": 0.04319223625811331
    },
    "hendrycksTest-astronomy": {
      "acc": 0.4276315789473684,
      "acc_stderr": 0.04026097083296557,
      "acc_norm": 0.4276315789473684,
      "acc_norm_stderr": 0.04026097083296557
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956912
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.47547169811320755,
      "acc_stderr": 0.030735822206205608,
      "acc_norm": 0.47547169811320755,
      "acc_norm_stderr": 0.030735822206205608
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4513888888888889,
      "acc_stderr": 0.04161402398403279,
      "acc_norm": 0.4513888888888889,
      "acc_norm_stderr": 0.04161402398403279
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3988439306358382,
      "acc_stderr": 0.037336266553835096,
      "acc_norm": 0.3988439306358382,
      "acc_norm_stderr": 0.037336266553835096
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.043364327079931785,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.043364327079931785
    },
    "hendrycksTest-computer_security": {
      "acc": 0.56,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.56,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.43829787234042555,
      "acc_stderr": 0.03243618636108101,
      "acc_norm": 0.43829787234042555,
      "acc_norm_stderr": 0.03243618636108101
    },
    "hendrycksTest-econometrics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.044346007015849245,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.044346007015849245
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.45517241379310347,
      "acc_stderr": 0.04149886942192117,
      "acc_norm": 0.45517241379310347,
      "acc_norm_stderr": 0.04149886942192117
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2830687830687831,
      "acc_stderr": 0.023201392938194978,
      "acc_norm": 0.2830687830687831,
      "acc_norm_stderr": 0.023201392938194978
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.03809523809523811,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.03809523809523811
    },
    "hendrycksTest-global_facts": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5032258064516129,
      "acc_stderr": 0.028443414226438316,
      "acc_norm": 0.5032258064516129,
      "acc_norm_stderr": 0.028443414226438316
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3399014778325123,
      "acc_stderr": 0.033327690684107895,
      "acc_norm": 0.3399014778325123,
      "acc_norm_stderr": 0.033327690684107895
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6,
      "acc_stderr": 0.03825460278380025,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.03825460278380025
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5303030303030303,
      "acc_stderr": 0.0355580405176393,
      "acc_norm": 0.5303030303030303,
      "acc_norm_stderr": 0.0355580405176393
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.6580310880829016,
      "acc_stderr": 0.03423465100104283,
      "acc_norm": 0.6580310880829016,
      "acc_norm_stderr": 0.03423465100104283
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4564102564102564,
      "acc_stderr": 0.025254485424799602,
      "acc_norm": 0.4564102564102564,
      "acc_norm_stderr": 0.025254485424799602
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.27037037037037037,
      "acc_stderr": 0.02708037281514566,
      "acc_norm": 0.27037037037037037,
      "acc_norm_stderr": 0.02708037281514566
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.03214536859788639,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.03214536859788639
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.25165562913907286,
      "acc_stderr": 0.035433042343899844,
      "acc_norm": 0.25165562913907286,
      "acc_norm_stderr": 0.035433042343899844
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6110091743119266,
      "acc_stderr": 0.020902300887392873,
      "acc_norm": 0.6110091743119266,
      "acc_norm_stderr": 0.020902300887392873
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.25,
      "acc_stderr": 0.029531221160930918,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.029531221160930918
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.5441176470588235,
      "acc_stderr": 0.03495624522015477,
      "acc_norm": 0.5441176470588235,
      "acc_norm_stderr": 0.03495624522015477
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6455696202531646,
      "acc_stderr": 0.0311373042971858,
      "acc_norm": 0.6455696202531646,
      "acc_norm_stderr": 0.0311373042971858
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5605381165919282,
      "acc_stderr": 0.03331092511038179,
      "acc_norm": 0.5605381165919282,
      "acc_norm_stderr": 0.03331092511038179
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.4961832061068702,
      "acc_stderr": 0.04385162325601553,
      "acc_norm": 0.4961832061068702,
      "acc_norm_stderr": 0.04385162325601553
    },
    "hendrycksTest-international_law": {
      "acc": 0.6528925619834711,
      "acc_stderr": 0.04345724570292534,
      "acc_norm": 0.6528925619834711,
      "acc_norm_stderr": 0.04345724570292534
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.04830366024635331,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.04830366024635331
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5153374233128835,
      "acc_stderr": 0.039265223787088445,
      "acc_norm": 0.5153374233128835,
      "acc_norm_stderr": 0.039265223787088445
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.39285714285714285,
      "acc_stderr": 0.04635550135609976,
      "acc_norm": 0.39285714285714285,
      "acc_norm_stderr": 0.04635550135609976
    },
    "hendrycksTest-management": {
      "acc": 0.5728155339805825,
      "acc_stderr": 0.04897957737781168,
      "acc_norm": 0.5728155339805825,
      "acc_norm_stderr": 0.04897957737781168
    },
    "hendrycksTest-marketing": {
      "acc": 0.6837606837606838,
      "acc_stderr": 0.03046365674734027,
      "acc_norm": 0.6837606837606838,
      "acc_norm_stderr": 0.03046365674734027
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.53,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6360153256704981,
      "acc_stderr": 0.017205684809032232,
      "acc_norm": 0.6360153256704981,
      "acc_norm_stderr": 0.017205684809032232
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.476878612716763,
      "acc_stderr": 0.026890297881303125,
      "acc_norm": 0.476878612716763,
      "acc_norm_stderr": 0.026890297881303125
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.4738562091503268,
      "acc_stderr": 0.028590752958852394,
      "acc_norm": 0.4738562091503268,
      "acc_norm_stderr": 0.028590752958852394
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5787781350482315,
      "acc_stderr": 0.028043399858210628,
      "acc_norm": 0.5787781350482315,
      "acc_norm_stderr": 0.028043399858210628
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5246913580246914,
      "acc_stderr": 0.02778680093142745,
      "acc_norm": 0.5246913580246914,
      "acc_norm_stderr": 0.02778680093142745
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.36524822695035464,
      "acc_stderr": 0.02872386385328128,
      "acc_norm": 0.36524822695035464,
      "acc_norm_stderr": 0.02872386385328128
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3624511082138201,
      "acc_stderr": 0.01227751253325248,
      "acc_norm": 0.3624511082138201,
      "acc_norm_stderr": 0.01227751253325248
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.5220588235294118,
      "acc_stderr": 0.030343264224213528,
      "acc_norm": 0.5220588235294118,
      "acc_norm_stderr": 0.030343264224213528
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.45751633986928103,
      "acc_stderr": 0.02015468571259089,
      "acc_norm": 0.45751633986928103,
      "acc_norm_stderr": 0.02015468571259089
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5545454545454546,
      "acc_stderr": 0.047605488214603246,
      "acc_norm": 0.5545454545454546,
      "acc_norm_stderr": 0.047605488214603246
    },
    "hendrycksTest-security_studies": {
      "acc": 0.44081632653061226,
      "acc_stderr": 0.03178419114175363,
      "acc_norm": 0.44081632653061226,
      "acc_norm_stderr": 0.03178419114175363
    },
    "hendrycksTest-sociology": {
      "acc": 0.5970149253731343,
      "acc_stderr": 0.034683432951111266,
      "acc_norm": 0.5970149253731343,
      "acc_norm_stderr": 0.034683432951111266
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.65,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.65,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-virology": {
      "acc": 0.42168674698795183,
      "acc_stderr": 0.03844453181770917,
      "acc_norm": 0.42168674698795183,
      "acc_norm_stderr": 0.03844453181770917
    },
    "hendrycksTest-world_religions": {
      "acc": 0.6608187134502924,
      "acc_stderr": 0.03631053496488905,
      "acc_norm": 0.6608187134502924,
      "acc_norm_stderr": 0.03631053496488905
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/llama2-7b-hf',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models_bak/my_llama2_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:05:35.745787",
    "model_name": "llama2_7b"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.4911242603550296,
      "acc_stderr": 0.03856975909879415,
      "acc_norm": 0.4911242603550296,
      "acc_norm_stderr": 0.03856975909879415
    },
    "Cmmlu-anatomy": {
      "acc": 0.41216216216216217,
      "acc_stderr": 0.04059795529886698,
      "acc_norm": 0.41216216216216217,
      "acc_norm_stderr": 0.04059795529886698
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.3719512195121951,
      "acc_stderr": 0.03785697250133438,
      "acc_norm": 0.3719512195121951,
      "acc_norm_stderr": 0.03785697250133438
    },
    "Cmmlu-arts": {
      "acc": 0.79375,
      "acc_stderr": 0.03208782538184615,
      "acc_norm": 0.79375,
      "acc_norm_stderr": 0.03208782538184615
    },
    "Cmmlu-astronomy": {
      "acc": 0.3878787878787879,
      "acc_stderr": 0.0380491365397101,
      "acc_norm": 0.3878787878787879,
      "acc_norm_stderr": 0.0380491365397101
    },
    "Cmmlu-business_ethics": {
      "acc": 0.5502392344497608,
      "acc_stderr": 0.03449331173477289,
      "acc_norm": 0.5502392344497608,
      "acc_norm_stderr": 0.03449331173477289
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.4875,
      "acc_stderr": 0.03964018591811396,
      "acc_norm": 0.4875,
      "acc_norm_stderr": 0.03964018591811396
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.7480916030534351,
      "acc_stderr": 0.03807387116306086,
      "acc_norm": 0.7480916030534351,
      "acc_norm_stderr": 0.03807387116306086
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.5808823529411765,
      "acc_stderr": 0.042466374059928515,
      "acc_norm": 0.5808823529411765,
      "acc_norm_stderr": 0.042466374059928515
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5700934579439252,
      "acc_stderr": 0.04808472349429953,
      "acc_norm": 0.5700934579439252,
      "acc_norm_stderr": 0.04808472349429953
    },
    "Cmmlu-chinese_history": {
      "acc": 0.631578947368421,
      "acc_stderr": 0.026881785181855217,
      "acc_norm": 0.631578947368421,
      "acc_norm_stderr": 0.026881785181855217
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.5294117647058824,
      "acc_stderr": 0.03503235296367992,
      "acc_norm": 0.5294117647058824,
      "acc_norm_stderr": 0.03503235296367992
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.7374301675977654,
      "acc_stderr": 0.03298168673967122,
      "acc_norm": 0.7374301675977654,
      "acc_norm_stderr": 0.03298168673967122
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4641350210970464,
      "acc_stderr": 0.03246338898055659,
      "acc_norm": 0.4641350210970464,
      "acc_norm_stderr": 0.03246338898055659
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.25471698113207547,
      "acc_stderr": 0.042520162237633115,
      "acc_norm": 0.25471698113207547,
      "acc_norm_stderr": 0.042520162237633115
    },
    "Cmmlu-college_education": {
      "acc": 0.7102803738317757,
      "acc_stderr": 0.04406065334748509,
      "acc_norm": 0.7102803738317757,
      "acc_norm_stderr": 0.04406065334748509
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.4528301886792453,
      "acc_stderr": 0.048577381460134975,
      "acc_norm": 0.4528301886792453,
      "acc_norm_stderr": 0.048577381460134975
    },
    "Cmmlu-college_law": {
      "acc": 0.5,
      "acc_stderr": 0.04833682445228318,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.04833682445228318
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3238095238095238,
      "acc_stderr": 0.045884147180674746,
      "acc_norm": 0.3238095238095238,
      "acc_norm_stderr": 0.045884147180674746
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.4528301886792453,
      "acc_stderr": 0.048577381460134975,
      "acc_norm": 0.4528301886792453,
      "acc_norm_stderr": 0.048577381460134975
    },
    "Cmmlu-college_medicine": {
      "acc": 0.4358974358974359,
      "acc_stderr": 0.030066767691175843,
      "acc_norm": 0.4358974358974359,
      "acc_norm_stderr": 0.030066767691175843
    },
    "Cmmlu-computer_science": {
      "acc": 0.553921568627451,
      "acc_stderr": 0.034888454513049734,
      "acc_norm": 0.553921568627451,
      "acc_norm_stderr": 0.034888454513049734
    },
    "Cmmlu-computer_security": {
      "acc": 0.6432748538011696,
      "acc_stderr": 0.036740130028609534,
      "acc_norm": 0.6432748538011696,
      "acc_norm_stderr": 0.036740130028609534
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.48299319727891155,
      "acc_stderr": 0.041356350546877384,
      "acc_norm": 0.48299319727891155,
      "acc_norm_stderr": 0.041356350546877384
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.4316546762589928,
      "acc_stderr": 0.04216332260808161,
      "acc_norm": 0.4316546762589928,
      "acc_norm_stderr": 0.04216332260808161
    },
    "Cmmlu-economics": {
      "acc": 0.6037735849056604,
      "acc_stderr": 0.038911701656396055,
      "acc_norm": 0.6037735849056604,
      "acc_norm_stderr": 0.038911701656396055
    },
    "Cmmlu-education": {
      "acc": 0.6503067484662577,
      "acc_stderr": 0.03746668325470022,
      "acc_norm": 0.6503067484662577,
      "acc_norm_stderr": 0.03746668325470022
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.5348837209302325,
      "acc_stderr": 0.0381427854509025,
      "acc_norm": 0.5348837209302325,
      "acc_norm_stderr": 0.0381427854509025
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.5674603174603174,
      "acc_stderr": 0.031271150966052506,
      "acc_norm": 0.5674603174603174,
      "acc_norm_stderr": 0.031271150966052506
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.6767676767676768,
      "acc_stderr": 0.03332299921070645,
      "acc_norm": 0.6767676767676768,
      "acc_norm_stderr": 0.03332299921070645
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.7142857142857143,
      "acc_stderr": 0.02934457250063434,
      "acc_norm": 0.7142857142857143,
      "acc_norm_stderr": 0.02934457250063434
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.34782608695652173,
      "acc_stderr": 0.0314735003381084,
      "acc_norm": 0.34782608695652173,
      "acc_norm_stderr": 0.0314735003381084
    },
    "Cmmlu-ethnology": {
      "acc": 0.5037037037037037,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.5037037037037037,
      "acc_norm_stderr": 0.04319223625811331
    },
    "Cmmlu-food_science": {
      "acc": 0.5314685314685315,
      "acc_stderr": 0.041875883974458995,
      "acc_norm": 0.5314685314685315,
      "acc_norm_stderr": 0.041875883974458995
    },
    "Cmmlu-genetics": {
      "acc": 0.4147727272727273,
      "acc_stderr": 0.03724331671462075,
      "acc_norm": 0.4147727272727273,
      "acc_norm_stderr": 0.03724331671462075
    },
    "Cmmlu-global_facts": {
      "acc": 0.610738255033557,
      "acc_stderr": 0.0400790636583561,
      "acc_norm": 0.610738255033557,
      "acc_norm_stderr": 0.0400790636583561
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.4556213017751479,
      "acc_stderr": 0.0384235892283593,
      "acc_norm": 0.4556213017751479,
      "acc_norm_stderr": 0.0384235892283593
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.4015151515151515,
      "acc_stderr": 0.042829391226308106,
      "acc_norm": 0.4015151515151515,
      "acc_norm_stderr": 0.042829391226308106
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.5847457627118644,
      "acc_stderr": 0.045556216394221444,
      "acc_norm": 0.5847457627118644,
      "acc_norm_stderr": 0.045556216394221444
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.31097560975609756,
      "acc_stderr": 0.036256565294446104,
      "acc_norm": 0.31097560975609756,
      "acc_norm_stderr": 0.036256565294446104
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.41818181818181815,
      "acc_stderr": 0.0472457740573157,
      "acc_norm": 0.41818181818181815,
      "acc_norm_stderr": 0.0472457740573157
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.4825174825174825,
      "acc_stderr": 0.04193341146460268,
      "acc_norm": 0.4825174825174825,
      "acc_norm_stderr": 0.04193341146460268
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.5476190476190477,
      "acc_stderr": 0.044518079590553275,
      "acc_norm": 0.5476190476190477,
      "acc_norm_stderr": 0.044518079590553275
    },
    "Cmmlu-international_law": {
      "acc": 0.4540540540540541,
      "acc_stderr": 0.03670453191802573,
      "acc_norm": 0.4540540540540541,
      "acc_norm_stderr": 0.03670453191802573
    },
    "Cmmlu-journalism": {
      "acc": 0.6104651162790697,
      "acc_stderr": 0.037291130444871784,
      "acc_norm": 0.6104651162790697,
      "acc_norm_stderr": 0.037291130444871784
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.559610705596107,
      "acc_stderr": 0.024517120139754624,
      "acc_norm": 0.559610705596107,
      "acc_norm_stderr": 0.024517120139754624
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8504672897196262,
      "acc_stderr": 0.024434705445711552,
      "acc_norm": 0.8504672897196262,
      "acc_norm_stderr": 0.024434705445711552
    },
    "Cmmlu-logical": {
      "acc": 0.4796747967479675,
      "acc_stderr": 0.04523045598338887,
      "acc_norm": 0.4796747967479675,
      "acc_norm_stderr": 0.04523045598338887
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4426229508196721,
      "acc_stderr": 0.04515426947106743,
      "acc_norm": 0.4426229508196721,
      "acc_norm_stderr": 0.04515426947106743
    },
    "Cmmlu-management": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.03260773253630126,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.03260773253630126
    },
    "Cmmlu-marketing": {
      "acc": 0.6166666666666667,
      "acc_stderr": 0.03634017498327464,
      "acc_norm": 0.6166666666666667,
      "acc_norm_stderr": 0.03634017498327464
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.7037037037037037,
      "acc_stderr": 0.03330267393083603,
      "acc_norm": 0.7037037037037037,
      "acc_norm_stderr": 0.03330267393083603
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.4396551724137931,
      "acc_stderr": 0.04628442331493869,
      "acc_norm": 0.4396551724137931,
      "acc_norm_stderr": 0.04628442331493869
    },
    "Cmmlu-nutrition": {
      "acc": 0.5655172413793104,
      "acc_stderr": 0.04130740879555498,
      "acc_norm": 0.5655172413793104,
      "acc_norm_stderr": 0.04130740879555498
    },
    "Cmmlu-philosophy": {
      "acc": 0.7142857142857143,
      "acc_stderr": 0.04429811949614585,
      "acc_norm": 0.7142857142857143,
      "acc_norm_stderr": 0.04429811949614585
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.6342857142857142,
      "acc_stderr": 0.036512267418799475,
      "acc_norm": 0.6342857142857142,
      "acc_norm_stderr": 0.036512267418799475
    },
    "Cmmlu-professional_law": {
      "acc": 0.41232227488151657,
      "acc_stderr": 0.033968656176936,
      "acc_norm": 0.41232227488151657,
      "acc_norm_stderr": 0.033968656176936
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.42021276595744683,
      "acc_stderr": 0.02548903017816822,
      "acc_norm": 0.42021276595744683,
      "acc_norm_stderr": 0.02548903017816822
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.646551724137931,
      "acc_stderr": 0.031452746950022696,
      "acc_norm": 0.646551724137931,
      "acc_norm_stderr": 0.031452746950022696
    },
    "Cmmlu-public_relations": {
      "acc": 0.6149425287356322,
      "acc_stderr": 0.03699618907790638,
      "acc_norm": 0.6149425287356322,
      "acc_norm_stderr": 0.03699618907790638
    },
    "Cmmlu-security_study": {
      "acc": 0.6814814814814815,
      "acc_stderr": 0.04024778401977111,
      "acc_norm": 0.6814814814814815,
      "acc_norm_stderr": 0.04024778401977111
    },
    "Cmmlu-sociology": {
      "acc": 0.5973451327433629,
      "acc_stderr": 0.03269549239276309,
      "acc_norm": 0.5973451327433629,
      "acc_norm_stderr": 0.03269549239276309
    },
    "Cmmlu-sports_science": {
      "acc": 0.6,
      "acc_stderr": 0.03825460278380026,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.03825460278380026
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.5243243243243243,
      "acc_stderr": 0.0368168445060319,
      "acc_norm": 0.5243243243243243,
      "acc_norm_stderr": 0.0368168445060319
    },
    "Cmmlu-virology": {
      "acc": 0.5502958579881657,
      "acc_stderr": 0.0383801727294894,
      "acc_norm": 0.5502958579881657,
      "acc_norm_stderr": 0.0383801727294894
    },
    "Cmmlu-world_history": {
      "acc": 0.639751552795031,
      "acc_stderr": 0.03795305517110724,
      "acc_norm": 0.639751552795031,
      "acc_norm_stderr": 0.03795305517110724
    },
    "Cmmlu-world_religions": {
      "acc": 0.75625,
      "acc_stderr": 0.034049163262375844,
      "acc_norm": 0.75625,
      "acc_norm_stderr": 0.034049163262375844
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/baichuan_13b,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_baichuan13b_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:03:45.164824",
    "model_name": "baichuan_13b"
  }
}
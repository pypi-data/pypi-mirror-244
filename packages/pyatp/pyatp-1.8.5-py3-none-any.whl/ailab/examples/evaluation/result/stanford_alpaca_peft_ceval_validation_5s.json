{
  "results": {
    "Ceval-valid-accountant": {
      "acc": 0.30612244897959184,
      "acc_stderr": 0.06652247352247599,
      "acc_norm": 0.30612244897959184,
      "acc_norm_stderr": 0.06652247352247599
    },
    "Ceval-valid-advanced_mathematics": {
      "acc": 0.10526315789473684,
      "acc_stderr": 0.0723351864143449,
      "acc_norm": 0.10526315789473684,
      "acc_norm_stderr": 0.0723351864143449
    },
    "Ceval-valid-art_studies": {
      "acc": 0.12121212121212122,
      "acc_stderr": 0.05769525080199928,
      "acc_norm": 0.12121212121212122,
      "acc_norm_stderr": 0.05769525080199928
    },
    "Ceval-valid-basic_medicine": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-business_administration": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.07872958216222171,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.07872958216222171
    },
    "Ceval-valid-chinese_language_and_literature": {
      "acc": 0.08695652173913043,
      "acc_stderr": 0.06007385040937024,
      "acc_norm": 0.08695652173913043,
      "acc_norm_stderr": 0.06007385040937024
    },
    "Ceval-valid-civil_servant": {
      "acc": 0.2978723404255319,
      "acc_stderr": 0.06742861107915606,
      "acc_norm": 0.2978723404255319,
      "acc_norm_stderr": 0.06742861107915606
    },
    "Ceval-valid-clinical_medicine": {
      "acc": 0.09090909090909091,
      "acc_stderr": 0.06273323266748675,
      "acc_norm": 0.09090909090909091,
      "acc_norm_stderr": 0.06273323266748675
    },
    "Ceval-valid-college_chemistry": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.10279899245732686,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.10279899245732686
    },
    "Ceval-valid-college_economics": {
      "acc": 0.3090909090909091,
      "acc_stderr": 0.06288639360110458,
      "acc_norm": 0.3090909090909091,
      "acc_norm_stderr": 0.06288639360110458
    },
    "Ceval-valid-college_physics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-college_programming": {
      "acc": 0.2702702702702703,
      "acc_stderr": 0.07401656182502248,
      "acc_norm": 0.2702702702702703,
      "acc_norm_stderr": 0.07401656182502248
    },
    "Ceval-valid-computer_architecture": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.10101525445522108,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.10101525445522108
    },
    "Ceval-valid-computer_network": {
      "acc": 0.15789473684210525,
      "acc_stderr": 0.08594700851870798,
      "acc_norm": 0.15789473684210525,
      "acc_norm_stderr": 0.08594700851870798
    },
    "Ceval-valid-discrete_mathematics": {
      "acc": 0.125,
      "acc_stderr": 0.08539125638299665,
      "acc_norm": 0.125,
      "acc_norm_stderr": 0.08539125638299665
    },
    "Ceval-valid-education_science": {
      "acc": 0.20689655172413793,
      "acc_stderr": 0.07655305550699536,
      "acc_norm": 0.20689655172413793,
      "acc_norm_stderr": 0.07655305550699536
    },
    "Ceval-valid-electrical_engineer": {
      "acc": 0.1891891891891892,
      "acc_stderr": 0.06527647182968213,
      "acc_norm": 0.1891891891891892,
      "acc_norm_stderr": 0.06527647182968213
    },
    "Ceval-valid-environmental_impact_assessment_engineer": {
      "acc": 0.3548387096774194,
      "acc_stderr": 0.08735525166275225,
      "acc_norm": 0.3548387096774194,
      "acc_norm_stderr": 0.08735525166275225
    },
    "Ceval-valid-fire_engineer": {
      "acc": 0.2903225806451613,
      "acc_stderr": 0.08287246824945245,
      "acc_norm": 0.2903225806451613,
      "acc_norm_stderr": 0.08287246824945245
    },
    "Ceval-valid-high_school_biology": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-high_school_chemistry": {
      "acc": 0.05263157894736842,
      "acc_stderr": 0.052631578947368404,
      "acc_norm": 0.05263157894736842,
      "acc_norm_stderr": 0.052631578947368404
    },
    "Ceval-valid-high_school_chinese": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-high_school_geography": {
      "acc": 0.15789473684210525,
      "acc_stderr": 0.08594700851870798,
      "acc_norm": 0.15789473684210525,
      "acc_norm_stderr": 0.08594700851870798
    },
    "Ceval-valid-high_school_history": {
      "acc": 0.4,
      "acc_stderr": 0.11239029738980327,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.11239029738980327
    },
    "Ceval-valid-high_school_mathematics": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.10083169033033672,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.10083169033033672
    },
    "Ceval-valid-high_school_physics": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-high_school_politics": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-ideological_and_moral_cultivation": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-law": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.10279899245732686,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.10279899245732686
    },
    "Ceval-valid-legal_professional": {
      "acc": 0.34782608695652173,
      "acc_stderr": 0.10154334054280735,
      "acc_norm": 0.34782608695652173,
      "acc_norm_stderr": 0.10154334054280735
    },
    "Ceval-valid-logic": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.09144861547306321,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.09144861547306321
    },
    "Ceval-valid-mao_zedong_thought": {
      "acc": 0.2916666666666667,
      "acc_stderr": 0.09477598811252415,
      "acc_norm": 0.2916666666666667,
      "acc_norm_stderr": 0.09477598811252415
    },
    "Ceval-valid-marxism": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-metrology_engineer": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.10389457216622949,
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.10389457216622949
    },
    "Ceval-valid-middle_school_biology": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.10101525445522108,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.10101525445522108
    },
    "Ceval-valid-middle_school_chemistry": {
      "acc": 0.3,
      "acc_stderr": 0.10513149660756933,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.10513149660756933
    },
    "Ceval-valid-middle_school_geography": {
      "acc": 0.25,
      "acc_stderr": 0.1305582419667734,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.1305582419667734
    },
    "Ceval-valid-middle_school_history": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.09144861547306321,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.09144861547306321
    },
    "Ceval-valid-middle_school_mathematics": {
      "acc": 0.10526315789473684,
      "acc_stderr": 0.07233518641434489,
      "acc_norm": 0.10526315789473684,
      "acc_norm_stderr": 0.07233518641434489
    },
    "Ceval-valid-middle_school_physics": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.10956136839295434,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.10956136839295434
    },
    "Ceval-valid-middle_school_politics": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.11065666703449763,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.11065666703449763
    },
    "Ceval-valid-modern_chinese_history": {
      "acc": 0.043478260869565216,
      "acc_stderr": 0.04347826086956523,
      "acc_norm": 0.043478260869565216,
      "acc_norm_stderr": 0.04347826086956523
    },
    "Ceval-valid-operating_system": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-physician": {
      "acc": 0.2653061224489796,
      "acc_stderr": 0.06372446937141221,
      "acc_norm": 0.2653061224489796,
      "acc_norm_stderr": 0.06372446937141221
    },
    "Ceval-valid-plant_protection": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.10497277621629558,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.10497277621629558
    },
    "Ceval-valid-probability_and_statistics": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.1008316903303367,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.1008316903303367
    },
    "Ceval-valid-professional_tour_guide": {
      "acc": 0.1724137931034483,
      "acc_stderr": 0.0713860923457608,
      "acc_norm": 0.1724137931034483,
      "acc_norm_stderr": 0.0713860923457608
    },
    "Ceval-valid-sports_science": {
      "acc": 0.5789473684210527,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.5789473684210527,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-tax_accountant": {
      "acc": 0.22448979591836735,
      "acc_stderr": 0.06022425581505364,
      "acc_norm": 0.22448979591836735,
      "acc_norm_stderr": 0.06022425581505364
    },
    "Ceval-valid-teacher_qualification": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.07102933373079212,
      "acc_norm": 0.3181818181818182,
      "acc_norm_stderr": 0.07102933373079212
    },
    "Ceval-valid-urban_and_rural_planner": {
      "acc": 0.32608695652173914,
      "acc_stderr": 0.06988152725357213,
      "acc_norm": 0.32608695652173914,
      "acc_norm_stderr": 0.06988152725357213
    },
    "Ceval-valid-veterinary_medicine": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.08793911249520549,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.08793911249520549
    }
  },
  "versions": {
    "Ceval-valid-accountant": 1,
    "Ceval-valid-advanced_mathematics": 1,
    "Ceval-valid-art_studies": 1,
    "Ceval-valid-basic_medicine": 1,
    "Ceval-valid-business_administration": 1,
    "Ceval-valid-chinese_language_and_literature": 1,
    "Ceval-valid-civil_servant": 1,
    "Ceval-valid-clinical_medicine": 1,
    "Ceval-valid-college_chemistry": 1,
    "Ceval-valid-college_economics": 1,
    "Ceval-valid-college_physics": 1,
    "Ceval-valid-college_programming": 1,
    "Ceval-valid-computer_architecture": 1,
    "Ceval-valid-computer_network": 1,
    "Ceval-valid-discrete_mathematics": 1,
    "Ceval-valid-education_science": 1,
    "Ceval-valid-electrical_engineer": 1,
    "Ceval-valid-environmental_impact_assessment_engineer": 1,
    "Ceval-valid-fire_engineer": 1,
    "Ceval-valid-high_school_biology": 1,
    "Ceval-valid-high_school_chemistry": 1,
    "Ceval-valid-high_school_chinese": 1,
    "Ceval-valid-high_school_geography": 1,
    "Ceval-valid-high_school_history": 1,
    "Ceval-valid-high_school_mathematics": 1,
    "Ceval-valid-high_school_physics": 1,
    "Ceval-valid-high_school_politics": 1,
    "Ceval-valid-ideological_and_moral_cultivation": 1,
    "Ceval-valid-law": 1,
    "Ceval-valid-legal_professional": 1,
    "Ceval-valid-logic": 1,
    "Ceval-valid-mao_zedong_thought": 1,
    "Ceval-valid-marxism": 1,
    "Ceval-valid-metrology_engineer": 1,
    "Ceval-valid-middle_school_biology": 1,
    "Ceval-valid-middle_school_chemistry": 1,
    "Ceval-valid-middle_school_geography": 1,
    "Ceval-valid-middle_school_history": 1,
    "Ceval-valid-middle_school_mathematics": 1,
    "Ceval-valid-middle_school_physics": 1,
    "Ceval-valid-middle_school_politics": 1,
    "Ceval-valid-modern_chinese_history": 1,
    "Ceval-valid-operating_system": 1,
    "Ceval-valid-physician": 1,
    "Ceval-valid-plant_protection": 1,
    "Ceval-valid-probability_and_statistics": 1,
    "Ceval-valid-professional_tour_guide": 1,
    "Ceval-valid-sports_science": 1,
    "Ceval-valid-tax_accountant": 1,
    "Ceval-valid-teacher_qualification": 1,
    "Ceval-valid-urban_and_rural_planner": 1,
    "Ceval-valid-veterinary_medicine": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama-7b-hf,load_in_8bit=True,dtype='float16',tokenizer=/home/sdk_token/llama-7b-hf_tokenizer,use_accelerate=False,peft=/home/finetuned_models/my_standford_alpaca_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:22:01.082917",
    "model_name": "stanford_alpaca"
  }
}
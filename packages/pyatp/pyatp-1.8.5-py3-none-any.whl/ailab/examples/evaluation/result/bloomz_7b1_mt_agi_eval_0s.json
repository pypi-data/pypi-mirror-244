{
  "results": {
    "agi_eval-aqua-rat": {
      "acc": 0.2125984251968504,
      "acc_stderr": 0.025722779833723057
    },
    "agi_eval-gaokao-biology": {
      "acc": 0.22857142857142856,
      "acc_stderr": 0.02904595687156657
    },
    "agi_eval-gaokao-chemistry": {
      "acc": 0.3140096618357488,
      "acc_stderr": 0.032336789150604006
    },
    "agi_eval-gaokao-chinese": {
      "acc": 0.2804878048780488,
      "acc_stderr": 0.02870073569367396
    },
    "agi_eval-gaokao-english": {
      "acc": 0.6143790849673203,
      "acc_stderr": 0.02787074527829027
    },
    "agi_eval-gaokao-geography": {
      "acc": 0.4120603015075377,
      "acc_stderr": 0.03497954737400385
    },
    "agi_eval-gaokao-history": {
      "acc": 0.4,
      "acc_stderr": 0.03202563076101735
    },
    "agi_eval-gaokao-mathqa": {
      "acc": 0.245014245014245,
      "acc_stderr": 0.02298957930108733
    },
    "agi_eval-logiqa-en": {
      "acc": 0.23655913978494625,
      "acc_stderr": 0.016668667667174192
    },
    "agi_eval-logiqa-zh": {
      "acc": 0.261136712749616,
      "acc_stderr": 0.01722897068240861
    },
    "agi_eval-lsat-ar": {
      "acc": 0.2391304347826087,
      "acc_stderr": 0.028187385293933945
    },
    "agi_eval-lsat-lr": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.01931676548053297
    },
    "agi_eval-lsat-rc": {
      "acc": 0.3345724907063197,
      "acc_stderr": 0.028822264091264628
    },
    "agi_eval-sat-en": {
      "acc": 0.441747572815534,
      "acc_stderr": 0.03468370354145869
    },
    "agi_eval-sat-en-without-passage": {
      "acc": 0.3300970873786408,
      "acc_stderr": 0.032843531514668484
    },
    "agi_eval-sat-math": {
      "acc": 0.2818181818181818,
      "acc_stderr": 0.030400424640665242
    }
  },
  "versions": {
    "agi_eval-aqua-rat": 0,
    "agi_eval-gaokao-biology": 0,
    "agi_eval-gaokao-chemistry": 0,
    "agi_eval-gaokao-chinese": 0,
    "agi_eval-gaokao-english": 0,
    "agi_eval-gaokao-geography": 0,
    "agi_eval-gaokao-history": 0,
    "agi_eval-gaokao-mathqa": 0,
    "agi_eval-logiqa-en": 0,
    "agi_eval-logiqa-zh": 0,
    "agi_eval-lsat-ar": 0,
    "agi_eval-lsat-lr": 0,
    "agi_eval-lsat-rc": 0,
    "agi_eval-sat-en": 0,
    "agi_eval-sat-en-without-passage": 0,
    "agi_eval-sat-math": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/bloomz_7b,dtype='float16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 0,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:48:45.755876",
    "model_name": "bloomz_7b1_mt"
  }
}
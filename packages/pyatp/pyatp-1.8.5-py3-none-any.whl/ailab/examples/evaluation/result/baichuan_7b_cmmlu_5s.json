{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.4437869822485207,
      "acc_stderr": 0.03833127038718734,
      "acc_norm": 0.4437869822485207,
      "acc_norm_stderr": 0.03833127038718734
    },
    "Cmmlu-anatomy": {
      "acc": 0.31756756756756754,
      "acc_stderr": 0.038396287341496804,
      "acc_norm": 0.31756756756756754,
      "acc_norm_stderr": 0.038396287341496804
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.3353658536585366,
      "acc_stderr": 0.03697915163403716,
      "acc_norm": 0.3353658536585366,
      "acc_norm_stderr": 0.03697915163403716
    },
    "Cmmlu-arts": {
      "acc": 0.6875,
      "acc_stderr": 0.03675892481369823,
      "acc_norm": 0.6875,
      "acc_norm_stderr": 0.03675892481369823
    },
    "Cmmlu-astronomy": {
      "acc": 0.3515151515151515,
      "acc_stderr": 0.0372820699868265,
      "acc_norm": 0.3515151515151515,
      "acc_norm_stderr": 0.0372820699868265
    },
    "Cmmlu-business_ethics": {
      "acc": 0.4784688995215311,
      "acc_stderr": 0.034636603284355416,
      "acc_norm": 0.4784688995215311,
      "acc_norm_stderr": 0.034636603284355416
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.3875,
      "acc_stderr": 0.03863583812241406,
      "acc_norm": 0.3875,
      "acc_norm_stderr": 0.03863583812241406
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5954198473282443,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.5954198473282443,
      "acc_norm_stderr": 0.043046937953806645
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.4852941176470588,
      "acc_stderr": 0.043014531310745674,
      "acc_norm": 0.4852941176470588,
      "acc_norm_stderr": 0.043014531310745674
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.45794392523364486,
      "acc_stderr": 0.04839219555189162,
      "acc_norm": 0.45794392523364486,
      "acc_norm_stderr": 0.04839219555189162
    },
    "Cmmlu-chinese_history": {
      "acc": 0.5201238390092879,
      "acc_stderr": 0.027841333447728503,
      "acc_norm": 0.5201238390092879,
      "acc_norm_stderr": 0.027841333447728503
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.03410785338904719,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.03410785338904719
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.6033519553072626,
      "acc_stderr": 0.036667223012526735,
      "acc_norm": 0.6033519553072626,
      "acc_norm_stderr": 0.036667223012526735
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4092827004219409,
      "acc_stderr": 0.03200704183359591,
      "acc_norm": 0.4092827004219409,
      "acc_norm_stderr": 0.03200704183359591
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2641509433962264,
      "acc_stderr": 0.043025487739590106,
      "acc_norm": 0.2641509433962264,
      "acc_norm_stderr": 0.043025487739590106
    },
    "Cmmlu-college_education": {
      "acc": 0.5700934579439252,
      "acc_stderr": 0.04808472349429953,
      "acc_norm": 0.5700934579439252,
      "acc_norm_stderr": 0.04808472349429953
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.04730439022852894,
      "acc_norm": 0.37735849056603776,
      "acc_norm_stderr": 0.04730439022852894
    },
    "Cmmlu-college_law": {
      "acc": 0.37962962962962965,
      "acc_stderr": 0.04691521224077742,
      "acc_norm": 0.37962962962962965,
      "acc_norm_stderr": 0.04691521224077742
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.04429811949614583,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.04429811949614583
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3867924528301887,
      "acc_stderr": 0.04752784159123843,
      "acc_norm": 0.3867924528301887,
      "acc_norm_stderr": 0.04752784159123843
    },
    "Cmmlu-college_medicine": {
      "acc": 0.3443223443223443,
      "acc_stderr": 0.028810005635470523,
      "acc_norm": 0.3443223443223443,
      "acc_norm_stderr": 0.028810005635470523
    },
    "Cmmlu-computer_science": {
      "acc": 0.39705882352941174,
      "acc_stderr": 0.0343413116471913,
      "acc_norm": 0.39705882352941174,
      "acc_norm_stderr": 0.0343413116471913
    },
    "Cmmlu-computer_security": {
      "acc": 0.4619883040935672,
      "acc_stderr": 0.03823727092882307,
      "acc_norm": 0.4619883040935672,
      "acc_norm_stderr": 0.03823727092882307
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.43537414965986393,
      "acc_stderr": 0.041033188994941856,
      "acc_norm": 0.43537414965986393,
      "acc_norm_stderr": 0.041033188994941856
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.34532374100719426,
      "acc_stderr": 0.04047501062151218,
      "acc_norm": 0.34532374100719426,
      "acc_norm_stderr": 0.04047501062151218
    },
    "Cmmlu-economics": {
      "acc": 0.4276729559748428,
      "acc_stderr": 0.03935949201960896,
      "acc_norm": 0.4276729559748428,
      "acc_norm_stderr": 0.03935949201960896
    },
    "Cmmlu-education": {
      "acc": 0.5766871165644172,
      "acc_stderr": 0.03881891213334383,
      "acc_norm": 0.5766871165644172,
      "acc_norm_stderr": 0.03881891213334383
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.47674418604651164,
      "acc_stderr": 0.03819457472859225,
      "acc_norm": 0.47674418604651164,
      "acc_norm_stderr": 0.03819457472859225
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.44841269841269843,
      "acc_stderr": 0.03139129414134258,
      "acc_norm": 0.44841269841269843,
      "acc_norm_stderr": 0.03139129414134258
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.4595959595959596,
      "acc_stderr": 0.035507024651313425,
      "acc_norm": 0.4595959595959596,
      "acc_norm_stderr": 0.035507024651313425
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.5630252100840336,
      "acc_stderr": 0.03221943636566196,
      "acc_norm": 0.5630252100840336,
      "acc_norm_stderr": 0.03221943636566196
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.2956521739130435,
      "acc_stderr": 0.030155489768916178,
      "acc_norm": 0.2956521739130435,
      "acc_norm_stderr": 0.030155489768916178
    },
    "Cmmlu-ethnology": {
      "acc": 0.45925925925925926,
      "acc_stderr": 0.04304979692464242,
      "acc_norm": 0.45925925925925926,
      "acc_norm_stderr": 0.04304979692464242
    },
    "Cmmlu-food_science": {
      "acc": 0.5034965034965035,
      "acc_stderr": 0.04195804195804197,
      "acc_norm": 0.5034965034965035,
      "acc_norm_stderr": 0.04195804195804197
    },
    "Cmmlu-genetics": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.03636363636363636,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03636363636363636
    },
    "Cmmlu-global_facts": {
      "acc": 0.5234899328859061,
      "acc_stderr": 0.04105436598675506,
      "acc_norm": 0.5234899328859061,
      "acc_norm_stderr": 0.04105436598675506
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.28994082840236685,
      "acc_stderr": 0.03500638924911012,
      "acc_norm": 0.28994082840236685,
      "acc_norm_stderr": 0.03500638924911012
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.32575757575757575,
      "acc_stderr": 0.04094677028657698,
      "acc_norm": 0.32575757575757575,
      "acc_norm_stderr": 0.04094677028657698
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.4745762711864407,
      "acc_stderr": 0.04616522112086746,
      "acc_norm": 0.4745762711864407,
      "acc_norm_stderr": 0.04616522112086746
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.27439024390243905,
      "acc_stderr": 0.03494959016177541,
      "acc_norm": 0.27439024390243905,
      "acc_norm_stderr": 0.03494959016177541
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.35454545454545455,
      "acc_stderr": 0.04582004841505417,
      "acc_norm": 0.35454545454545455,
      "acc_norm_stderr": 0.04582004841505417
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.43356643356643354,
      "acc_stderr": 0.04158705287172622,
      "acc_norm": 0.43356643356643354,
      "acc_norm_stderr": 0.04158705287172622
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.5,
      "acc_stderr": 0.04472135954999579,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.04472135954999579
    },
    "Cmmlu-international_law": {
      "acc": 0.3621621621621622,
      "acc_stderr": 0.035432171151384854,
      "acc_norm": 0.3621621621621622,
      "acc_norm_stderr": 0.035432171151384854
    },
    "Cmmlu-journalism": {
      "acc": 0.5116279069767442,
      "acc_stderr": 0.03822561461565635,
      "acc_norm": 0.5116279069767442,
      "acc_norm_stderr": 0.03822561461565635
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.4744525547445255,
      "acc_stderr": 0.024660985680502453,
      "acc_norm": 0.4744525547445255,
      "acc_norm_stderr": 0.024660985680502453
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.719626168224299,
      "acc_stderr": 0.030777434132644298,
      "acc_norm": 0.719626168224299,
      "acc_norm_stderr": 0.030777434132644298
    },
    "Cmmlu-logical": {
      "acc": 0.35772357723577236,
      "acc_stderr": 0.04339651526440302,
      "acc_norm": 0.35772357723577236,
      "acc_norm_stderr": 0.04339651526440302
    },
    "Cmmlu-machine_learning": {
      "acc": 0.3524590163934426,
      "acc_stderr": 0.0434305428342706,
      "acc_norm": 0.3524590163934426,
      "acc_norm_stderr": 0.0434305428342706
    },
    "Cmmlu-management": {
      "acc": 0.5142857142857142,
      "acc_stderr": 0.0345716036894725,
      "acc_norm": 0.5142857142857142,
      "acc_norm_stderr": 0.0345716036894725
    },
    "Cmmlu-marketing": {
      "acc": 0.4777777777777778,
      "acc_stderr": 0.03733482601727583,
      "acc_norm": 0.4777777777777778,
      "acc_norm_stderr": 0.03733482601727583
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.5873015873015873,
      "acc_stderr": 0.03590608560215488,
      "acc_norm": 0.5873015873015873,
      "acc_norm_stderr": 0.03590608560215488
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.35344827586206895,
      "acc_stderr": 0.04457749404392889,
      "acc_norm": 0.35344827586206895,
      "acc_norm_stderr": 0.04457749404392889
    },
    "Cmmlu-nutrition": {
      "acc": 0.4,
      "acc_stderr": 0.04082482904638628,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04082482904638628
    },
    "Cmmlu-philosophy": {
      "acc": 0.5047619047619047,
      "acc_stderr": 0.049026810195176226,
      "acc_norm": 0.5047619047619047,
      "acc_norm_stderr": 0.049026810195176226
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5257142857142857,
      "acc_stderr": 0.03785474169043359,
      "acc_norm": 0.5257142857142857,
      "acc_norm_stderr": 0.03785474169043359
    },
    "Cmmlu-professional_law": {
      "acc": 0.3412322274881517,
      "acc_stderr": 0.03271760807501987,
      "acc_norm": 0.3412322274881517,
      "acc_norm_stderr": 0.03271760807501987
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.3617021276595745,
      "acc_stderr": 0.02481256125466088,
      "acc_norm": 0.3617021276595745,
      "acc_norm_stderr": 0.02481256125466088
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.521551724137931,
      "acc_stderr": 0.03286701020956285,
      "acc_norm": 0.521551724137931,
      "acc_norm_stderr": 0.03286701020956285
    },
    "Cmmlu-public_relations": {
      "acc": 0.4885057471264368,
      "acc_stderr": 0.03800425000198232,
      "acc_norm": 0.4885057471264368,
      "acc_norm_stderr": 0.03800425000198232
    },
    "Cmmlu-security_study": {
      "acc": 0.562962962962963,
      "acc_stderr": 0.042849586397534,
      "acc_norm": 0.562962962962963,
      "acc_norm_stderr": 0.042849586397534
    },
    "Cmmlu-sociology": {
      "acc": 0.5088495575221239,
      "acc_stderr": 0.03332811194650094,
      "acc_norm": 0.5088495575221239,
      "acc_norm_stderr": 0.03332811194650094
    },
    "Cmmlu-sports_science": {
      "acc": 0.4727272727272727,
      "acc_stderr": 0.0389853160557942,
      "acc_norm": 0.4727272727272727,
      "acc_norm_stderr": 0.0389853160557942
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3675675675675676,
      "acc_stderr": 0.03554403659088362,
      "acc_norm": 0.3675675675675676,
      "acc_norm_stderr": 0.03554403659088362
    },
    "Cmmlu-virology": {
      "acc": 0.46153846153846156,
      "acc_stderr": 0.038461538461538464,
      "acc_norm": 0.46153846153846156,
      "acc_norm_stderr": 0.038461538461538464
    },
    "Cmmlu-world_history": {
      "acc": 0.43478260869565216,
      "acc_stderr": 0.039190774733304186,
      "acc_norm": 0.43478260869565216,
      "acc_norm_stderr": 0.039190774733304186
    },
    "Cmmlu-world_religions": {
      "acc": 0.5375,
      "acc_stderr": 0.03954089913497815,
      "acc_norm": 0.5375,
      "acc_norm_stderr": 0.03954089913497815
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/baichuan_7b,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "4:32:28.919131",
    "model_name": "baichuan_7b"
  }
}
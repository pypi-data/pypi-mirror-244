{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "hendrycksTest-anatomy": {
      "acc": 0.48148148148148145,
      "acc_stderr": 0.04316378599511324,
      "acc_norm": 0.48148148148148145,
      "acc_norm_stderr": 0.04316378599511324
    },
    "hendrycksTest-astronomy": {
      "acc": 0.506578947368421,
      "acc_stderr": 0.040685900502249704,
      "acc_norm": 0.506578947368421,
      "acc_norm_stderr": 0.040685900502249704
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5886792452830188,
      "acc_stderr": 0.030285009259009798,
      "acc_norm": 0.5886792452830188,
      "acc_norm_stderr": 0.030285009259009798
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.041553199555931467,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.041553199555931467
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110175,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110175
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145632,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.5086705202312138,
      "acc_stderr": 0.03811890988940412,
      "acc_norm": 0.5086705202312138,
      "acc_norm_stderr": 0.03811890988940412
    },
    "hendrycksTest-college_physics": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.04440521906179328,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.04440521906179328
    },
    "hendrycksTest-computer_security": {
      "acc": 0.74,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.74,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.4297872340425532,
      "acc_stderr": 0.03236214467715564,
      "acc_norm": 0.4297872340425532,
      "acc_norm_stderr": 0.03236214467715564
    },
    "hendrycksTest-econometrics": {
      "acc": 0.30701754385964913,
      "acc_stderr": 0.04339138322579861,
      "acc_norm": 0.30701754385964913,
      "acc_norm_stderr": 0.04339138322579861
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.496551724137931,
      "acc_stderr": 0.041665675771015785,
      "acc_norm": 0.496551724137931,
      "acc_norm_stderr": 0.041665675771015785
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.3544973544973545,
      "acc_stderr": 0.024636830602842,
      "acc_norm": 0.3544973544973545,
      "acc_norm_stderr": 0.024636830602842
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.0393253768039287,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.0393253768039287
    },
    "hendrycksTest-global_facts": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5935483870967742,
      "acc_stderr": 0.027941727346256308,
      "acc_norm": 0.5935483870967742,
      "acc_norm_stderr": 0.027941727346256308
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3842364532019704,
      "acc_stderr": 0.03422398565657551,
      "acc_norm": 0.3842364532019704,
      "acc_norm_stderr": 0.03422398565657551
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.03453131801885416,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.03453131801885416
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7272727272727273,
      "acc_stderr": 0.03173071239071724,
      "acc_norm": 0.7272727272727273,
      "acc_norm_stderr": 0.03173071239071724
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.8238341968911918,
      "acc_stderr": 0.027493504244548064,
      "acc_norm": 0.8238341968911918,
      "acc_norm_stderr": 0.027493504244548064
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.49230769230769234,
      "acc_stderr": 0.025348006031534785,
      "acc_norm": 0.49230769230769234,
      "acc_norm_stderr": 0.025348006031534785
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.026067159222275798,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.026067159222275798
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5126050420168067,
      "acc_stderr": 0.03246816765752174,
      "acc_norm": 0.5126050420168067,
      "acc_norm_stderr": 0.03246816765752174
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2913907284768212,
      "acc_stderr": 0.037101857261199946,
      "acc_norm": 0.2913907284768212,
      "acc_norm_stderr": 0.037101857261199946
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7431192660550459,
      "acc_stderr": 0.018732492928342483,
      "acc_norm": 0.7431192660550459,
      "acc_norm_stderr": 0.018732492928342483
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3472222222222222,
      "acc_stderr": 0.032468872436376486,
      "acc_norm": 0.3472222222222222,
      "acc_norm_stderr": 0.032468872436376486
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.35784313725490197,
      "acc_stderr": 0.03364487286088299,
      "acc_norm": 0.35784313725490197,
      "acc_norm_stderr": 0.03364487286088299
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6371308016877637,
      "acc_stderr": 0.031299208255302136,
      "acc_norm": 0.6371308016877637,
      "acc_norm_stderr": 0.031299208255302136
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6143497757847534,
      "acc_stderr": 0.03266842214289201,
      "acc_norm": 0.6143497757847534,
      "acc_norm_stderr": 0.03266842214289201
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6259541984732825,
      "acc_stderr": 0.042438692422305246,
      "acc_norm": 0.6259541984732825,
      "acc_norm_stderr": 0.042438692422305246
    },
    "hendrycksTest-international_law": {
      "acc": 0.71900826446281,
      "acc_stderr": 0.04103203830514512,
      "acc_norm": 0.71900826446281,
      "acc_norm_stderr": 0.04103203830514512
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6388888888888888,
      "acc_stderr": 0.04643454608906275,
      "acc_norm": 0.6388888888888888,
      "acc_norm_stderr": 0.04643454608906275
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6748466257668712,
      "acc_stderr": 0.03680350371286461,
      "acc_norm": 0.6748466257668712,
      "acc_norm_stderr": 0.03680350371286461
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.3482142857142857,
      "acc_stderr": 0.04521829902833587,
      "acc_norm": 0.3482142857142857,
      "acc_norm_stderr": 0.04521829902833587
    },
    "hendrycksTest-management": {
      "acc": 0.7281553398058253,
      "acc_stderr": 0.044052680241409216,
      "acc_norm": 0.7281553398058253,
      "acc_norm_stderr": 0.044052680241409216
    },
    "hendrycksTest-marketing": {
      "acc": 0.782051282051282,
      "acc_stderr": 0.027046857630716677,
      "acc_norm": 0.782051282051282,
      "acc_norm_stderr": 0.027046857630716677
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.55,
      "acc_stderr": 0.04999999999999999,
      "acc_norm": 0.55,
      "acc_norm_stderr": 0.04999999999999999
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7381864623243933,
      "acc_stderr": 0.01572083867844526,
      "acc_norm": 0.7381864623243933,
      "acc_norm_stderr": 0.01572083867844526
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5982658959537572,
      "acc_stderr": 0.026394104177643634,
      "acc_norm": 0.5982658959537572,
      "acc_norm_stderr": 0.026394104177643634
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.26145251396648045,
      "acc_stderr": 0.01469659965036456,
      "acc_norm": 0.26145251396648045,
      "acc_norm_stderr": 0.01469659965036456
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6078431372549019,
      "acc_stderr": 0.02795604616542452,
      "acc_norm": 0.6078431372549019,
      "acc_norm_stderr": 0.02795604616542452
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6366559485530546,
      "acc_stderr": 0.02731684767419271,
      "acc_norm": 0.6366559485530546,
      "acc_norm_stderr": 0.02731684767419271
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6203703703703703,
      "acc_stderr": 0.027002521034516468,
      "acc_norm": 0.6203703703703703,
      "acc_norm_stderr": 0.027002521034516468
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.34397163120567376,
      "acc_stderr": 0.028338017428611324,
      "acc_norm": 0.34397163120567376,
      "acc_norm_stderr": 0.028338017428611324
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3767926988265971,
      "acc_stderr": 0.012376459593894404,
      "acc_norm": 0.3767926988265971,
      "acc_norm_stderr": 0.012376459593894404
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.02952009569768776,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.02952009569768776
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5310457516339869,
      "acc_stderr": 0.020188804456361887,
      "acc_norm": 0.5310457516339869,
      "acc_norm_stderr": 0.020188804456361887
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5818181818181818,
      "acc_stderr": 0.04724577405731572,
      "acc_norm": 0.5818181818181818,
      "acc_norm_stderr": 0.04724577405731572
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5183673469387755,
      "acc_stderr": 0.03198761546763127,
      "acc_norm": 0.5183673469387755,
      "acc_norm_stderr": 0.03198761546763127
    },
    "hendrycksTest-sociology": {
      "acc": 0.7313432835820896,
      "acc_stderr": 0.03134328358208954,
      "acc_norm": 0.7313432835820896,
      "acc_norm_stderr": 0.03134328358208954
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.81,
      "acc_stderr": 0.03942772444036625,
      "acc_norm": 0.81,
      "acc_norm_stderr": 0.03942772444036625
    },
    "hendrycksTest-virology": {
      "acc": 0.46987951807228917,
      "acc_stderr": 0.03885425420866767,
      "acc_norm": 0.46987951807228917,
      "acc_norm_stderr": 0.03885425420866767
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7543859649122807,
      "acc_stderr": 0.03301405946987249,
      "acc_norm": 0.7543859649122807,
      "acc_norm_stderr": 0.03301405946987249
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/baichuan_13b',trust_remote_code=True,use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_baichuan13b_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "3:24:39.328259",
    "model_name": "baichuan_13b"
  }
}
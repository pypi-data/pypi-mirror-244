{
  "results": {
    "agi_eval-aqua-rat": {
      "acc": 0.2047244094488189,
      "acc_stderr": 0.025367833544738514
    },
    "agi_eval-gaokao-biology": {
      "acc": 0.24761904761904763,
      "acc_stderr": 0.02985642316467189
    },
    "agi_eval-gaokao-chemistry": {
      "acc": 0.30917874396135264,
      "acc_stderr": 0.0321998649400045
    },
    "agi_eval-gaokao-chinese": {
      "acc": 0.24796747967479674,
      "acc_stderr": 0.027588788664854164
    },
    "agi_eval-gaokao-english": {
      "acc": 0.5522875816993464,
      "acc_stderr": 0.028472938478033526
    },
    "agi_eval-gaokao-geography": {
      "acc": 0.36180904522613067,
      "acc_stderr": 0.034149349640988196
    },
    "agi_eval-gaokao-history": {
      "acc": 0.33191489361702126,
      "acc_stderr": 0.030783736757745647
    },
    "agi_eval-gaokao-mathqa": {
      "acc": 0.2792022792022792,
      "acc_stderr": 0.023979060299146246
    },
    "agi_eval-logiqa-en": {
      "acc": 0.29339477726574503,
      "acc_stderr": 0.017859032704399504
    },
    "agi_eval-logiqa-zh": {
      "acc": 0.27342549923195086,
      "acc_stderr": 0.01748247454768128
    },
    "agi_eval-lsat-ar": {
      "acc": 0.2,
      "acc_stderr": 0.026432744018203554
    },
    "agi_eval-lsat-lr": {
      "acc": 0.29215686274509806,
      "acc_stderr": 0.02015661811960828
    },
    "agi_eval-lsat-rc": {
      "acc": 0.3159851301115242,
      "acc_stderr": 0.028398715525657033
    },
    "agi_eval-sat-en": {
      "acc": 0.4563106796116505,
      "acc_stderr": 0.034787945997877434
    },
    "agi_eval-sat-en-without-passage": {
      "acc": 0.3446601941747573,
      "acc_stderr": 0.033193412858590815
    },
    "agi_eval-sat-math": {
      "acc": 0.2590909090909091,
      "acc_stderr": 0.029606460630040726
    }
  },
  "versions": {
    "agi_eval-aqua-rat": 0,
    "agi_eval-gaokao-biology": 0,
    "agi_eval-gaokao-chemistry": 0,
    "agi_eval-gaokao-chinese": 0,
    "agi_eval-gaokao-english": 0,
    "agi_eval-gaokao-geography": 0,
    "agi_eval-gaokao-history": 0,
    "agi_eval-gaokao-mathqa": 0,
    "agi_eval-logiqa-en": 0,
    "agi_eval-logiqa-zh": 0,
    "agi_eval-lsat-ar": 0,
    "agi_eval-lsat-lr": 0,
    "agi_eval-lsat-rc": 0,
    "agi_eval-sat-en": 0,
    "agi_eval-sat-en-without-passage": 0,
    "agi_eval-sat-math": 0
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/ziya_llama_13b/Ziya-LLaMA-13B,load_in_8bit=True,dtype='float16',use_accelerate=False,peft=/home/finetuned_models/my_ziya_llama13b/final",
    "num_fewshot": 0,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:13:24.915563",
    "model_name": "ziya_llama_13b"
  }
}
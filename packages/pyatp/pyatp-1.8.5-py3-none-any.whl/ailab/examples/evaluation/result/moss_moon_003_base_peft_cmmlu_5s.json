{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.2781065088757396,
      "acc_stderr": 0.03456905430376244,
      "acc_norm": 0.2781065088757396,
      "acc_norm_stderr": 0.03456905430376244
    },
    "Cmmlu-anatomy": {
      "acc": 0.22972972972972974,
      "acc_stderr": 0.03469536825407606,
      "acc_norm": 0.22972972972972974,
      "acc_norm_stderr": 0.03469536825407606
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.18902439024390244,
      "acc_stderr": 0.030666839281449514,
      "acc_norm": 0.18902439024390244,
      "acc_norm_stderr": 0.030666839281449514
    },
    "Cmmlu-arts": {
      "acc": 0.3125,
      "acc_stderr": 0.03675892481369823,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.03675892481369823
    },
    "Cmmlu-astronomy": {
      "acc": 0.2787878787878788,
      "acc_stderr": 0.035014387062967806,
      "acc_norm": 0.2787878787878788,
      "acc_norm_stderr": 0.035014387062967806
    },
    "Cmmlu-business_ethics": {
      "acc": 0.3253588516746411,
      "acc_stderr": 0.03248523846063361,
      "acc_norm": 0.3253588516746411,
      "acc_norm_stderr": 0.03248523846063361
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.25,
      "acc_stderr": 0.03434014098717226,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03434014098717226
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.3053435114503817,
      "acc_stderr": 0.040393149787245605,
      "acc_norm": 0.3053435114503817,
      "acc_norm_stderr": 0.040393149787245605
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.3014705882352941,
      "acc_stderr": 0.039495529298273935,
      "acc_norm": 0.3014705882352941,
      "acc_norm_stderr": 0.039495529298273935
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.308411214953271,
      "acc_stderr": 0.04485760883316699,
      "acc_norm": 0.308411214953271,
      "acc_norm_stderr": 0.04485760883316699
    },
    "Cmmlu-chinese_history": {
      "acc": 0.2260061919504644,
      "acc_stderr": 0.023307783544544925,
      "acc_norm": 0.2260061919504644,
      "acc_norm_stderr": 0.023307783544544925
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.03149328104507957,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.03149328104507957
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.2849162011173184,
      "acc_stderr": 0.03383195081328523,
      "acc_norm": 0.2849162011173184,
      "acc_norm_stderr": 0.03383195081328523
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.270042194092827,
      "acc_stderr": 0.028900721906293426,
      "acc_norm": 0.270042194092827,
      "acc_norm_stderr": 0.028900721906293426
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2169811320754717,
      "acc_stderr": 0.040225592469367126,
      "acc_norm": 0.2169811320754717,
      "acc_norm_stderr": 0.040225592469367126
    },
    "Cmmlu-college_education": {
      "acc": 0.37383177570093457,
      "acc_stderr": 0.04699273118994851,
      "acc_norm": 0.37383177570093457,
      "acc_norm_stderr": 0.04699273118994851
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3490566037735849,
      "acc_stderr": 0.04651841326529026,
      "acc_norm": 0.3490566037735849,
      "acc_norm_stderr": 0.04651841326529026
    },
    "Cmmlu-college_law": {
      "acc": 0.28703703703703703,
      "acc_stderr": 0.043733130409147614,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.043733130409147614
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.044298119496145844,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.044298119496145844
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.22641509433962265,
      "acc_stderr": 0.040842473153370994,
      "acc_norm": 0.22641509433962265,
      "acc_norm_stderr": 0.040842473153370994
    },
    "Cmmlu-college_medicine": {
      "acc": 0.27106227106227104,
      "acc_stderr": 0.02695226692070332,
      "acc_norm": 0.27106227106227104,
      "acc_norm_stderr": 0.02695226692070332
    },
    "Cmmlu-computer_science": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.031321798030832904,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.031321798030832904
    },
    "Cmmlu-computer_security": {
      "acc": 0.26900584795321636,
      "acc_stderr": 0.03401052620104089,
      "acc_norm": 0.26900584795321636,
      "acc_norm_stderr": 0.03401052620104089
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.29931972789115646,
      "acc_stderr": 0.0379010453091039,
      "acc_norm": 0.29931972789115646,
      "acc_norm_stderr": 0.0379010453091039
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.3597122302158273,
      "acc_stderr": 0.04085316066826206,
      "acc_norm": 0.3597122302158273,
      "acc_norm_stderr": 0.04085316066826206
    },
    "Cmmlu-economics": {
      "acc": 0.27672955974842767,
      "acc_stderr": 0.03559177035707934,
      "acc_norm": 0.27672955974842767,
      "acc_norm_stderr": 0.03559177035707934
    },
    "Cmmlu-education": {
      "acc": 0.31901840490797545,
      "acc_stderr": 0.03661997551073836,
      "acc_norm": 0.31901840490797545,
      "acc_norm_stderr": 0.03661997551073836
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.313953488372093,
      "acc_stderr": 0.03549043982227172,
      "acc_norm": 0.313953488372093,
      "acc_norm_stderr": 0.03549043982227172
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.25793650793650796,
      "acc_stderr": 0.027614684139414543,
      "acc_norm": 0.25793650793650796,
      "acc_norm_stderr": 0.027614684139414543
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.29292929292929293,
      "acc_stderr": 0.03242497958178817,
      "acc_norm": 0.29292929292929293,
      "acc_norm_stderr": 0.03242497958178817
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.3025210084033613,
      "acc_stderr": 0.029837962388291932,
      "acc_norm": 0.3025210084033613,
      "acc_norm_stderr": 0.029837962388291932
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.21739130434782608,
      "acc_stderr": 0.02725685083881996,
      "acc_norm": 0.21739130434782608,
      "acc_norm_stderr": 0.02725685083881996
    },
    "Cmmlu-ethnology": {
      "acc": 0.22962962962962963,
      "acc_stderr": 0.036333844140734636,
      "acc_norm": 0.22962962962962963,
      "acc_norm_stderr": 0.036333844140734636
    },
    "Cmmlu-food_science": {
      "acc": 0.34965034965034963,
      "acc_stderr": 0.04001716028382395,
      "acc_norm": 0.34965034965034963,
      "acc_norm_stderr": 0.04001716028382395
    },
    "Cmmlu-genetics": {
      "acc": 0.30113636363636365,
      "acc_stderr": 0.03467837977202437,
      "acc_norm": 0.30113636363636365,
      "acc_norm_stderr": 0.03467837977202437
    },
    "Cmmlu-global_facts": {
      "acc": 0.26174496644295303,
      "acc_stderr": 0.036133623910754545,
      "acc_norm": 0.26174496644295303,
      "acc_norm_stderr": 0.036133623910754545
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.33136094674556216,
      "acc_stderr": 0.03631548844087171,
      "acc_norm": 0.33136094674556216,
      "acc_norm_stderr": 0.03631548844087171
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.19696969696969696,
      "acc_stderr": 0.03474801718164945,
      "acc_norm": 0.19696969696969696,
      "acc_norm_stderr": 0.03474801718164945
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2711864406779661,
      "acc_stderr": 0.04110070549339209,
      "acc_norm": 0.2711864406779661,
      "acc_norm_stderr": 0.04110070549339209
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2682926829268293,
      "acc_stderr": 0.03470398212814535,
      "acc_norm": 0.2682926829268293,
      "acc_norm_stderr": 0.03470398212814535
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.24545454545454545,
      "acc_stderr": 0.04122066502878285,
      "acc_norm": 0.24545454545454545,
      "acc_norm_stderr": 0.04122066502878285
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.24475524475524477,
      "acc_stderr": 0.03607993033081378,
      "acc_norm": 0.24475524475524477,
      "acc_norm_stderr": 0.03607993033081378
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.041049472699033945,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.041049472699033945
    },
    "Cmmlu-international_law": {
      "acc": 0.2972972972972973,
      "acc_stderr": 0.03369553691877717,
      "acc_norm": 0.2972972972972973,
      "acc_norm_stderr": 0.03369553691877717
    },
    "Cmmlu-journalism": {
      "acc": 0.25,
      "acc_stderr": 0.033113308926626096,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.033113308926626096
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.29927007299270075,
      "acc_stderr": 0.022615961145736805,
      "acc_norm": 0.29927007299270075,
      "acc_norm_stderr": 0.022615961145736805
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.37850467289719625,
      "acc_stderr": 0.033232633255714746,
      "acc_norm": 0.37850467289719625,
      "acc_norm_stderr": 0.033232633255714746
    },
    "Cmmlu-logical": {
      "acc": 0.34146341463414637,
      "acc_stderr": 0.04293209956379032,
      "acc_norm": 0.34146341463414637,
      "acc_norm_stderr": 0.04293209956379032
    },
    "Cmmlu-machine_learning": {
      "acc": 0.29508196721311475,
      "acc_stderr": 0.04146178164901212,
      "acc_norm": 0.29508196721311475,
      "acc_norm_stderr": 0.04146178164901212
    },
    "Cmmlu-management": {
      "acc": 0.29523809523809524,
      "acc_stderr": 0.031552535545053974,
      "acc_norm": 0.29523809523809524,
      "acc_norm_stderr": 0.031552535545053974
    },
    "Cmmlu-marketing": {
      "acc": 0.3277777777777778,
      "acc_stderr": 0.03508485373860689,
      "acc_norm": 0.3277777777777778,
      "acc_norm_stderr": 0.03508485373860689
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.31216931216931215,
      "acc_stderr": 0.03379535035917228,
      "acc_norm": 0.31216931216931215,
      "acc_norm_stderr": 0.03379535035917228
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.1724137931034483,
      "acc_stderr": 0.03522440816932755,
      "acc_norm": 0.1724137931034483,
      "acc_norm_stderr": 0.03522440816932755
    },
    "Cmmlu-nutrition": {
      "acc": 0.2827586206896552,
      "acc_stderr": 0.03752833958003337,
      "acc_norm": 0.2827586206896552,
      "acc_norm_stderr": 0.03752833958003337
    },
    "Cmmlu-philosophy": {
      "acc": 0.3523809523809524,
      "acc_stderr": 0.04684350139437753,
      "acc_norm": 0.3523809523809524,
      "acc_norm_stderr": 0.04684350139437753
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.26857142857142857,
      "acc_stderr": 0.033600151915923894,
      "acc_norm": 0.26857142857142857,
      "acc_norm_stderr": 0.033600151915923894
    },
    "Cmmlu-professional_law": {
      "acc": 0.26540284360189575,
      "acc_stderr": 0.030469670650846666,
      "acc_norm": 0.26540284360189575,
      "acc_norm_stderr": 0.030469670650846666
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2632978723404255,
      "acc_stderr": 0.022743327388426438,
      "acc_norm": 0.2632978723404255,
      "acc_norm_stderr": 0.022743327388426438
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.30603448275862066,
      "acc_stderr": 0.03032132235357866,
      "acc_norm": 0.30603448275862066,
      "acc_norm_stderr": 0.03032132235357866
    },
    "Cmmlu-public_relations": {
      "acc": 0.22988505747126436,
      "acc_stderr": 0.03198969467577206,
      "acc_norm": 0.22988505747126436,
      "acc_norm_stderr": 0.03198969467577206
    },
    "Cmmlu-security_study": {
      "acc": 0.2962962962962963,
      "acc_stderr": 0.03944624162501116,
      "acc_norm": 0.2962962962962963,
      "acc_norm_stderr": 0.03944624162501116
    },
    "Cmmlu-sociology": {
      "acc": 0.2831858407079646,
      "acc_stderr": 0.030036394245092305,
      "acc_norm": 0.2831858407079646,
      "acc_norm_stderr": 0.030036394245092305
    },
    "Cmmlu-sports_science": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.033175059300091826,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.033175059300091826
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.25405405405405407,
      "acc_stderr": 0.03209281645145386,
      "acc_norm": 0.25405405405405407,
      "acc_norm_stderr": 0.03209281645145386
    },
    "Cmmlu-virology": {
      "acc": 0.33727810650887574,
      "acc_stderr": 0.03647582250277504,
      "acc_norm": 0.33727810650887574,
      "acc_norm_stderr": 0.03647582250277504
    },
    "Cmmlu-world_history": {
      "acc": 0.32919254658385094,
      "acc_stderr": 0.03715043857896316,
      "acc_norm": 0.32919254658385094,
      "acc_norm_stderr": 0.03715043857896316
    },
    "Cmmlu-world_religions": {
      "acc": 0.36875,
      "acc_stderr": 0.03826204233503226,
      "acc_norm": 0.36875,
      "acc_norm_stderr": 0.03826204233503226
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/moss-moon-003-base,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_moss_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:53:22.329137",
    "model_name": "moss_moon_003_base"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-anatomy": {
      "acc": 0.4888888888888889,
      "acc_stderr": 0.043182754919779756,
      "acc_norm": 0.4888888888888889,
      "acc_norm_stderr": 0.043182754919779756
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5592105263157895,
      "acc_stderr": 0.04040311062490436,
      "acc_norm": 0.5592105263157895,
      "acc_norm_stderr": 0.04040311062490436
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5584905660377358,
      "acc_stderr": 0.030561590426731837,
      "acc_norm": 0.5584905660377358,
      "acc_norm_stderr": 0.030561590426731837
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5902777777777778,
      "acc_stderr": 0.04112490974670787,
      "acc_norm": 0.5902777777777778,
      "acc_norm_stderr": 0.04112490974670787
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252605,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252605
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.34,
      "acc_stderr": 0.047609522856952344,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.047609522856952344
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.4624277456647399,
      "acc_stderr": 0.0380168510452446,
      "acc_norm": 0.4624277456647399,
      "acc_norm_stderr": 0.0380168510452446
    },
    "hendrycksTest-college_physics": {
      "acc": 0.28431372549019607,
      "acc_stderr": 0.04488482852329017,
      "acc_norm": 0.28431372549019607,
      "acc_norm_stderr": 0.04488482852329017
    },
    "hendrycksTest-computer_security": {
      "acc": 0.69,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.69,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.451063829787234,
      "acc_stderr": 0.032529096196131965,
      "acc_norm": 0.451063829787234,
      "acc_norm_stderr": 0.032529096196131965
    },
    "hendrycksTest-econometrics": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.043727482902780064,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.043727482902780064
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.5862068965517241,
      "acc_stderr": 0.041042692118062316,
      "acc_norm": 0.5862068965517241,
      "acc_norm_stderr": 0.041042692118062316
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.36772486772486773,
      "acc_stderr": 0.024833839825562413,
      "acc_norm": 0.36772486772486773,
      "acc_norm_stderr": 0.024833839825562413
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.0393253768039287,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.0393253768039287
    },
    "hendrycksTest-global_facts": {
      "acc": 0.38,
      "acc_stderr": 0.048783173121456316,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.048783173121456316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6258064516129033,
      "acc_stderr": 0.027528904299845704,
      "acc_norm": 0.6258064516129033,
      "acc_norm_stderr": 0.027528904299845704
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.37438423645320196,
      "acc_stderr": 0.03405155380561952,
      "acc_norm": 0.37438423645320196,
      "acc_norm_stderr": 0.03405155380561952
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.296969696969697,
      "acc_stderr": 0.03567969772268049,
      "acc_norm": 0.296969696969697,
      "acc_norm_stderr": 0.03567969772268049
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.7222222222222222,
      "acc_stderr": 0.03191178226713548,
      "acc_norm": 0.7222222222222222,
      "acc_norm_stderr": 0.03191178226713548
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.8134715025906736,
      "acc_stderr": 0.02811209121011746,
      "acc_norm": 0.8134715025906736,
      "acc_norm_stderr": 0.02811209121011746
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4897435897435897,
      "acc_stderr": 0.025345672221942374,
      "acc_norm": 0.4897435897435897,
      "acc_norm_stderr": 0.025345672221942374
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.23703703703703705,
      "acc_stderr": 0.02592887613276612,
      "acc_norm": 0.23703703703703705,
      "acc_norm_stderr": 0.02592887613276612
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5210084033613446,
      "acc_stderr": 0.032449808499900284,
      "acc_norm": 0.5210084033613446,
      "acc_norm_stderr": 0.032449808499900284
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.037345356767871984,
      "acc_norm": 0.2980132450331126,
      "acc_norm_stderr": 0.037345356767871984
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7211009174311926,
      "acc_stderr": 0.019227468876463507,
      "acc_norm": 0.7211009174311926,
      "acc_norm_stderr": 0.019227468876463507
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.38425925925925924,
      "acc_stderr": 0.03317354514310742,
      "acc_norm": 0.38425925925925924,
      "acc_norm_stderr": 0.03317354514310742
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.4362745098039216,
      "acc_stderr": 0.03480693138457038,
      "acc_norm": 0.4362745098039216,
      "acc_norm_stderr": 0.03480693138457038
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6075949367088608,
      "acc_stderr": 0.03178471874564729,
      "acc_norm": 0.6075949367088608,
      "acc_norm_stderr": 0.03178471874564729
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5964125560538116,
      "acc_stderr": 0.03292802819330314,
      "acc_norm": 0.5964125560538116,
      "acc_norm_stderr": 0.03292802819330314
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6183206106870229,
      "acc_stderr": 0.04260735157644559,
      "acc_norm": 0.6183206106870229,
      "acc_norm_stderr": 0.04260735157644559
    },
    "hendrycksTest-international_law": {
      "acc": 0.7024793388429752,
      "acc_stderr": 0.04173349148083499,
      "acc_norm": 0.7024793388429752,
      "acc_norm_stderr": 0.04173349148083499
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.04557239513497751,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.04557239513497751
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6625766871165644,
      "acc_stderr": 0.03714908409935573,
      "acc_norm": 0.6625766871165644,
      "acc_norm_stderr": 0.03714908409935573
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.32142857142857145,
      "acc_stderr": 0.04432804055291519,
      "acc_norm": 0.32142857142857145,
      "acc_norm_stderr": 0.04432804055291519
    },
    "hendrycksTest-management": {
      "acc": 0.7087378640776699,
      "acc_stderr": 0.044986763205729224,
      "acc_norm": 0.7087378640776699,
      "acc_norm_stderr": 0.044986763205729224
    },
    "hendrycksTest-marketing": {
      "acc": 0.8076923076923077,
      "acc_stderr": 0.02581923325648371,
      "acc_norm": 0.8076923076923077,
      "acc_norm_stderr": 0.02581923325648371
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.58,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.7547892720306514,
      "acc_stderr": 0.01538435228454394,
      "acc_norm": 0.7547892720306514,
      "acc_norm_stderr": 0.01538435228454394
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5924855491329479,
      "acc_stderr": 0.026454578146931505,
      "acc_norm": 0.5924855491329479,
      "acc_norm_stderr": 0.026454578146931505
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.264804469273743,
      "acc_stderr": 0.01475690648326066,
      "acc_norm": 0.264804469273743,
      "acc_norm_stderr": 0.01475690648326066
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5980392156862745,
      "acc_stderr": 0.028074158947600656,
      "acc_norm": 0.5980392156862745,
      "acc_norm_stderr": 0.028074158947600656
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6205787781350482,
      "acc_stderr": 0.02755994980234782,
      "acc_norm": 0.6205787781350482,
      "acc_norm_stderr": 0.02755994980234782
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6327160493827161,
      "acc_stderr": 0.026822801759507894,
      "acc_norm": 0.6327160493827161,
      "acc_norm_stderr": 0.026822801759507894
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.3900709219858156,
      "acc_stderr": 0.02909767559946393,
      "acc_norm": 0.3900709219858156,
      "acc_norm_stderr": 0.02909767559946393
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3833116036505867,
      "acc_stderr": 0.012417603662901188,
      "acc_norm": 0.3833116036505867,
      "acc_norm_stderr": 0.012417603662901188
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.029896163033125468,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.029896163033125468
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5179738562091504,
      "acc_stderr": 0.020214761037872404,
      "acc_norm": 0.5179738562091504,
      "acc_norm_stderr": 0.020214761037872404
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6181818181818182,
      "acc_stderr": 0.046534298079135075,
      "acc_norm": 0.6181818181818182,
      "acc_norm_stderr": 0.046534298079135075
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5551020408163265,
      "acc_stderr": 0.03181425118197786,
      "acc_norm": 0.5551020408163265,
      "acc_norm_stderr": 0.03181425118197786
    },
    "hendrycksTest-sociology": {
      "acc": 0.746268656716418,
      "acc_stderr": 0.030769444967296018,
      "acc_norm": 0.746268656716418,
      "acc_norm_stderr": 0.030769444967296018
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.77,
      "acc_stderr": 0.042295258468165065,
      "acc_norm": 0.77,
      "acc_norm_stderr": 0.042295258468165065
    },
    "hendrycksTest-virology": {
      "acc": 0.4457831325301205,
      "acc_stderr": 0.03869543323472101,
      "acc_norm": 0.4457831325301205,
      "acc_norm_stderr": 0.03869543323472101
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7309941520467836,
      "acc_stderr": 0.034010526201040885,
      "acc_norm": 0.7309941520467836,
      "acc_norm_stderr": 0.034010526201040885
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/baichuan_13b',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "3:17:03.227807",
    "model_name": "baichuan_13b"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.35502958579881655,
      "acc_stderr": 0.03691879594576913,
      "acc_norm": 0.35502958579881655,
      "acc_norm_stderr": 0.03691879594576913
    },
    "Cmmlu-anatomy": {
      "acc": 0.24324324324324326,
      "acc_stderr": 0.035386684903133896,
      "acc_norm": 0.24324324324324326,
      "acc_norm_stderr": 0.035386684903133896
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.18902439024390244,
      "acc_stderr": 0.030666839281449514,
      "acc_norm": 0.18902439024390244,
      "acc_norm_stderr": 0.030666839281449514
    },
    "Cmmlu-arts": {
      "acc": 0.3,
      "acc_stderr": 0.036342189215581536,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.036342189215581536
    },
    "Cmmlu-astronomy": {
      "acc": 0.296969696969697,
      "acc_stderr": 0.03567969772268049,
      "acc_norm": 0.296969696969697,
      "acc_norm_stderr": 0.03567969772268049
    },
    "Cmmlu-business_ethics": {
      "acc": 0.40669856459330145,
      "acc_stderr": 0.03405982026516628,
      "acc_norm": 0.40669856459330145,
      "acc_norm_stderr": 0.03405982026516628
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.31875,
      "acc_stderr": 0.036955560385363254,
      "acc_norm": 0.31875,
      "acc_norm_stderr": 0.036955560385363254
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.4961832061068702,
      "acc_stderr": 0.043851623256015534,
      "acc_norm": 0.4961832061068702,
      "acc_norm_stderr": 0.043851623256015534
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.33088235294117646,
      "acc_stderr": 0.04049684225945661,
      "acc_norm": 0.33088235294117646,
      "acc_norm_stderr": 0.04049684225945661
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.3177570093457944,
      "acc_stderr": 0.045223500773820306,
      "acc_norm": 0.3177570093457944,
      "acc_norm_stderr": 0.045223500773820306
    },
    "Cmmlu-chinese_history": {
      "acc": 0.38699690402476783,
      "acc_stderr": 0.027142956048365807,
      "acc_norm": 0.38699690402476783,
      "acc_norm_stderr": 0.027142956048365807
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.030964517926923403,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.030964517926923403
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.39664804469273746,
      "acc_stderr": 0.036667223012526735,
      "acc_norm": 0.39664804469273746,
      "acc_norm_stderr": 0.036667223012526735
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.3037974683544304,
      "acc_stderr": 0.029936696387138605,
      "acc_norm": 0.3037974683544304,
      "acc_norm_stderr": 0.029936696387138605
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2641509433962264,
      "acc_stderr": 0.0430254877395901,
      "acc_norm": 0.2641509433962264,
      "acc_norm_stderr": 0.0430254877395901
    },
    "Cmmlu-college_education": {
      "acc": 0.35514018691588783,
      "acc_stderr": 0.04648144634449113,
      "acc_norm": 0.35514018691588783,
      "acc_norm_stderr": 0.04648144634449113
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.41509433962264153,
      "acc_stderr": 0.04808633394970664,
      "acc_norm": 0.41509433962264153,
      "acc_norm_stderr": 0.04808633394970664
    },
    "Cmmlu-college_law": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.04524596007030048,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.04524596007030048
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.24761904761904763,
      "acc_stderr": 0.04232473532055043,
      "acc_norm": 0.24761904761904763,
      "acc_norm_stderr": 0.04232473532055043
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.2169811320754717,
      "acc_stderr": 0.040225592469367126,
      "acc_norm": 0.2169811320754717,
      "acc_norm_stderr": 0.040225592469367126
    },
    "Cmmlu-college_medicine": {
      "acc": 0.31868131868131866,
      "acc_stderr": 0.02825328818739862,
      "acc_norm": 0.31868131868131866,
      "acc_norm_stderr": 0.02825328818739862
    },
    "Cmmlu-computer_science": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.03460228327239171,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.03460228327239171
    },
    "Cmmlu-computer_security": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03615507630310935,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03615507630310935
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.35374149659863946,
      "acc_stderr": 0.03957033361777323,
      "acc_norm": 0.35374149659863946,
      "acc_norm_stderr": 0.03957033361777323
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.3237410071942446,
      "acc_stderr": 0.039830507521054596,
      "acc_norm": 0.3237410071942446,
      "acc_norm_stderr": 0.039830507521054596
    },
    "Cmmlu-economics": {
      "acc": 0.36477987421383645,
      "acc_stderr": 0.03829561213441044,
      "acc_norm": 0.36477987421383645,
      "acc_norm_stderr": 0.03829561213441044
    },
    "Cmmlu-education": {
      "acc": 0.3987730061349693,
      "acc_stderr": 0.03847021420456023,
      "acc_norm": 0.3987730061349693,
      "acc_norm_stderr": 0.03847021420456023
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.4186046511627907,
      "acc_stderr": 0.037725911890875034,
      "acc_norm": 0.4186046511627907,
      "acc_norm_stderr": 0.037725911890875034
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.24206349206349206,
      "acc_stderr": 0.02703610967923697,
      "acc_norm": 0.24206349206349206,
      "acc_norm_stderr": 0.02703610967923697
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.29292929292929293,
      "acc_stderr": 0.032424979581788194,
      "acc_norm": 0.29292929292929293,
      "acc_norm_stderr": 0.032424979581788194
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.4369747899159664,
      "acc_stderr": 0.032219436365661956,
      "acc_norm": 0.4369747899159664,
      "acc_norm_stderr": 0.032219436365661956
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.27391304347826084,
      "acc_stderr": 0.0294701898150059,
      "acc_norm": 0.27391304347826084,
      "acc_norm_stderr": 0.0294701898150059
    },
    "Cmmlu-ethnology": {
      "acc": 0.32592592592592595,
      "acc_stderr": 0.040491220417025055,
      "acc_norm": 0.32592592592592595,
      "acc_norm_stderr": 0.040491220417025055
    },
    "Cmmlu-food_science": {
      "acc": 0.3706293706293706,
      "acc_stderr": 0.040530221749257606,
      "acc_norm": 0.3706293706293706,
      "acc_norm_stderr": 0.040530221749257606
    },
    "Cmmlu-genetics": {
      "acc": 0.32386363636363635,
      "acc_stderr": 0.03537359640062134,
      "acc_norm": 0.32386363636363635,
      "acc_norm_stderr": 0.03537359640062134
    },
    "Cmmlu-global_facts": {
      "acc": 0.31543624161073824,
      "acc_stderr": 0.03819723167141383,
      "acc_norm": 0.31543624161073824,
      "acc_norm_stderr": 0.03819723167141383
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.23668639053254437,
      "acc_stderr": 0.03279317792268948,
      "acc_norm": 0.23668639053254437,
      "acc_norm_stderr": 0.03279317792268948
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.2878787878787879,
      "acc_stderr": 0.039559076642353884,
      "acc_norm": 0.2878787878787879,
      "acc_norm_stderr": 0.039559076642353884
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3305084745762712,
      "acc_stderr": 0.043488147791922734,
      "acc_norm": 0.3305084745762712,
      "acc_norm_stderr": 0.043488147791922734
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.22560975609756098,
      "acc_stderr": 0.03273897454566342,
      "acc_norm": 0.22560975609756098,
      "acc_norm_stderr": 0.03273897454566342
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3,
      "acc_stderr": 0.04389311454644286,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.04389311454644286
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.040368457798807794,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.040368457798807794
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04216370213557836,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04216370213557836
    },
    "Cmmlu-international_law": {
      "acc": 0.32432432432432434,
      "acc_stderr": 0.034510399895624946,
      "acc_norm": 0.32432432432432434,
      "acc_norm_stderr": 0.034510399895624946
    },
    "Cmmlu-journalism": {
      "acc": 0.37790697674418605,
      "acc_stderr": 0.037078492187232796,
      "acc_norm": 0.37790697674418605,
      "acc_norm_stderr": 0.037078492187232796
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.3381995133819951,
      "acc_stderr": 0.023364586634695362,
      "acc_norm": 0.3381995133819951,
      "acc_norm_stderr": 0.023364586634695362
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.46261682242990654,
      "acc_stderr": 0.03416354604102856,
      "acc_norm": 0.46261682242990654,
      "acc_norm_stderr": 0.03416354604102856
    },
    "Cmmlu-logical": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.041193230302085666,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.041193230302085666
    },
    "Cmmlu-machine_learning": {
      "acc": 0.3114754098360656,
      "acc_stderr": 0.04209969267310141,
      "acc_norm": 0.3114754098360656,
      "acc_norm_stderr": 0.04209969267310141
    },
    "Cmmlu-management": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03260773253630125,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03260773253630125
    },
    "Cmmlu-marketing": {
      "acc": 0.35555555555555557,
      "acc_stderr": 0.035778321396489204,
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.035778321396489204
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.3968253968253968,
      "acc_stderr": 0.03568143635446716,
      "acc_norm": 0.3968253968253968,
      "acc_norm_stderr": 0.03568143635446716
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.25,
      "acc_stderr": 0.04037864265436242,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04037864265436242
    },
    "Cmmlu-nutrition": {
      "acc": 0.3310344827586207,
      "acc_stderr": 0.039215453124671215,
      "acc_norm": 0.3310344827586207,
      "acc_norm_stderr": 0.039215453124671215
    },
    "Cmmlu-philosophy": {
      "acc": 0.3523809523809524,
      "acc_stderr": 0.04684350139437753,
      "acc_norm": 0.3523809523809524,
      "acc_norm_stderr": 0.04684350139437753
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.4514285714285714,
      "acc_stderr": 0.03772562898529837,
      "acc_norm": 0.4514285714285714,
      "acc_norm_stderr": 0.03772562898529837
    },
    "Cmmlu-professional_law": {
      "acc": 0.2796208530805687,
      "acc_stderr": 0.030971033440870915,
      "acc_norm": 0.2796208530805687,
      "acc_norm_stderr": 0.030971033440870915
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2872340425531915,
      "acc_stderr": 0.023365538575816754,
      "acc_norm": 0.2872340425531915,
      "acc_norm_stderr": 0.023365538575816754
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.35344827586206895,
      "acc_stderr": 0.031452746950022696,
      "acc_norm": 0.35344827586206895,
      "acc_norm_stderr": 0.031452746950022696
    },
    "Cmmlu-public_relations": {
      "acc": 0.3563218390804598,
      "acc_stderr": 0.036410995772554904,
      "acc_norm": 0.3563218390804598,
      "acc_norm_stderr": 0.036410995772554904
    },
    "Cmmlu-security_study": {
      "acc": 0.3851851851851852,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.3851851851851852,
      "acc_norm_stderr": 0.042039210401562783
    },
    "Cmmlu-sociology": {
      "acc": 0.35398230088495575,
      "acc_stderr": 0.0318802503506933,
      "acc_norm": 0.35398230088495575,
      "acc_norm_stderr": 0.0318802503506933
    },
    "Cmmlu-sports_science": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.03713158067481913,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.03713158067481913
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.32432432432432434,
      "acc_stderr": 0.034510399895624946,
      "acc_norm": 0.32432432432432434,
      "acc_norm_stderr": 0.034510399895624946
    },
    "Cmmlu-virology": {
      "acc": 0.40236686390532544,
      "acc_stderr": 0.03783326285416537,
      "acc_norm": 0.40236686390532544,
      "acc_norm_stderr": 0.03783326285416537
    },
    "Cmmlu-world_history": {
      "acc": 0.40372670807453415,
      "acc_stderr": 0.03878880744346832,
      "acc_norm": 0.40372670807453415,
      "acc_norm_stderr": 0.03878880744346832
    },
    "Cmmlu-world_religions": {
      "acc": 0.36875,
      "acc_stderr": 0.03826204233503226,
      "acc_norm": 0.36875,
      "acc_norm_stderr": 0.03826204233503226
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/belle_7b_2m,dtype='bfloat16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_belle_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:40:10.373536",
    "model_name": "belle_7b_2m"
  }
}
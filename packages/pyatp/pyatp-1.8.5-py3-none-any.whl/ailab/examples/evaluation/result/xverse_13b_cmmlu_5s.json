{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.5562130177514792,
      "acc_stderr": 0.03833127038718733,
      "acc_norm": 0.5562130177514792,
      "acc_norm_stderr": 0.03833127038718733
    },
    "Cmmlu-anatomy": {
      "acc": 0.5,
      "acc_stderr": 0.041239304942116126,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.041239304942116126
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.36585365853658536,
      "acc_stderr": 0.03772720610111752,
      "acc_norm": 0.36585365853658536,
      "acc_norm_stderr": 0.03772720610111752
    },
    "Cmmlu-arts": {
      "acc": 0.8125,
      "acc_stderr": 0.030953784783978033,
      "acc_norm": 0.8125,
      "acc_norm_stderr": 0.030953784783978033
    },
    "Cmmlu-astronomy": {
      "acc": 0.4,
      "acc_stderr": 0.038254602783800246,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.038254602783800246
    },
    "Cmmlu-business_ethics": {
      "acc": 0.6172248803827751,
      "acc_stderr": 0.03370248274774052,
      "acc_norm": 0.6172248803827751,
      "acc_norm_stderr": 0.03370248274774052
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.5375,
      "acc_stderr": 0.039540899134978144,
      "acc_norm": 0.5375,
      "acc_norm_stderr": 0.039540899134978144
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.732824427480916,
      "acc_stderr": 0.038808483010823944,
      "acc_norm": 0.732824427480916,
      "acc_norm_stderr": 0.038808483010823944
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.5661764705882353,
      "acc_stderr": 0.04265457074552586,
      "acc_norm": 0.5661764705882353,
      "acc_norm_stderr": 0.04265457074552586
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.6355140186915887,
      "acc_stderr": 0.046746602211107775,
      "acc_norm": 0.6355140186915887,
      "acc_norm_stderr": 0.046746602211107775
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6996904024767802,
      "acc_stderr": 0.025545218898401948,
      "acc_norm": 0.6996904024767802,
      "acc_norm_stderr": 0.025545218898401948
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.5490196078431373,
      "acc_stderr": 0.03492406104163613,
      "acc_norm": 0.5490196078431373,
      "acc_norm_stderr": 0.03492406104163613
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.7150837988826816,
      "acc_stderr": 0.033831950813285244,
      "acc_norm": 0.7150837988826816,
      "acc_norm_stderr": 0.033831950813285244
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.5738396624472574,
      "acc_stderr": 0.03219035703131774,
      "acc_norm": 0.5738396624472574,
      "acc_norm_stderr": 0.03219035703131774
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.330188679245283,
      "acc_stderr": 0.045894715469579954,
      "acc_norm": 0.330188679245283,
      "acc_norm_stderr": 0.045894715469579954
    },
    "Cmmlu-college_education": {
      "acc": 0.6448598130841121,
      "acc_stderr": 0.04648144634449115,
      "acc_norm": 0.6448598130841121,
      "acc_norm_stderr": 0.04648144634449115
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.5094339622641509,
      "acc_stderr": 0.04878631739837742,
      "acc_norm": 0.5094339622641509,
      "acc_norm_stderr": 0.04878631739837742
    },
    "Cmmlu-college_law": {
      "acc": 0.5185185185185185,
      "acc_stderr": 0.0483036602463533,
      "acc_norm": 0.5185185185185185,
      "acc_norm_stderr": 0.0483036602463533
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2761904761904762,
      "acc_stderr": 0.043842955869188835,
      "acc_norm": 0.2761904761904762,
      "acc_norm_stderr": 0.043842955869188835
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.5188679245283019,
      "acc_stderr": 0.04876024936691517,
      "acc_norm": 0.5188679245283019,
      "acc_norm_stderr": 0.04876024936691517
    },
    "Cmmlu-college_medicine": {
      "acc": 0.6263736263736264,
      "acc_stderr": 0.029332632560525537,
      "acc_norm": 0.6263736263736264,
      "acc_norm_stderr": 0.029332632560525537
    },
    "Cmmlu-computer_science": {
      "acc": 0.5735294117647058,
      "acc_stderr": 0.03471157907953424,
      "acc_norm": 0.5735294117647058,
      "acc_norm_stderr": 0.03471157907953424
    },
    "Cmmlu-computer_security": {
      "acc": 0.695906432748538,
      "acc_stderr": 0.03528211258245231,
      "acc_norm": 0.695906432748538,
      "acc_norm_stderr": 0.03528211258245231
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.5374149659863946,
      "acc_stderr": 0.04126427692494573,
      "acc_norm": 0.5374149659863946,
      "acc_norm_stderr": 0.04126427692494573
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.5179856115107914,
      "acc_stderr": 0.04253528098920135,
      "acc_norm": 0.5179856115107914,
      "acc_norm_stderr": 0.04253528098920135
    },
    "Cmmlu-economics": {
      "acc": 0.5534591194968553,
      "acc_stderr": 0.039549850176757044,
      "acc_norm": 0.5534591194968553,
      "acc_norm_stderr": 0.039549850176757044
    },
    "Cmmlu-education": {
      "acc": 0.6687116564417178,
      "acc_stderr": 0.03697983910025588,
      "acc_norm": 0.6687116564417178,
      "acc_norm_stderr": 0.03697983910025588
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.5406976744186046,
      "acc_stderr": 0.038109084678980304,
      "acc_norm": 0.5406976744186046,
      "acc_norm_stderr": 0.038109084678980304
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.5158730158730159,
      "acc_stderr": 0.03154381303686603,
      "acc_norm": 0.5158730158730159,
      "acc_norm_stderr": 0.03154381303686603
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.6262626262626263,
      "acc_stderr": 0.03446897738659333,
      "acc_norm": 0.6262626262626263,
      "acc_norm_stderr": 0.03446897738659333
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.7941176470588235,
      "acc_stderr": 0.02626502460827588,
      "acc_norm": 0.7941176470588235,
      "acc_norm_stderr": 0.02626502460827588
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.30434782608695654,
      "acc_stderr": 0.030406290061389885,
      "acc_norm": 0.30434782608695654,
      "acc_norm_stderr": 0.030406290061389885
    },
    "Cmmlu-ethnology": {
      "acc": 0.5925925925925926,
      "acc_stderr": 0.04244633238353229,
      "acc_norm": 0.5925925925925926,
      "acc_norm_stderr": 0.04244633238353229
    },
    "Cmmlu-food_science": {
      "acc": 0.6223776223776224,
      "acc_stderr": 0.040682878492098076,
      "acc_norm": 0.6223776223776224,
      "acc_norm_stderr": 0.040682878492098076
    },
    "Cmmlu-genetics": {
      "acc": 0.4375,
      "acc_stderr": 0.0375,
      "acc_norm": 0.4375,
      "acc_norm_stderr": 0.0375
    },
    "Cmmlu-global_facts": {
      "acc": 0.6241610738255033,
      "acc_stderr": 0.039812400260513865,
      "acc_norm": 0.6241610738255033,
      "acc_norm_stderr": 0.039812400260513865
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.5088757396449705,
      "acc_stderr": 0.038569759098794135,
      "acc_norm": 0.5088757396449705,
      "acc_norm_stderr": 0.038569759098794135
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.04295706512553091,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.04295706512553091
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.6440677966101694,
      "acc_stderr": 0.04426459583315515,
      "acc_norm": 0.6440677966101694,
      "acc_norm_stderr": 0.04426459583315515
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.25609756097560976,
      "acc_stderr": 0.03418746588364998,
      "acc_norm": 0.25609756097560976,
      "acc_norm_stderr": 0.03418746588364998
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.04709306978661896,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.04709306978661896
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5944055944055944,
      "acc_stderr": 0.04120436731133788,
      "acc_norm": 0.5944055944055944,
      "acc_norm_stderr": 0.04120436731133788
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.6428571428571429,
      "acc_stderr": 0.04285714285714281,
      "acc_norm": 0.6428571428571429,
      "acc_norm_stderr": 0.04285714285714281
    },
    "Cmmlu-international_law": {
      "acc": 0.572972972972973,
      "acc_stderr": 0.03646580777990099,
      "acc_norm": 0.572972972972973,
      "acc_norm_stderr": 0.03646580777990099
    },
    "Cmmlu-journalism": {
      "acc": 0.6569767441860465,
      "acc_stderr": 0.03630268317574834,
      "acc_norm": 0.6569767441860465,
      "acc_norm_stderr": 0.03630268317574834
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.6374695863746959,
      "acc_stderr": 0.023741602511407774,
      "acc_norm": 0.6374695863746959,
      "acc_norm_stderr": 0.023741602511407774
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.9345794392523364,
      "acc_stderr": 0.016942419881802298,
      "acc_norm": 0.9345794392523364,
      "acc_norm_stderr": 0.016942419881802298
    },
    "Cmmlu-logical": {
      "acc": 0.5040650406504065,
      "acc_stderr": 0.045266376933577476,
      "acc_norm": 0.5040650406504065,
      "acc_norm_stderr": 0.045266376933577476
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4672131147540984,
      "acc_stderr": 0.045356714734823195,
      "acc_norm": 0.4672131147540984,
      "acc_norm_stderr": 0.045356714734823195
    },
    "Cmmlu-management": {
      "acc": 0.719047619047619,
      "acc_stderr": 0.031090094469344617,
      "acc_norm": 0.719047619047619,
      "acc_norm_stderr": 0.031090094469344617
    },
    "Cmmlu-marketing": {
      "acc": 0.6555555555555556,
      "acc_stderr": 0.03551712696743982,
      "acc_norm": 0.6555555555555556,
      "acc_norm_stderr": 0.03551712696743982
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.7777777777777778,
      "acc_stderr": 0.030320934606100854,
      "acc_norm": 0.7777777777777778,
      "acc_norm_stderr": 0.030320934606100854
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.43103448275862066,
      "acc_stderr": 0.046179588699434615,
      "acc_norm": 0.43103448275862066,
      "acc_norm_stderr": 0.046179588699434615
    },
    "Cmmlu-nutrition": {
      "acc": 0.6137931034482759,
      "acc_stderr": 0.04057324734419036,
      "acc_norm": 0.6137931034482759,
      "acc_norm_stderr": 0.04057324734419036
    },
    "Cmmlu-philosophy": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.04622501635210239,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.04622501635210239
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.68,
      "acc_stderr": 0.03536346578947938,
      "acc_norm": 0.68,
      "acc_norm_stderr": 0.03536346578947938
    },
    "Cmmlu-professional_law": {
      "acc": 0.4834123222748815,
      "acc_stderr": 0.03448428551340369,
      "acc_norm": 0.4834123222748815,
      "acc_norm_stderr": 0.03448428551340369
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.5292553191489362,
      "acc_stderr": 0.02577565395492386,
      "acc_norm": 0.5292553191489362,
      "acc_norm_stderr": 0.02577565395492386
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.7112068965517241,
      "acc_stderr": 0.029818472937939,
      "acc_norm": 0.7112068965517241,
      "acc_norm_stderr": 0.029818472937939
    },
    "Cmmlu-public_relations": {
      "acc": 0.6264367816091954,
      "acc_stderr": 0.03677880611869061,
      "acc_norm": 0.6264367816091954,
      "acc_norm_stderr": 0.03677880611869061
    },
    "Cmmlu-security_study": {
      "acc": 0.6962962962962963,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.6962962962962963,
      "acc_norm_stderr": 0.039725528847851375
    },
    "Cmmlu-sociology": {
      "acc": 0.6460176991150443,
      "acc_stderr": 0.03188025035069329,
      "acc_norm": 0.6460176991150443,
      "acc_norm_stderr": 0.03188025035069329
    },
    "Cmmlu-sports_science": {
      "acc": 0.6727272727272727,
      "acc_stderr": 0.03663974994391242,
      "acc_norm": 0.6727272727272727,
      "acc_norm_stderr": 0.03663974994391242
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.6270270270270271,
      "acc_stderr": 0.03565109718452138,
      "acc_norm": 0.6270270270270271,
      "acc_norm_stderr": 0.03565109718452138
    },
    "Cmmlu-virology": {
      "acc": 0.7159763313609467,
      "acc_stderr": 0.03479140427262331,
      "acc_norm": 0.7159763313609467,
      "acc_norm_stderr": 0.03479140427262331
    },
    "Cmmlu-world_history": {
      "acc": 0.6024844720496895,
      "acc_stderr": 0.038689221123968776,
      "acc_norm": 0.6024844720496895,
      "acc_norm_stderr": 0.038689221123968776
    },
    "Cmmlu-world_religions": {
      "acc": 0.7375,
      "acc_stderr": 0.034893706520187605,
      "acc_norm": 0.7375,
      "acc_norm_stderr": 0.034893706520187605
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/xverse_13b,dtype='bfloat16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:41:13.952461",
    "model_name": "xverse_13b"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-anatomy": {
      "acc": 0.21481481481481482,
      "acc_stderr": 0.035478541985608236,
      "acc_norm": 0.21481481481481482,
      "acc_norm_stderr": 0.035478541985608236
    },
    "hendrycksTest-astronomy": {
      "acc": 0.25,
      "acc_stderr": 0.03523807393012047,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03523807393012047
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.2,
      "acc_stderr": 0.04020151261036845,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.04020151261036845
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.028254200344438655,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.028254200344438655
    },
    "hendrycksTest-college_biology": {
      "acc": 0.24305555555555555,
      "acc_stderr": 0.0358687928008034,
      "acc_norm": 0.24305555555555555,
      "acc_norm_stderr": 0.0358687928008034
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322695
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.27,
      "acc_stderr": 0.044619604333847394,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.044619604333847394
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768079,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768079
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.23121387283236994,
      "acc_stderr": 0.032147373020294696,
      "acc_norm": 0.23121387283236994,
      "acc_norm_stderr": 0.032147373020294696
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237656,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237656
    },
    "hendrycksTest-computer_security": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384739,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384739
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.2851063829787234,
      "acc_stderr": 0.029513196625539355,
      "acc_norm": 0.2851063829787234,
      "acc_norm_stderr": 0.029513196625539355
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2543859649122807,
      "acc_stderr": 0.040969851398436716,
      "acc_norm": 0.2543859649122807,
      "acc_norm_stderr": 0.040969851398436716
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.30344827586206896,
      "acc_stderr": 0.038312260488503336,
      "acc_norm": 0.30344827586206896,
      "acc_norm_stderr": 0.038312260488503336
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.24338624338624337,
      "acc_stderr": 0.02210112878741543,
      "acc_norm": 0.24338624338624337,
      "acc_norm_stderr": 0.02210112878741543
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.037184890068181146,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.037184890068181146
    },
    "hendrycksTest-global_facts": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.24838709677419354,
      "acc_stderr": 0.024580028921481006,
      "acc_norm": 0.24838709677419354,
      "acc_norm_stderr": 0.024580028921481006
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2512315270935961,
      "acc_stderr": 0.030516530732694433,
      "acc_norm": 0.2512315270935961,
      "acc_norm_stderr": 0.030516530732694433
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.03401506715249039,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.03401506715249039
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.19696969696969696,
      "acc_stderr": 0.028335609732463348,
      "acc_norm": 0.19696969696969696,
      "acc_norm_stderr": 0.028335609732463348
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.25906735751295334,
      "acc_stderr": 0.031618779179354115,
      "acc_norm": 0.25906735751295334,
      "acc_norm_stderr": 0.031618779179354115
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.24102564102564103,
      "acc_stderr": 0.0216855466653332,
      "acc_norm": 0.24102564102564103,
      "acc_norm_stderr": 0.0216855466653332
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.02671924078371218,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.02671924078371218
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.2605042016806723,
      "acc_stderr": 0.028510251512341944,
      "acc_norm": 0.2605042016806723,
      "acc_norm_stderr": 0.028510251512341944
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2913907284768212,
      "acc_stderr": 0.037101857261199946,
      "acc_norm": 0.2913907284768212,
      "acc_norm_stderr": 0.037101857261199946
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.22752293577981653,
      "acc_stderr": 0.017974463578776502,
      "acc_norm": 0.22752293577981653,
      "acc_norm_stderr": 0.017974463578776502
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.1527777777777778,
      "acc_stderr": 0.02453632602613422,
      "acc_norm": 0.1527777777777778,
      "acc_norm_stderr": 0.02453632602613422
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3088235294117647,
      "acc_stderr": 0.03242661719827218,
      "acc_norm": 0.3088235294117647,
      "acc_norm_stderr": 0.03242661719827218
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.270042194092827,
      "acc_stderr": 0.028900721906293426,
      "acc_norm": 0.270042194092827,
      "acc_norm_stderr": 0.028900721906293426
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4484304932735426,
      "acc_stderr": 0.033378837362550984,
      "acc_norm": 0.4484304932735426,
      "acc_norm_stderr": 0.033378837362550984
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.2900763358778626,
      "acc_stderr": 0.03980066246467765,
      "acc_norm": 0.2900763358778626,
      "acc_norm_stderr": 0.03980066246467765
    },
    "hendrycksTest-international_law": {
      "acc": 0.2396694214876033,
      "acc_stderr": 0.038968789850704164,
      "acc_norm": 0.2396694214876033,
      "acc_norm_stderr": 0.038968789850704164
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.04453197507374984,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.04453197507374984
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.24539877300613497,
      "acc_stderr": 0.03380939813943354,
      "acc_norm": 0.24539877300613497,
      "acc_norm_stderr": 0.03380939813943354
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04547960999764376,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04547960999764376
    },
    "hendrycksTest-management": {
      "acc": 0.2621359223300971,
      "acc_stderr": 0.04354631077260597,
      "acc_norm": 0.2621359223300971,
      "acc_norm_stderr": 0.04354631077260597
    },
    "hendrycksTest-marketing": {
      "acc": 0.3162393162393162,
      "acc_stderr": 0.030463656747340247,
      "acc_norm": 0.3162393162393162,
      "acc_norm_stderr": 0.030463656747340247
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.30395913154533843,
      "acc_stderr": 0.016448321686769043,
      "acc_norm": 0.30395913154533843,
      "acc_norm_stderr": 0.016448321686769043
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.30057803468208094,
      "acc_stderr": 0.024685316867257806,
      "acc_norm": 0.30057803468208094,
      "acc_norm_stderr": 0.024685316867257806
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24134078212290502,
      "acc_stderr": 0.014310999547961447,
      "acc_norm": 0.24134078212290502,
      "acc_norm_stderr": 0.014310999547961447
    },
    "hendrycksTest-nutrition": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.025646863097137918,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.025646863097137918
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3022508038585209,
      "acc_stderr": 0.026082700695399672,
      "acc_norm": 0.3022508038585209,
      "acc_norm_stderr": 0.026082700695399672
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.025842248700902168,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.025842248700902168
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.30851063829787234,
      "acc_stderr": 0.027553366165101362,
      "acc_norm": 0.30851063829787234,
      "acc_norm_stderr": 0.027553366165101362
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2457627118644068,
      "acc_stderr": 0.010996156635142692,
      "acc_norm": 0.2457627118644068,
      "acc_norm_stderr": 0.010996156635142692
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.2757352941176471,
      "acc_stderr": 0.027146271936625166,
      "acc_norm": 0.2757352941176471,
      "acc_norm_stderr": 0.027146271936625166
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.2679738562091503,
      "acc_stderr": 0.017917974069594722,
      "acc_norm": 0.2679738562091503,
      "acc_norm_stderr": 0.017917974069594722
    },
    "hendrycksTest-public_relations": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.046534298079135075,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.046534298079135075
    },
    "hendrycksTest-security_studies": {
      "acc": 0.27346938775510204,
      "acc_stderr": 0.02853556033712844,
      "acc_norm": 0.27346938775510204,
      "acc_norm_stderr": 0.02853556033712844
    },
    "hendrycksTest-sociology": {
      "acc": 0.3582089552238806,
      "acc_stderr": 0.03390393042268813,
      "acc_norm": 0.3582089552238806,
      "acc_norm_stderr": 0.03390393042268813
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-virology": {
      "acc": 0.3795180722891566,
      "acc_stderr": 0.03777798822748017,
      "acc_norm": 0.3795180722891566,
      "acc_norm_stderr": 0.03777798822748017
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.036996580176568775,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.036996580176568775
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/falcon-7b',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:08:53.227857",
    "model_name": "falcon_7b"
  }
}
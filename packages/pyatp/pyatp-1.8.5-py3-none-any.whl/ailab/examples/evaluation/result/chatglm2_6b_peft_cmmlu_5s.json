{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3254437869822485,
      "acc_stderr": 0.03614867847292204,
      "acc_norm": 0.3254437869822485,
      "acc_norm_stderr": 0.03614867847292204
    },
    "Cmmlu-anatomy": {
      "acc": 0.31756756756756754,
      "acc_stderr": 0.038396287341496804,
      "acc_norm": 0.31756756756756754,
      "acc_norm_stderr": 0.038396287341496804
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2682926829268293,
      "acc_stderr": 0.034703982128145336,
      "acc_norm": 0.2682926829268293,
      "acc_norm_stderr": 0.034703982128145336
    },
    "Cmmlu-arts": {
      "acc": 0.6125,
      "acc_stderr": 0.038635838122414064,
      "acc_norm": 0.6125,
      "acc_norm_stderr": 0.038635838122414064
    },
    "Cmmlu-astronomy": {
      "acc": 0.3151515151515151,
      "acc_stderr": 0.0362773057502241,
      "acc_norm": 0.3151515151515151,
      "acc_norm_stderr": 0.0362773057502241
    },
    "Cmmlu-business_ethics": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.03462071128843533,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.03462071128843533
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.41875,
      "acc_stderr": 0.039125538756915115,
      "acc_norm": 0.41875,
      "acc_norm_stderr": 0.039125538756915115
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5419847328244275,
      "acc_stderr": 0.04369802690578756,
      "acc_norm": 0.5419847328244275,
      "acc_norm_stderr": 0.04369802690578756
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.4632352941176471,
      "acc_stderr": 0.042916659667864274,
      "acc_norm": 0.4632352941176471,
      "acc_norm_stderr": 0.042916659667864274
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5607476635514018,
      "acc_stderr": 0.048204529006379074,
      "acc_norm": 0.5607476635514018,
      "acc_norm_stderr": 0.048204529006379074
    },
    "Cmmlu-chinese_history": {
      "acc": 0.631578947368421,
      "acc_stderr": 0.026881785181855217,
      "acc_norm": 0.631578947368421,
      "acc_norm_stderr": 0.026881785181855217
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.3431372549019608,
      "acc_stderr": 0.03332139944668086,
      "acc_norm": 0.3431372549019608,
      "acc_norm_stderr": 0.03332139944668086
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.5363128491620112,
      "acc_stderr": 0.0373776188053803,
      "acc_norm": 0.5363128491620112,
      "acc_norm_stderr": 0.0373776188053803
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.35864978902953587,
      "acc_stderr": 0.03121956944530184,
      "acc_norm": 0.35864978902953587,
      "acc_norm_stderr": 0.03121956944530184
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.25471698113207547,
      "acc_stderr": 0.04252016223763312,
      "acc_norm": 0.25471698113207547,
      "acc_norm_stderr": 0.04252016223763312
    },
    "Cmmlu-college_education": {
      "acc": 0.5514018691588785,
      "acc_stderr": 0.04830698295619322,
      "acc_norm": 0.5514018691588785,
      "acc_norm_stderr": 0.04830698295619322
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.41509433962264153,
      "acc_stderr": 0.04808633394970665,
      "acc_norm": 0.41509433962264153,
      "acc_norm_stderr": 0.04808633394970665
    },
    "Cmmlu-college_law": {
      "acc": 0.42592592592592593,
      "acc_stderr": 0.0478034362693679,
      "acc_norm": 0.42592592592592593,
      "acc_norm_stderr": 0.0478034362693679
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2571428571428571,
      "acc_stderr": 0.042857142857142844,
      "acc_norm": 0.2571428571428571,
      "acc_norm_stderr": 0.042857142857142844
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.04730439022852894,
      "acc_norm": 0.37735849056603776,
      "acc_norm_stderr": 0.04730439022852894
    },
    "Cmmlu-college_medicine": {
      "acc": 0.42124542124542125,
      "acc_stderr": 0.029938522567897165,
      "acc_norm": 0.42124542124542125,
      "acc_norm_stderr": 0.029938522567897165
    },
    "Cmmlu-computer_science": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.03460228327239172,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.03460228327239172
    },
    "Cmmlu-computer_security": {
      "acc": 0.5146198830409356,
      "acc_stderr": 0.03833185275213026,
      "acc_norm": 0.5146198830409356,
      "acc_norm_stderr": 0.03833185275213026
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.040955869934356876,
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.040955869934356876
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.3669064748201439,
      "acc_stderr": 0.04102721909807841,
      "acc_norm": 0.3669064748201439,
      "acc_norm_stderr": 0.04102721909807841
    },
    "Cmmlu-economics": {
      "acc": 0.4339622641509434,
      "acc_stderr": 0.0394293967186222,
      "acc_norm": 0.4339622641509434,
      "acc_norm_stderr": 0.0394293967186222
    },
    "Cmmlu-education": {
      "acc": 0.5644171779141104,
      "acc_stderr": 0.03895632464138936,
      "acc_norm": 0.5644171779141104,
      "acc_norm_stderr": 0.03895632464138936
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3546511627906977,
      "acc_stderr": 0.03658473425938542,
      "acc_norm": 0.3546511627906977,
      "acc_norm_stderr": 0.03658473425938542
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.4126984126984127,
      "acc_stderr": 0.031074927625190884,
      "acc_norm": 0.4126984126984127,
      "acc_norm_stderr": 0.031074927625190884
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.5151515151515151,
      "acc_stderr": 0.03560716516531061,
      "acc_norm": 0.5151515151515151,
      "acc_norm_stderr": 0.03560716516531061
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.6008403361344538,
      "acc_stderr": 0.03181110032413925,
      "acc_norm": 0.6008403361344538,
      "acc_norm_stderr": 0.03181110032413925
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.3391304347826087,
      "acc_stderr": 0.03128408938822598,
      "acc_norm": 0.3391304347826087,
      "acc_norm_stderr": 0.03128408938822598
    },
    "Cmmlu-ethnology": {
      "acc": 0.4222222222222222,
      "acc_stderr": 0.04266763404099582,
      "acc_norm": 0.4222222222222222,
      "acc_norm_stderr": 0.04266763404099582
    },
    "Cmmlu-food_science": {
      "acc": 0.4405594405594406,
      "acc_stderr": 0.041661514977676506,
      "acc_norm": 0.4405594405594406,
      "acc_norm_stderr": 0.041661514977676506
    },
    "Cmmlu-genetics": {
      "acc": 0.39204545454545453,
      "acc_stderr": 0.03690496026403127,
      "acc_norm": 0.39204545454545453,
      "acc_norm_stderr": 0.03690496026403127
    },
    "Cmmlu-global_facts": {
      "acc": 0.4563758389261745,
      "acc_stderr": 0.04094301680967171,
      "acc_norm": 0.4563758389261745,
      "acc_norm_stderr": 0.04094301680967171
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.591715976331361,
      "acc_stderr": 0.0379212984888554,
      "acc_norm": 0.591715976331361,
      "acc_norm_stderr": 0.0379212984888554
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.49242424242424243,
      "acc_stderr": 0.04368018817392578,
      "acc_norm": 0.49242424242424243,
      "acc_norm_stderr": 0.04368018817392578
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.4152542372881356,
      "acc_stderr": 0.045556216394221444,
      "acc_norm": 0.4152542372881356,
      "acc_norm_stderr": 0.045556216394221444
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2804878048780488,
      "acc_stderr": 0.035187002288015794,
      "acc_norm": 0.2804878048780488,
      "acc_norm_stderr": 0.035187002288015794
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.4818181818181818,
      "acc_stderr": 0.04785964010794917,
      "acc_norm": 0.4818181818181818,
      "acc_norm_stderr": 0.04785964010794917
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5244755244755245,
      "acc_stderr": 0.04190876649540683,
      "acc_norm": 0.5244755244755245,
      "acc_norm_stderr": 0.04190876649540683
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.42063492063492064,
      "acc_stderr": 0.04415438226743744,
      "acc_norm": 0.42063492063492064,
      "acc_norm_stderr": 0.04415438226743744
    },
    "Cmmlu-international_law": {
      "acc": 0.33513513513513515,
      "acc_stderr": 0.03479907984892718,
      "acc_norm": 0.33513513513513515,
      "acc_norm_stderr": 0.03479907984892718
    },
    "Cmmlu-journalism": {
      "acc": 0.4476744186046512,
      "acc_stderr": 0.03802600168672209,
      "acc_norm": 0.4476744186046512,
      "acc_norm_stderr": 0.03802600168672209
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.46958637469586373,
      "acc_stderr": 0.02464751565157793,
      "acc_norm": 0.46958637469586373,
      "acc_norm_stderr": 0.02464751565157793
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.7897196261682243,
      "acc_stderr": 0.027921968584579328,
      "acc_norm": 0.7897196261682243,
      "acc_norm_stderr": 0.027921968584579328
    },
    "Cmmlu-logical": {
      "acc": 0.3821138211382114,
      "acc_stderr": 0.043991695270045095,
      "acc_norm": 0.3821138211382114,
      "acc_norm_stderr": 0.043991695270045095
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4098360655737705,
      "acc_stderr": 0.04470938897168401,
      "acc_norm": 0.4098360655737705,
      "acc_norm_stderr": 0.04470938897168401
    },
    "Cmmlu-management": {
      "acc": 0.5285714285714286,
      "acc_stderr": 0.03452921053595503,
      "acc_norm": 0.5285714285714286,
      "acc_norm_stderr": 0.03452921053595503
    },
    "Cmmlu-marketing": {
      "acc": 0.5333333333333333,
      "acc_stderr": 0.03728861381239125,
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.03728861381239125
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.5396825396825397,
      "acc_stderr": 0.036351219362932556,
      "acc_norm": 0.5396825396825397,
      "acc_norm_stderr": 0.036351219362932556
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.31896551724137934,
      "acc_stderr": 0.043461778915984337,
      "acc_norm": 0.31896551724137934,
      "acc_norm_stderr": 0.043461778915984337
    },
    "Cmmlu-nutrition": {
      "acc": 0.46206896551724136,
      "acc_stderr": 0.041546596717075474,
      "acc_norm": 0.46206896551724136,
      "acc_norm_stderr": 0.041546596717075474
    },
    "Cmmlu-philosophy": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.04852615860619701,
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.04852615860619701
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5028571428571429,
      "acc_stderr": 0.03790428331834743,
      "acc_norm": 0.5028571428571429,
      "acc_norm_stderr": 0.03790428331834743
    },
    "Cmmlu-professional_law": {
      "acc": 0.3696682464454976,
      "acc_stderr": 0.03331048984038976,
      "acc_norm": 0.3696682464454976,
      "acc_norm_stderr": 0.03331048984038976
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.32180851063829785,
      "acc_stderr": 0.02412455419244379,
      "acc_norm": 0.32180851063829785,
      "acc_norm_stderr": 0.02412455419244379
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5043103448275862,
      "acc_stderr": 0.032896362312049884,
      "acc_norm": 0.5043103448275862,
      "acc_norm_stderr": 0.032896362312049884
    },
    "Cmmlu-public_relations": {
      "acc": 0.5057471264367817,
      "acc_stderr": 0.03801178479702085,
      "acc_norm": 0.5057471264367817,
      "acc_norm_stderr": 0.03801178479702085
    },
    "Cmmlu-security_study": {
      "acc": 0.5851851851851851,
      "acc_stderr": 0.04256193767901407,
      "acc_norm": 0.5851851851851851,
      "acc_norm_stderr": 0.04256193767901407
    },
    "Cmmlu-sociology": {
      "acc": 0.47345132743362833,
      "acc_stderr": 0.0332863113663503,
      "acc_norm": 0.47345132743362833,
      "acc_norm_stderr": 0.0332863113663503
    },
    "Cmmlu-sports_science": {
      "acc": 0.44242424242424244,
      "acc_stderr": 0.03878372113711274,
      "acc_norm": 0.44242424242424244,
      "acc_norm_stderr": 0.03878372113711274
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.41621621621621624,
      "acc_stderr": 0.03633930360945234,
      "acc_norm": 0.41621621621621624,
      "acc_norm_stderr": 0.03633930360945234
    },
    "Cmmlu-virology": {
      "acc": 0.44970414201183434,
      "acc_stderr": 0.038380172729489376,
      "acc_norm": 0.44970414201183434,
      "acc_norm_stderr": 0.038380172729489376
    },
    "Cmmlu-world_history": {
      "acc": 0.5527950310559007,
      "acc_stderr": 0.0393074964777559,
      "acc_norm": 0.5527950310559007,
      "acc_norm_stderr": 0.0393074964777559
    },
    "Cmmlu-world_religions": {
      "acc": 0.475,
      "acc_stderr": 0.03960298254443845,
      "acc_norm": 0.475,
      "acc_norm_stderr": 0.03960298254443845
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained=/home/sdk_models/chatglm2_6b,add_special_tokens=True,dtype='float16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_chatglm2_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:41:26.247606",
    "model_name": "chatglm2_6b"
  }
}
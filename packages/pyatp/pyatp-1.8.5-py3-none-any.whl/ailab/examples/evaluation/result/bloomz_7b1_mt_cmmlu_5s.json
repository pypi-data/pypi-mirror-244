{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3905325443786982,
      "acc_stderr": 0.03763996705629265,
      "acc_norm": 0.3905325443786982,
      "acc_norm_stderr": 0.03763996705629265
    },
    "Cmmlu-anatomy": {
      "acc": 0.25,
      "acc_stderr": 0.03571428571428571,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03571428571428571
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2865853658536585,
      "acc_stderr": 0.03541638332993505,
      "acc_norm": 0.2865853658536585,
      "acc_norm_stderr": 0.03541638332993505
    },
    "Cmmlu-arts": {
      "acc": 0.40625,
      "acc_stderr": 0.03894932504400619,
      "acc_norm": 0.40625,
      "acc_norm_stderr": 0.03894932504400619
    },
    "Cmmlu-astronomy": {
      "acc": 0.2606060606060606,
      "acc_stderr": 0.03427743175816524,
      "acc_norm": 0.2606060606060606,
      "acc_norm_stderr": 0.03427743175816524
    },
    "Cmmlu-business_ethics": {
      "acc": 0.44976076555023925,
      "acc_stderr": 0.03449331173477288,
      "acc_norm": 0.44976076555023925,
      "acc_norm_stderr": 0.03449331173477288
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.3625,
      "acc_stderr": 0.03812374340644891,
      "acc_norm": 0.3625,
      "acc_norm_stderr": 0.03812374340644891
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.6106870229007634,
      "acc_stderr": 0.042764865428145914,
      "acc_norm": 0.6106870229007634,
      "acc_norm_stderr": 0.042764865428145914
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.04182495883863045,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.04182495883863045
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.4766355140186916,
      "acc_stderr": 0.048511241723296745,
      "acc_norm": 0.4766355140186916,
      "acc_norm_stderr": 0.048511241723296745
    },
    "Cmmlu-chinese_history": {
      "acc": 0.47368421052631576,
      "acc_stderr": 0.027825291191456943,
      "acc_norm": 0.47368421052631576,
      "acc_norm_stderr": 0.027825291191456943
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.0309645179269234,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.0309645179269234
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.45251396648044695,
      "acc_stderr": 0.03730718795553105,
      "acc_norm": 0.45251396648044695,
      "acc_norm_stderr": 0.03730718795553105
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.31645569620253167,
      "acc_stderr": 0.03027497488021897,
      "acc_norm": 0.31645569620253167,
      "acc_norm_stderr": 0.03027497488021897
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.27358490566037735,
      "acc_stderr": 0.04350546818999061,
      "acc_norm": 0.27358490566037735,
      "acc_norm_stderr": 0.04350546818999061
    },
    "Cmmlu-college_education": {
      "acc": 0.5420560747663551,
      "acc_stderr": 0.04839219555189162,
      "acc_norm": 0.5420560747663551,
      "acc_norm_stderr": 0.04839219555189162
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.4716981132075472,
      "acc_stderr": 0.04871677165040775,
      "acc_norm": 0.4716981132075472,
      "acc_norm_stderr": 0.04871677165040775
    },
    "Cmmlu-college_law": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.044531975073749834,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.044531975073749834
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2571428571428571,
      "acc_stderr": 0.042857142857142844,
      "acc_norm": 0.2571428571428571,
      "acc_norm_stderr": 0.042857142857142844
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.29245283018867924,
      "acc_stderr": 0.04439263906199628,
      "acc_norm": 0.29245283018867924,
      "acc_norm_stderr": 0.04439263906199628
    },
    "Cmmlu-college_medicine": {
      "acc": 0.358974358974359,
      "acc_stderr": 0.02908606451836629,
      "acc_norm": 0.358974358974359,
      "acc_norm_stderr": 0.02908606451836629
    },
    "Cmmlu-computer_science": {
      "acc": 0.39215686274509803,
      "acc_stderr": 0.03426712349247272,
      "acc_norm": 0.39215686274509803,
      "acc_norm_stderr": 0.03426712349247272
    },
    "Cmmlu-computer_security": {
      "acc": 0.4678362573099415,
      "acc_stderr": 0.038268824176603704,
      "acc_norm": 0.4678362573099415,
      "acc_norm_stderr": 0.038268824176603704
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.29931972789115646,
      "acc_stderr": 0.0379010453091039,
      "acc_norm": 0.29931972789115646,
      "acc_norm_stderr": 0.0379010453091039
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.34532374100719426,
      "acc_stderr": 0.04047501062151218,
      "acc_norm": 0.34532374100719426,
      "acc_norm_stderr": 0.04047501062151218
    },
    "Cmmlu-economics": {
      "acc": 0.41509433962264153,
      "acc_stderr": 0.03920015409714327,
      "acc_norm": 0.41509433962264153,
      "acc_norm_stderr": 0.03920015409714327
    },
    "Cmmlu-education": {
      "acc": 0.49079754601226994,
      "acc_stderr": 0.03927705600787443,
      "acc_norm": 0.49079754601226994,
      "acc_norm_stderr": 0.03927705600787443
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.4011627906976744,
      "acc_stderr": 0.03748147347872575,
      "acc_norm": 0.4011627906976744,
      "acc_norm_stderr": 0.03748147347872575
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.3253968253968254,
      "acc_stderr": 0.02957290480961354,
      "acc_norm": 0.3253968253968254,
      "acc_norm_stderr": 0.02957290480961354
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.4090909090909091,
      "acc_stderr": 0.03502975799413007,
      "acc_norm": 0.4090909090909091,
      "acc_norm_stderr": 0.03502975799413007
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.49159663865546216,
      "acc_stderr": 0.03247390276569669,
      "acc_norm": 0.49159663865546216,
      "acc_norm_stderr": 0.03247390276569669
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.22608695652173913,
      "acc_stderr": 0.027641785707241327,
      "acc_norm": 0.22608695652173913,
      "acc_norm_stderr": 0.027641785707241327
    },
    "Cmmlu-ethnology": {
      "acc": 0.362962962962963,
      "acc_stderr": 0.041539484047424,
      "acc_norm": 0.362962962962963,
      "acc_norm_stderr": 0.041539484047424
    },
    "Cmmlu-food_science": {
      "acc": 0.4125874125874126,
      "acc_stderr": 0.04131287692392343,
      "acc_norm": 0.4125874125874126,
      "acc_norm_stderr": 0.04131287692392343
    },
    "Cmmlu-genetics": {
      "acc": 0.3352272727272727,
      "acc_stderr": 0.03568512682153707,
      "acc_norm": 0.3352272727272727,
      "acc_norm_stderr": 0.03568512682153707
    },
    "Cmmlu-global_facts": {
      "acc": 0.37583892617449666,
      "acc_stderr": 0.03981240026051386,
      "acc_norm": 0.37583892617449666,
      "acc_norm_stderr": 0.03981240026051386
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.27218934911242604,
      "acc_stderr": 0.03433919627548535,
      "acc_norm": 0.27218934911242604,
      "acc_norm_stderr": 0.03433919627548535
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.25757575757575757,
      "acc_stderr": 0.03820699814849796,
      "acc_norm": 0.25757575757575757,
      "acc_norm_stderr": 0.03820699814849796
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.4067796610169492,
      "acc_stderr": 0.04541451708861589,
      "acc_norm": 0.4067796610169492,
      "acc_norm_stderr": 0.04541451708861589
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.03363591048272823,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.03363591048272823
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3,
      "acc_stderr": 0.04389311454644287,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.04389311454644287
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.40559440559440557,
      "acc_stderr": 0.04120436731133788,
      "acc_norm": 0.40559440559440557,
      "acc_norm_stderr": 0.04120436731133788
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.4365079365079365,
      "acc_stderr": 0.04435932892851466,
      "acc_norm": 0.4365079365079365,
      "acc_norm_stderr": 0.04435932892851466
    },
    "Cmmlu-international_law": {
      "acc": 0.3621621621621622,
      "acc_stderr": 0.035432171151384854,
      "acc_norm": 0.3621621621621622,
      "acc_norm_stderr": 0.035432171151384854
    },
    "Cmmlu-journalism": {
      "acc": 0.4011627906976744,
      "acc_stderr": 0.03748147347872574,
      "acc_norm": 0.4011627906976744,
      "acc_norm_stderr": 0.03748147347872574
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.3795620437956204,
      "acc_stderr": 0.023966170197079785,
      "acc_norm": 0.3795620437956204,
      "acc_norm_stderr": 0.023966170197079785
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.6355140186915887,
      "acc_stderr": 0.03297715461451676,
      "acc_norm": 0.6355140186915887,
      "acc_norm_stderr": 0.03297715461451676
    },
    "Cmmlu-logical": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04267895997763195,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04267895997763195
    },
    "Cmmlu-machine_learning": {
      "acc": 0.319672131147541,
      "acc_stderr": 0.04239540943837383,
      "acc_norm": 0.319672131147541,
      "acc_norm_stderr": 0.04239540943837383
    },
    "Cmmlu-management": {
      "acc": 0.47619047619047616,
      "acc_stderr": 0.03454648810047677,
      "acc_norm": 0.47619047619047616,
      "acc_norm_stderr": 0.03454648810047677
    },
    "Cmmlu-marketing": {
      "acc": 0.43333333333333335,
      "acc_stderr": 0.037038071576695496,
      "acc_norm": 0.43333333333333335,
      "acc_norm_stderr": 0.037038071576695496
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.48677248677248675,
      "acc_stderr": 0.03645348485324587,
      "acc_norm": 0.48677248677248675,
      "acc_norm_stderr": 0.03645348485324587
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3017241379310345,
      "acc_stderr": 0.04280254792505461,
      "acc_norm": 0.3017241379310345,
      "acc_norm_stderr": 0.04280254792505461
    },
    "Cmmlu-nutrition": {
      "acc": 0.3724137931034483,
      "acc_stderr": 0.04028731532947559,
      "acc_norm": 0.3724137931034483,
      "acc_norm_stderr": 0.04028731532947559
    },
    "Cmmlu-philosophy": {
      "acc": 0.3904761904761905,
      "acc_stderr": 0.047838322981141455,
      "acc_norm": 0.3904761904761905,
      "acc_norm_stderr": 0.047838322981141455
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.4057142857142857,
      "acc_stderr": 0.03722486840883875,
      "acc_norm": 0.4057142857142857,
      "acc_norm_stderr": 0.03722486840883875
    },
    "Cmmlu-professional_law": {
      "acc": 0.3696682464454976,
      "acc_stderr": 0.033310489840389756,
      "acc_norm": 0.3696682464454976,
      "acc_norm_stderr": 0.033310489840389756
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2898936170212766,
      "acc_stderr": 0.023429628412566187,
      "acc_norm": 0.2898936170212766,
      "acc_norm_stderr": 0.023429628412566187
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.4224137931034483,
      "acc_stderr": 0.032499109578172314,
      "acc_norm": 0.4224137931034483,
      "acc_norm_stderr": 0.032499109578172314
    },
    "Cmmlu-public_relations": {
      "acc": 0.4482758620689655,
      "acc_stderr": 0.037810343077949025,
      "acc_norm": 0.4482758620689655,
      "acc_norm_stderr": 0.037810343077949025
    },
    "Cmmlu-security_study": {
      "acc": 0.43703703703703706,
      "acc_stderr": 0.042849586397533994,
      "acc_norm": 0.43703703703703706,
      "acc_norm_stderr": 0.042849586397533994
    },
    "Cmmlu-sociology": {
      "acc": 0.42035398230088494,
      "acc_stderr": 0.032907716883880764,
      "acc_norm": 0.42035398230088494,
      "acc_norm_stderr": 0.032907716883880764
    },
    "Cmmlu-sports_science": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.03793713171165634,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.03793713171165634
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3945945945945946,
      "acc_stderr": 0.03603211886269591,
      "acc_norm": 0.3945945945945946,
      "acc_norm_stderr": 0.03603211886269591
    },
    "Cmmlu-virology": {
      "acc": 0.4260355029585799,
      "acc_stderr": 0.03815142551613446,
      "acc_norm": 0.4260355029585799,
      "acc_norm_stderr": 0.03815142551613446
    },
    "Cmmlu-world_history": {
      "acc": 0.40372670807453415,
      "acc_stderr": 0.03878880744346832,
      "acc_norm": 0.40372670807453415,
      "acc_norm_stderr": 0.03878880744346832
    },
    "Cmmlu-world_religions": {
      "acc": 0.475,
      "acc_stderr": 0.03960298254443846,
      "acc_norm": 0.475,
      "acc_norm_stderr": 0.03960298254443846
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/bloomz_7b,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "5:54:54.137698",
    "model_name": "bloomz_7b1_mt"
  }
}
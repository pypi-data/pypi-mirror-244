{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.28402366863905326,
      "acc_stderr": 0.03479140427262331,
      "acc_norm": 0.28402366863905326,
      "acc_norm_stderr": 0.03479140427262331
    },
    "Cmmlu-anatomy": {
      "acc": 0.2635135135135135,
      "acc_stderr": 0.036335000433819875,
      "acc_norm": 0.2635135135135135,
      "acc_norm_stderr": 0.036335000433819875
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.03563788836258827,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.03563788836258827
    },
    "Cmmlu-arts": {
      "acc": 0.2375,
      "acc_stderr": 0.03374839851779224,
      "acc_norm": 0.2375,
      "acc_norm_stderr": 0.03374839851779224
    },
    "Cmmlu-astronomy": {
      "acc": 0.2787878787878788,
      "acc_stderr": 0.03501438706296781,
      "acc_norm": 0.2787878787878788,
      "acc_norm_stderr": 0.03501438706296781
    },
    "Cmmlu-business_ethics": {
      "acc": 0.2535885167464115,
      "acc_stderr": 0.030166316298848004,
      "acc_norm": 0.2535885167464115,
      "acc_norm_stderr": 0.030166316298848004
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.21875,
      "acc_stderr": 0.032784644885244255,
      "acc_norm": 0.21875,
      "acc_norm_stderr": 0.032784644885244255
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.35877862595419846,
      "acc_stderr": 0.04206739313864908,
      "acc_norm": 0.35877862595419846,
      "acc_norm_stderr": 0.04206739313864908
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.2426470588235294,
      "acc_stderr": 0.03689519326996807,
      "acc_norm": 0.2426470588235294,
      "acc_norm_stderr": 0.03689519326996807
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.2523364485981308,
      "acc_stderr": 0.04218811928205305,
      "acc_norm": 0.2523364485981308,
      "acc_norm_stderr": 0.04218811928205305
    },
    "Cmmlu-chinese_history": {
      "acc": 0.25386996904024767,
      "acc_stderr": 0.024254090252458067,
      "acc_norm": 0.25386996904024767,
      "acc_norm_stderr": 0.024254090252458067
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.031321798030832904,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.031321798030832904
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.27932960893854747,
      "acc_stderr": 0.03362922238714362,
      "acc_norm": 0.27932960893854747,
      "acc_norm_stderr": 0.03362922238714362
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.29535864978902954,
      "acc_stderr": 0.029696338713422886,
      "acc_norm": 0.29535864978902954,
      "acc_norm_stderr": 0.029696338713422886
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.16037735849056603,
      "acc_stderr": 0.03581120619691076,
      "acc_norm": 0.16037735849056603,
      "acc_norm_stderr": 0.03581120619691076
    },
    "Cmmlu-college_education": {
      "acc": 0.3177570093457944,
      "acc_stderr": 0.045223500773820306,
      "acc_norm": 0.3177570093457944,
      "acc_norm_stderr": 0.045223500773820306
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.2169811320754717,
      "acc_stderr": 0.040225592469367126,
      "acc_norm": 0.2169811320754717,
      "acc_norm_stderr": 0.040225592469367126
    },
    "Cmmlu-college_law": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.040191074725573483,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.040191074725573483
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.04336290903919942,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.04336290903919942
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.22641509433962265,
      "acc_stderr": 0.040842473153370994,
      "acc_norm": 0.22641509433962265,
      "acc_norm_stderr": 0.040842473153370994
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2893772893772894,
      "acc_stderr": 0.027495860234525278,
      "acc_norm": 0.2893772893772894,
      "acc_norm_stderr": 0.027495860234525278
    },
    "Cmmlu-computer_science": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.031493281045079556,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.031493281045079556
    },
    "Cmmlu-computer_security": {
      "acc": 0.26900584795321636,
      "acc_stderr": 0.03401052620104089,
      "acc_norm": 0.26900584795321636,
      "acc_norm_stderr": 0.03401052620104089
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.22448979591836735,
      "acc_stderr": 0.034531515032766795,
      "acc_norm": 0.22448979591836735,
      "acc_norm_stderr": 0.034531515032766795
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2446043165467626,
      "acc_stderr": 0.03659146222520568,
      "acc_norm": 0.2446043165467626,
      "acc_norm_stderr": 0.03659146222520568
    },
    "Cmmlu-economics": {
      "acc": 0.32075471698113206,
      "acc_stderr": 0.03713396279871006,
      "acc_norm": 0.32075471698113206,
      "acc_norm_stderr": 0.03713396279871006
    },
    "Cmmlu-education": {
      "acc": 0.25766871165644173,
      "acc_stderr": 0.03436150827846917,
      "acc_norm": 0.25766871165644173,
      "acc_norm_stderr": 0.03436150827846917
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3081395348837209,
      "acc_stderr": 0.03530895898152282,
      "acc_norm": 0.3081395348837209,
      "acc_norm_stderr": 0.03530895898152282
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.23412698412698413,
      "acc_stderr": 0.02672804899930241,
      "acc_norm": 0.23412698412698413,
      "acc_norm_stderr": 0.02672804899930241
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.20707070707070707,
      "acc_stderr": 0.02886977846026705,
      "acc_norm": 0.20707070707070707,
      "acc_norm_stderr": 0.02886977846026705
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.2815126050420168,
      "acc_stderr": 0.02921354941437216,
      "acc_norm": 0.2815126050420168,
      "acc_norm_stderr": 0.02921354941437216
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.1956521739130435,
      "acc_stderr": 0.026214799709819585,
      "acc_norm": 0.1956521739130435,
      "acc_norm_stderr": 0.026214799709819585
    },
    "Cmmlu-ethnology": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.03591444084196971,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.03591444084196971
    },
    "Cmmlu-food_science": {
      "acc": 0.26573426573426573,
      "acc_stderr": 0.03706860462623558,
      "acc_norm": 0.26573426573426573,
      "acc_norm_stderr": 0.03706860462623558
    },
    "Cmmlu-genetics": {
      "acc": 0.24431818181818182,
      "acc_stderr": 0.03248092256353737,
      "acc_norm": 0.24431818181818182,
      "acc_norm_stderr": 0.03248092256353737
    },
    "Cmmlu-global_facts": {
      "acc": 0.2550335570469799,
      "acc_stderr": 0.03582912165111174,
      "acc_norm": 0.2550335570469799,
      "acc_norm_stderr": 0.03582912165111174
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.28994082840236685,
      "acc_stderr": 0.03500638924911012,
      "acc_norm": 0.28994082840236685,
      "acc_norm_stderr": 0.03500638924911012
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.03661433360410718,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.03661433360410718
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3135593220338983,
      "acc_stderr": 0.042891223336625726,
      "acc_norm": 0.3135593220338983,
      "acc_norm_stderr": 0.042891223336625726
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.03363591048272823,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.03363591048272823
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.04350271442923243,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.04350271442923243
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.17482517482517482,
      "acc_stderr": 0.03187357652966491,
      "acc_norm": 0.17482517482517482,
      "acc_norm_stderr": 0.03187357652966491
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.24603174603174602,
      "acc_stderr": 0.038522733649243183,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.038522733649243183
    },
    "Cmmlu-international_law": {
      "acc": 0.2918918918918919,
      "acc_stderr": 0.03351597731741764,
      "acc_norm": 0.2918918918918919,
      "acc_norm_stderr": 0.03351597731741764
    },
    "Cmmlu-journalism": {
      "acc": 0.28488372093023256,
      "acc_stderr": 0.0345162887625062,
      "acc_norm": 0.28488372093023256,
      "acc_norm_stderr": 0.0345162887625062
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.2798053527980535,
      "acc_stderr": 0.022169761725927828,
      "acc_norm": 0.2798053527980535,
      "acc_norm_stderr": 0.022169761725927828
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.3177570093457944,
      "acc_stderr": 0.03190269039219334,
      "acc_norm": 0.3177570093457944,
      "acc_norm_stderr": 0.03190269039219334
    },
    "Cmmlu-logical": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.03887917804888516,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.03887917804888516
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2786885245901639,
      "acc_stderr": 0.04075944659069249,
      "acc_norm": 0.2786885245901639,
      "acc_norm_stderr": 0.04075944659069249
    },
    "Cmmlu-management": {
      "acc": 0.22857142857142856,
      "acc_stderr": 0.02904595687156657,
      "acc_norm": 0.22857142857142856,
      "acc_norm_stderr": 0.02904595687156657
    },
    "Cmmlu-marketing": {
      "acc": 0.29444444444444445,
      "acc_stderr": 0.03406754001349689,
      "acc_norm": 0.29444444444444445,
      "acc_norm_stderr": 0.03406754001349689
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.291005291005291,
      "acc_stderr": 0.03312783200356568,
      "acc_norm": 0.291005291005291,
      "acc_norm_stderr": 0.03312783200356568
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.21551724137931033,
      "acc_stderr": 0.038342797072854595,
      "acc_norm": 0.21551724137931033,
      "acc_norm_stderr": 0.038342797072854595
    },
    "Cmmlu-nutrition": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.037245636197746346,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.037245636197746346
    },
    "Cmmlu-philosophy": {
      "acc": 0.29523809523809524,
      "acc_stderr": 0.044729159560441434,
      "acc_norm": 0.29523809523809524,
      "acc_norm_stderr": 0.044729159560441434
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.29714285714285715,
      "acc_stderr": 0.034645078898843724,
      "acc_norm": 0.29714285714285715,
      "acc_norm_stderr": 0.034645078898843724
    },
    "Cmmlu-professional_law": {
      "acc": 0.27014218009478674,
      "acc_stderr": 0.03064119407629314,
      "acc_norm": 0.27014218009478674,
      "acc_norm_stderr": 0.03064119407629314
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.28191489361702127,
      "acc_stderr": 0.023234393263661224,
      "acc_norm": 0.28191489361702127,
      "acc_norm_stderr": 0.023234393263661224
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.23275862068965517,
      "acc_stderr": 0.02780436020996173,
      "acc_norm": 0.23275862068965517,
      "acc_norm_stderr": 0.02780436020996173
    },
    "Cmmlu-public_relations": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.03398079939585583,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.03398079939585583
    },
    "Cmmlu-security_study": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.0391545063041425,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.0391545063041425
    },
    "Cmmlu-sociology": {
      "acc": 0.20353982300884957,
      "acc_stderr": 0.02684203697009399,
      "acc_norm": 0.20353982300884957,
      "acc_norm_stderr": 0.02684203697009399
    },
    "Cmmlu-sports_science": {
      "acc": 0.3090909090909091,
      "acc_stderr": 0.03608541011573967,
      "acc_norm": 0.3090909090909091,
      "acc_norm_stderr": 0.03608541011573967
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2702702702702703,
      "acc_stderr": 0.03273943999002356,
      "acc_norm": 0.2702702702702703,
      "acc_norm_stderr": 0.03273943999002356
    },
    "Cmmlu-virology": {
      "acc": 0.23668639053254437,
      "acc_stderr": 0.03279317792268949,
      "acc_norm": 0.23668639053254437,
      "acc_norm_stderr": 0.03279317792268949
    },
    "Cmmlu-world_history": {
      "acc": 0.2608695652173913,
      "acc_stderr": 0.03471460744058984,
      "acc_norm": 0.2608695652173913,
      "acc_norm_stderr": 0.03471460744058984
    },
    "Cmmlu-world_religions": {
      "acc": 0.28125,
      "acc_stderr": 0.03565632932250201,
      "acc_norm": 0.28125,
      "acc_norm_stderr": 0.03565632932250201
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama-7b-hf,load_in_8bit=True,dtype='float16',tokenizer=/home/sdk_token/chinese_llama_vicuna_tokenizer,use_accelerate=False,peft=/home/finetuned_models/my_chinese_llama_vicuna_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:37:02.193260",
    "model_name": "chinese_llama_vicuna"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.44970414201183434,
      "acc_stderr": 0.03838017272948937,
      "acc_norm": 0.44970414201183434,
      "acc_norm_stderr": 0.03838017272948937
    },
    "Cmmlu-anatomy": {
      "acc": 0.3581081081081081,
      "acc_stderr": 0.039543886841064475,
      "acc_norm": 0.3581081081081081,
      "acc_norm_stderr": 0.039543886841064475
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.29878048780487804,
      "acc_stderr": 0.03585166336909663,
      "acc_norm": 0.29878048780487804,
      "acc_norm_stderr": 0.03585166336909663
    },
    "Cmmlu-arts": {
      "acc": 0.7375,
      "acc_stderr": 0.034893706520187605,
      "acc_norm": 0.7375,
      "acc_norm_stderr": 0.034893706520187605
    },
    "Cmmlu-astronomy": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.037131580674819135,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.037131580674819135
    },
    "Cmmlu-business_ethics": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.03462071128843533,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.03462071128843533
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.50625,
      "acc_stderr": 0.03964948130713095,
      "acc_norm": 0.50625,
      "acc_norm_stderr": 0.03964948130713095
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.7022900763358778,
      "acc_stderr": 0.04010358942462203,
      "acc_norm": 0.7022900763358778,
      "acc_norm_stderr": 0.04010358942462203
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.5294117647058824,
      "acc_stderr": 0.04295863196118948,
      "acc_norm": 0.5294117647058824,
      "acc_norm_stderr": 0.04295863196118948
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.6728971962616822,
      "acc_stderr": 0.04556837693674772,
      "acc_norm": 0.6728971962616822,
      "acc_norm_stderr": 0.04556837693674772
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6377708978328174,
      "acc_stderr": 0.026785273354320075,
      "acc_norm": 0.6377708978328174,
      "acc_norm_stderr": 0.026785273354320075
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.4166666666666667,
      "acc_stderr": 0.0346022832723917,
      "acc_norm": 0.4166666666666667,
      "acc_norm_stderr": 0.0346022832723917
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.7206703910614525,
      "acc_stderr": 0.033629222387143644,
      "acc_norm": 0.7206703910614525,
      "acc_norm_stderr": 0.033629222387143644
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4345991561181435,
      "acc_stderr": 0.03226759995510145,
      "acc_norm": 0.4345991561181435,
      "acc_norm_stderr": 0.03226759995510145
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.18867924528301888,
      "acc_stderr": 0.0381824426969915,
      "acc_norm": 0.18867924528301888,
      "acc_norm_stderr": 0.0381824426969915
    },
    "Cmmlu-college_education": {
      "acc": 0.5981308411214953,
      "acc_stderr": 0.04761979313593578,
      "acc_norm": 0.5981308411214953,
      "acc_norm_stderr": 0.04761979313593578
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.44339622641509435,
      "acc_stderr": 0.048481318229754794,
      "acc_norm": 0.44339622641509435,
      "acc_norm_stderr": 0.048481318229754794
    },
    "Cmmlu-college_law": {
      "acc": 0.39814814814814814,
      "acc_stderr": 0.04732332615978813,
      "acc_norm": 0.39814814814814814,
      "acc_norm_stderr": 0.04732332615978813
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.24761904761904763,
      "acc_stderr": 0.04232473532055044,
      "acc_norm": 0.24761904761904763,
      "acc_norm_stderr": 0.04232473532055044
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.4528301886792453,
      "acc_stderr": 0.048577381460134975,
      "acc_norm": 0.4528301886792453,
      "acc_norm_stderr": 0.048577381460134975
    },
    "Cmmlu-college_medicine": {
      "acc": 0.43223443223443225,
      "acc_stderr": 0.03003722126167518,
      "acc_norm": 0.43223443223443225,
      "acc_norm_stderr": 0.03003722126167518
    },
    "Cmmlu-computer_science": {
      "acc": 0.46568627450980393,
      "acc_stderr": 0.03501038327635897,
      "acc_norm": 0.46568627450980393,
      "acc_norm_stderr": 0.03501038327635897
    },
    "Cmmlu-computer_security": {
      "acc": 0.6432748538011696,
      "acc_stderr": 0.03674013002860954,
      "acc_norm": 0.6432748538011696,
      "acc_norm_stderr": 0.03674013002860954
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.04095586993435687,
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.04095586993435687
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.38848920863309355,
      "acc_stderr": 0.041490818209760694,
      "acc_norm": 0.38848920863309355,
      "acc_norm_stderr": 0.041490818209760694
    },
    "Cmmlu-economics": {
      "acc": 0.5283018867924528,
      "acc_stderr": 0.03971408920065892,
      "acc_norm": 0.5283018867924528,
      "acc_norm_stderr": 0.03971408920065892
    },
    "Cmmlu-education": {
      "acc": 0.6380368098159509,
      "acc_stderr": 0.037757007291414416,
      "acc_norm": 0.6380368098159509,
      "acc_norm_stderr": 0.037757007291414416
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.4941860465116279,
      "acc_stderr": 0.038233370649948514,
      "acc_norm": 0.4941860465116279,
      "acc_norm_stderr": 0.038233370649948514
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.44841269841269843,
      "acc_stderr": 0.031391294141342584,
      "acc_norm": 0.44841269841269843,
      "acc_norm_stderr": 0.031391294141342584
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.6060606060606061,
      "acc_stderr": 0.03481285338232963,
      "acc_norm": 0.6060606060606061,
      "acc_norm_stderr": 0.03481285338232963
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.7184873949579832,
      "acc_stderr": 0.029213549414372163,
      "acc_norm": 0.7184873949579832,
      "acc_norm_stderr": 0.029213549414372163
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.29130434782608694,
      "acc_stderr": 0.030025180463241888,
      "acc_norm": 0.29130434782608694,
      "acc_norm_stderr": 0.030025180463241888
    },
    "Cmmlu-ethnology": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.04292596718256981,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.04292596718256981
    },
    "Cmmlu-food_science": {
      "acc": 0.5804195804195804,
      "acc_stderr": 0.041412787292137106,
      "acc_norm": 0.5804195804195804,
      "acc_norm_stderr": 0.041412787292137106
    },
    "Cmmlu-genetics": {
      "acc": 0.4034090909090909,
      "acc_stderr": 0.037084474709274375,
      "acc_norm": 0.4034090909090909,
      "acc_norm_stderr": 0.037084474709274375
    },
    "Cmmlu-global_facts": {
      "acc": 0.5906040268456376,
      "acc_stderr": 0.04041933160039476,
      "acc_norm": 0.5906040268456376,
      "acc_norm_stderr": 0.04041933160039476
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.4911242603550296,
      "acc_stderr": 0.038569759098794135,
      "acc_norm": 0.4911242603550296,
      "acc_norm_stderr": 0.038569759098794135
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.48484848484848486,
      "acc_stderr": 0.043665140741140564,
      "acc_norm": 0.48484848484848486,
      "acc_norm_stderr": 0.043665140741140564
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.6016949152542372,
      "acc_stderr": 0.045258813582940136,
      "acc_norm": 0.6016949152542372,
      "acc_norm_stderr": 0.045258813582940136
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2804878048780488,
      "acc_stderr": 0.035187002288015794,
      "acc_norm": 0.2804878048780488,
      "acc_norm_stderr": 0.035187002288015794
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.5272727272727272,
      "acc_stderr": 0.04782001791380062,
      "acc_norm": 0.5272727272727272,
      "acc_norm_stderr": 0.04782001791380062
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.6013986013986014,
      "acc_stderr": 0.04108719032366424,
      "acc_norm": 0.6013986013986014,
      "acc_norm_stderr": 0.04108719032366424
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.5873015873015873,
      "acc_stderr": 0.04403438954768176,
      "acc_norm": 0.5873015873015873,
      "acc_norm_stderr": 0.04403438954768176
    },
    "Cmmlu-international_law": {
      "acc": 0.4,
      "acc_stderr": 0.03611575592573071,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.03611575592573071
    },
    "Cmmlu-journalism": {
      "acc": 0.5523255813953488,
      "acc_stderr": 0.03802600168672208,
      "acc_norm": 0.5523255813953488,
      "acc_norm_stderr": 0.03802600168672208
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5206812652068127,
      "acc_stderr": 0.024672107548831108,
      "acc_norm": 0.5206812652068127,
      "acc_norm_stderr": 0.024672107548831108
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8691588785046729,
      "acc_stderr": 0.02310638007346452,
      "acc_norm": 0.8691588785046729,
      "acc_norm_stderr": 0.02310638007346452
    },
    "Cmmlu-logical": {
      "acc": 0.42276422764227645,
      "acc_stderr": 0.04472453350546201,
      "acc_norm": 0.42276422764227645,
      "acc_norm_stderr": 0.04472453350546201
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4262295081967213,
      "acc_stderr": 0.04495708831296081,
      "acc_norm": 0.4262295081967213,
      "acc_norm_stderr": 0.04495708831296081
    },
    "Cmmlu-management": {
      "acc": 0.638095238095238,
      "acc_stderr": 0.033240439515935055,
      "acc_norm": 0.638095238095238,
      "acc_norm_stderr": 0.033240439515935055
    },
    "Cmmlu-marketing": {
      "acc": 0.6055555555555555,
      "acc_stderr": 0.0365294726567113,
      "acc_norm": 0.6055555555555555,
      "acc_norm_stderr": 0.0365294726567113
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.6190476190476191,
      "acc_stderr": 0.03541754466656241,
      "acc_norm": 0.6190476190476191,
      "acc_norm_stderr": 0.03541754466656241
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3706896551724138,
      "acc_stderr": 0.04503900094657779,
      "acc_norm": 0.3706896551724138,
      "acc_norm_stderr": 0.04503900094657779
    },
    "Cmmlu-nutrition": {
      "acc": 0.47586206896551725,
      "acc_stderr": 0.0416180850350153,
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.0416180850350153
    },
    "Cmmlu-philosophy": {
      "acc": 0.6095238095238096,
      "acc_stderr": 0.047838322981141455,
      "acc_norm": 0.6095238095238096,
      "acc_norm_stderr": 0.047838322981141455
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5771428571428572,
      "acc_stderr": 0.037451039271051256,
      "acc_norm": 0.5771428571428572,
      "acc_norm_stderr": 0.037451039271051256
    },
    "Cmmlu-professional_law": {
      "acc": 0.41706161137440756,
      "acc_stderr": 0.03402528637381252,
      "acc_norm": 0.41706161137440756,
      "acc_norm_stderr": 0.03402528637381252
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.34308510638297873,
      "acc_stderr": 0.024515449069850318,
      "acc_norm": 0.34308510638297873,
      "acc_norm_stderr": 0.024515449069850318
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5948275862068966,
      "acc_stderr": 0.032300518598102644,
      "acc_norm": 0.5948275862068966,
      "acc_norm_stderr": 0.032300518598102644
    },
    "Cmmlu-public_relations": {
      "acc": 0.5402298850574713,
      "acc_stderr": 0.03789104827773085,
      "acc_norm": 0.5402298850574713,
      "acc_norm_stderr": 0.03789104827773085
    },
    "Cmmlu-security_study": {
      "acc": 0.6074074074074074,
      "acc_stderr": 0.04218506215368879,
      "acc_norm": 0.6074074074074074,
      "acc_norm_stderr": 0.04218506215368879
    },
    "Cmmlu-sociology": {
      "acc": 0.5530973451327433,
      "acc_stderr": 0.0331448452127496,
      "acc_norm": 0.5530973451327433,
      "acc_norm_stderr": 0.0331448452127496
    },
    "Cmmlu-sports_science": {
      "acc": 0.5393939393939394,
      "acc_stderr": 0.03892207016552013,
      "acc_norm": 0.5393939393939394,
      "acc_norm_stderr": 0.03892207016552013
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.4918918918918919,
      "acc_stderr": 0.036855642198496893,
      "acc_norm": 0.4918918918918919,
      "acc_norm_stderr": 0.036855642198496893
    },
    "Cmmlu-virology": {
      "acc": 0.47337278106508873,
      "acc_stderr": 0.038521097436200316,
      "acc_norm": 0.47337278106508873,
      "acc_norm_stderr": 0.038521097436200316
    },
    "Cmmlu-world_history": {
      "acc": 0.6894409937888198,
      "acc_stderr": 0.03658142543288739,
      "acc_norm": 0.6894409937888198,
      "acc_norm_stderr": 0.03658142543288739
    },
    "Cmmlu-world_religions": {
      "acc": 0.53125,
      "acc_stderr": 0.039575057062617526,
      "acc_norm": 0.53125,
      "acc_norm_stderr": 0.039575057062617526
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/internlm_7b,dtype='float16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_internlm_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:01:03.038064",
    "model_name": "internlm_7b"
  }
}
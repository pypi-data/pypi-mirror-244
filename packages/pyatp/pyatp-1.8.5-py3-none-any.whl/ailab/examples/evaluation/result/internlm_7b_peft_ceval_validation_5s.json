{
  "results": {
    "Ceval-valid-accountant": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.07142857142857144,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.07142857142857144
    },
    "Ceval-valid-advanced_mathematics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.10379087338771256,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.10379087338771256
    },
    "Ceval-valid-art_studies": {
      "acc": 0.5757575757575758,
      "acc_stderr": 0.08736789844447573,
      "acc_norm": 0.5757575757575758,
      "acc_norm_stderr": 0.08736789844447573
    },
    "Ceval-valid-basic_medicine": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-business_administration": {
      "acc": 0.42424242424242425,
      "acc_stderr": 0.08736789844447573,
      "acc_norm": 0.42424242424242425,
      "acc_norm_stderr": 0.08736789844447573
    },
    "Ceval-valid-chinese_language_and_literature": {
      "acc": 0.43478260869565216,
      "acc_stderr": 0.10568965974008647,
      "acc_norm": 0.43478260869565216,
      "acc_norm_stderr": 0.10568965974008647
    },
    "Ceval-valid-civil_servant": {
      "acc": 0.48936170212765956,
      "acc_stderr": 0.07370428968378204,
      "acc_norm": 0.48936170212765956,
      "acc_norm_stderr": 0.07370428968378204
    },
    "Ceval-valid-clinical_medicine": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.10865714630312667,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.10865714630312667
    },
    "Ceval-valid-college_chemistry": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.09829463743659808,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.09829463743659808
    },
    "Ceval-valid-college_economics": {
      "acc": 0.43636363636363634,
      "acc_stderr": 0.06748805288278813,
      "acc_norm": 0.43636363636363634,
      "acc_norm_stderr": 0.06748805288278813
    },
    "Ceval-valid-college_physics": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.11369720523522558,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.11369720523522558
    },
    "Ceval-valid-college_programming": {
      "acc": 0.5135135135135135,
      "acc_stderr": 0.08330289193201319,
      "acc_norm": 0.5135135135135135,
      "acc_norm_stderr": 0.08330289193201319
    },
    "Ceval-valid-computer_architecture": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.11065666703449763,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.11065666703449763
    },
    "Ceval-valid-computer_network": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-discrete_mathematics": {
      "acc": 0.125,
      "acc_stderr": 0.08539125638299665,
      "acc_norm": 0.125,
      "acc_norm_stderr": 0.08539125638299665
    },
    "Ceval-valid-education_science": {
      "acc": 0.6551724137931034,
      "acc_stderr": 0.08982552969857374,
      "acc_norm": 0.6551724137931034,
      "acc_norm_stderr": 0.08982552969857374
    },
    "Ceval-valid-electrical_engineer": {
      "acc": 0.32432432432432434,
      "acc_stderr": 0.07802030664724673,
      "acc_norm": 0.32432432432432434,
      "acc_norm_stderr": 0.07802030664724673
    },
    "Ceval-valid-environmental_impact_assessment_engineer": {
      "acc": 0.6451612903225806,
      "acc_stderr": 0.08735525166275225,
      "acc_norm": 0.6451612903225806,
      "acc_norm_stderr": 0.08735525166275225
    },
    "Ceval-valid-fire_engineer": {
      "acc": 0.4838709677419355,
      "acc_stderr": 0.09123958466923197,
      "acc_norm": 0.4838709677419355,
      "acc_norm_stderr": 0.09123958466923197
    },
    "Ceval-valid-high_school_biology": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.11768778828946262,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.11768778828946262
    },
    "Ceval-valid-high_school_chemistry": {
      "acc": 0.42105263157894735,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.42105263157894735,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-high_school_chinese": {
      "acc": 0.7368421052631579,
      "acc_stderr": 0.10379087338771255,
      "acc_norm": 0.7368421052631579,
      "acc_norm_stderr": 0.10379087338771255
    },
    "Ceval-valid-high_school_geography": {
      "acc": 0.5789473684210527,
      "acc_stderr": 0.11637279966159299,
      "acc_norm": 0.5789473684210527,
      "acc_norm_stderr": 0.11637279966159299
    },
    "Ceval-valid-high_school_history": {
      "acc": 0.65,
      "acc_stderr": 0.10942433098048308,
      "acc_norm": 0.65,
      "acc_norm_stderr": 0.10942433098048308
    },
    "Ceval-valid-high_school_mathematics": {
      "acc": 0.1111111111111111,
      "acc_stderr": 0.0762215933966706,
      "acc_norm": 0.1111111111111111,
      "acc_norm_stderr": 0.0762215933966706
    },
    "Ceval-valid-high_school_physics": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.1176877882894626,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.1176877882894626
    },
    "Ceval-valid-high_school_politics": {
      "acc": 0.8947368421052632,
      "acc_stderr": 0.0723351864143449,
      "acc_norm": 0.8947368421052632,
      "acc_norm_stderr": 0.0723351864143449
    },
    "Ceval-valid-ideological_and_moral_cultivation": {
      "acc": 0.7894736842105263,
      "acc_stderr": 0.0960916767552923,
      "acc_norm": 0.7894736842105263,
      "acc_norm_stderr": 0.0960916767552923
    },
    "Ceval-valid-law": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.09829463743659808,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.09829463743659808
    },
    "Ceval-valid-legal_professional": {
      "acc": 0.43478260869565216,
      "acc_stderr": 0.10568965974008647,
      "acc_norm": 0.43478260869565216,
      "acc_norm_stderr": 0.10568965974008647
    },
    "Ceval-valid-logic": {
      "acc": 0.5454545454545454,
      "acc_stderr": 0.10865714630312667,
      "acc_norm": 0.5454545454545454,
      "acc_norm_stderr": 0.10865714630312667
    },
    "Ceval-valid-mao_zedong_thought": {
      "acc": 0.7083333333333334,
      "acc_stderr": 0.09477598811252413,
      "acc_norm": 0.7083333333333334,
      "acc_norm_stderr": 0.09477598811252413
    },
    "Ceval-valid-marxism": {
      "acc": 0.6842105263157895,
      "acc_stderr": 0.10956136839295434,
      "acc_norm": 0.6842105263157895,
      "acc_norm_stderr": 0.10956136839295434
    },
    "Ceval-valid-metrology_engineer": {
      "acc": 0.4583333333333333,
      "acc_stderr": 0.10389457216622949,
      "acc_norm": 0.4583333333333333,
      "acc_norm_stderr": 0.10389457216622949
    },
    "Ceval-valid-middle_school_biology": {
      "acc": 0.7619047619047619,
      "acc_stderr": 0.09523809523809523,
      "acc_norm": 0.7619047619047619,
      "acc_norm_stderr": 0.09523809523809523
    },
    "Ceval-valid-middle_school_chemistry": {
      "acc": 0.85,
      "acc_stderr": 0.08191780219091252,
      "acc_norm": 0.85,
      "acc_norm_stderr": 0.08191780219091252
    },
    "Ceval-valid-middle_school_geography": {
      "acc": 0.8333333333333334,
      "acc_stderr": 0.11236664374387367,
      "acc_norm": 0.8333333333333334,
      "acc_norm_stderr": 0.11236664374387367
    },
    "Ceval-valid-middle_school_history": {
      "acc": 0.8181818181818182,
      "acc_stderr": 0.08416546361568647,
      "acc_norm": 0.8181818181818182,
      "acc_norm_stderr": 0.08416546361568647
    },
    "Ceval-valid-middle_school_mathematics": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.1136972052352256,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.1136972052352256
    },
    "Ceval-valid-middle_school_physics": {
      "acc": 0.631578947368421,
      "acc_stderr": 0.11369720523522563,
      "acc_norm": 0.631578947368421,
      "acc_norm_stderr": 0.11369720523522563
    },
    "Ceval-valid-middle_school_politics": {
      "acc": 0.8571428571428571,
      "acc_stderr": 0.07824607964359516,
      "acc_norm": 0.8571428571428571,
      "acc_norm_stderr": 0.07824607964359516
    },
    "Ceval-valid-modern_chinese_history": {
      "acc": 0.7391304347826086,
      "acc_stderr": 0.09361833424764437,
      "acc_norm": 0.7391304347826086,
      "acc_norm_stderr": 0.09361833424764437
    },
    "Ceval-valid-operating_system": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.1136972052352256,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.1136972052352256
    },
    "Ceval-valid-physician": {
      "acc": 0.3673469387755102,
      "acc_stderr": 0.06958255967849923,
      "acc_norm": 0.3673469387755102,
      "acc_norm_stderr": 0.06958255967849923
    },
    "Ceval-valid-plant_protection": {
      "acc": 0.6363636363636364,
      "acc_stderr": 0.1049727762162956,
      "acc_norm": 0.6363636363636364,
      "acc_norm_stderr": 0.1049727762162956
    },
    "Ceval-valid-probability_and_statistics": {
      "acc": 0.5,
      "acc_stderr": 0.12126781251816651,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.12126781251816651
    },
    "Ceval-valid-professional_tour_guide": {
      "acc": 0.6896551724137931,
      "acc_stderr": 0.08742975048915692,
      "acc_norm": 0.6896551724137931,
      "acc_norm_stderr": 0.08742975048915692
    },
    "Ceval-valid-sports_science": {
      "acc": 0.6842105263157895,
      "acc_stderr": 0.10956136839295434,
      "acc_norm": 0.6842105263157895,
      "acc_norm_stderr": 0.10956136839295434
    },
    "Ceval-valid-tax_accountant": {
      "acc": 0.3469387755102041,
      "acc_stderr": 0.06870411522695291,
      "acc_norm": 0.3469387755102041,
      "acc_norm_stderr": 0.06870411522695291
    },
    "Ceval-valid-teacher_qualification": {
      "acc": 0.7727272727272727,
      "acc_stderr": 0.06390760676613885,
      "acc_norm": 0.7727272727272727,
      "acc_norm_stderr": 0.06390760676613885
    },
    "Ceval-valid-urban_and_rural_planner": {
      "acc": 0.6304347826086957,
      "acc_stderr": 0.07195473383945741,
      "acc_norm": 0.6304347826086957,
      "acc_norm_stderr": 0.07195473383945741
    },
    "Ceval-valid-veterinary_medicine": {
      "acc": 0.4782608695652174,
      "acc_stderr": 0.10649955403405124,
      "acc_norm": 0.4782608695652174,
      "acc_norm_stderr": 0.10649955403405124
    }
  },
  "versions": {
    "Ceval-valid-accountant": 1,
    "Ceval-valid-advanced_mathematics": 1,
    "Ceval-valid-art_studies": 1,
    "Ceval-valid-basic_medicine": 1,
    "Ceval-valid-business_administration": 1,
    "Ceval-valid-chinese_language_and_literature": 1,
    "Ceval-valid-civil_servant": 1,
    "Ceval-valid-clinical_medicine": 1,
    "Ceval-valid-college_chemistry": 1,
    "Ceval-valid-college_economics": 1,
    "Ceval-valid-college_physics": 1,
    "Ceval-valid-college_programming": 1,
    "Ceval-valid-computer_architecture": 1,
    "Ceval-valid-computer_network": 1,
    "Ceval-valid-discrete_mathematics": 1,
    "Ceval-valid-education_science": 1,
    "Ceval-valid-electrical_engineer": 1,
    "Ceval-valid-environmental_impact_assessment_engineer": 1,
    "Ceval-valid-fire_engineer": 1,
    "Ceval-valid-high_school_biology": 1,
    "Ceval-valid-high_school_chemistry": 1,
    "Ceval-valid-high_school_chinese": 1,
    "Ceval-valid-high_school_geography": 1,
    "Ceval-valid-high_school_history": 1,
    "Ceval-valid-high_school_mathematics": 1,
    "Ceval-valid-high_school_physics": 1,
    "Ceval-valid-high_school_politics": 1,
    "Ceval-valid-ideological_and_moral_cultivation": 1,
    "Ceval-valid-law": 1,
    "Ceval-valid-legal_professional": 1,
    "Ceval-valid-logic": 1,
    "Ceval-valid-mao_zedong_thought": 1,
    "Ceval-valid-marxism": 1,
    "Ceval-valid-metrology_engineer": 1,
    "Ceval-valid-middle_school_biology": 1,
    "Ceval-valid-middle_school_chemistry": 1,
    "Ceval-valid-middle_school_geography": 1,
    "Ceval-valid-middle_school_history": 1,
    "Ceval-valid-middle_school_mathematics": 1,
    "Ceval-valid-middle_school_physics": 1,
    "Ceval-valid-middle_school_politics": 1,
    "Ceval-valid-modern_chinese_history": 1,
    "Ceval-valid-operating_system": 1,
    "Ceval-valid-physician": 1,
    "Ceval-valid-plant_protection": 1,
    "Ceval-valid-probability_and_statistics": 1,
    "Ceval-valid-professional_tour_guide": 1,
    "Ceval-valid-sports_science": 1,
    "Ceval-valid-tax_accountant": 1,
    "Ceval-valid-teacher_qualification": 1,
    "Ceval-valid-urban_and_rural_planner": 1,
    "Ceval-valid-veterinary_medicine": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/internlm_7b,dtype='float16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_internlm_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:13:25.231387",
    "model_name": "internlm_7b"
  }
}
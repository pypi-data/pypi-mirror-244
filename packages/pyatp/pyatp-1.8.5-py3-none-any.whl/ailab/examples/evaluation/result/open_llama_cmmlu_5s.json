{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.25443786982248523,
      "acc_stderr": 0.033603007963315265,
      "acc_norm": 0.25443786982248523,
      "acc_norm_stderr": 0.033603007963315265
    },
    "Cmmlu-anatomy": {
      "acc": 0.22972972972972974,
      "acc_stderr": 0.03469536825407606,
      "acc_norm": 0.22972972972972974,
      "acc_norm_stderr": 0.03469536825407606
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.22560975609756098,
      "acc_stderr": 0.03273897454566342,
      "acc_norm": 0.22560975609756098,
      "acc_norm_stderr": 0.03273897454566342
    },
    "Cmmlu-arts": {
      "acc": 0.24375,
      "acc_stderr": 0.034049163262375844,
      "acc_norm": 0.24375,
      "acc_norm_stderr": 0.034049163262375844
    },
    "Cmmlu-astronomy": {
      "acc": 0.21818181818181817,
      "acc_stderr": 0.03225078108306289,
      "acc_norm": 0.21818181818181817,
      "acc_norm_stderr": 0.03225078108306289
    },
    "Cmmlu-business_ethics": {
      "acc": 0.23923444976076555,
      "acc_stderr": 0.029580506819430464,
      "acc_norm": 0.23923444976076555,
      "acc_norm_stderr": 0.029580506819430464
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.225,
      "acc_stderr": 0.03311643267635493,
      "acc_norm": 0.225,
      "acc_norm_stderr": 0.03311643267635493
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.26717557251908397,
      "acc_stderr": 0.03880848301082395,
      "acc_norm": 0.26717557251908397,
      "acc_norm_stderr": 0.03880848301082395
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.25,
      "acc_stderr": 0.037267799624996496,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.037267799624996496
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.205607476635514,
      "acc_stderr": 0.03925401580070483,
      "acc_norm": 0.205607476635514,
      "acc_norm_stderr": 0.03925401580070483
    },
    "Cmmlu-chinese_history": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.022719255121142968,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.022719255121142968
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.030190282453501954,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.030190282453501954
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.2681564245810056,
      "acc_stderr": 0.033204216306737165,
      "acc_norm": 0.2681564245810056,
      "acc_norm_stderr": 0.033204216306737165
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.23628691983122363,
      "acc_stderr": 0.027652153144159274,
      "acc_norm": 0.23628691983122363,
      "acc_norm_stderr": 0.027652153144159274
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.2830188679245283,
      "acc_stderr": 0.043960933774393765,
      "acc_norm": 0.2830188679245283,
      "acc_norm_stderr": 0.043960933774393765
    },
    "Cmmlu-college_education": {
      "acc": 0.29906542056074764,
      "acc_stderr": 0.044470182376718334,
      "acc_norm": 0.29906542056074764,
      "acc_norm_stderr": 0.044470182376718334
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.2641509433962264,
      "acc_stderr": 0.0430254877395901,
      "acc_norm": 0.2641509433962264,
      "acc_norm_stderr": 0.0430254877395901
    },
    "Cmmlu-college_law": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.04489931073591312,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.04489931073591312
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.29523809523809524,
      "acc_stderr": 0.04472915956044143,
      "acc_norm": 0.29523809523809524,
      "acc_norm_stderr": 0.04472915956044143
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.25471698113207547,
      "acc_stderr": 0.0425201622376331,
      "acc_norm": 0.25471698113207547,
      "acc_norm_stderr": 0.0425201622376331
    },
    "Cmmlu-college_medicine": {
      "acc": 0.21611721611721613,
      "acc_stderr": 0.024956621558492757,
      "acc_norm": 0.21611721611721613,
      "acc_norm_stderr": 0.024956621558492757
    },
    "Cmmlu-computer_science": {
      "acc": 0.24019607843137256,
      "acc_stderr": 0.02998373305591361,
      "acc_norm": 0.24019607843137256,
      "acc_norm_stderr": 0.02998373305591361
    },
    "Cmmlu-computer_security": {
      "acc": 0.2573099415204678,
      "acc_stderr": 0.03352799844161865,
      "acc_norm": 0.2573099415204678,
      "acc_norm_stderr": 0.03352799844161865
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.24489795918367346,
      "acc_stderr": 0.035589261576067566,
      "acc_norm": 0.24489795918367346,
      "acc_norm_stderr": 0.035589261576067566
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2949640287769784,
      "acc_stderr": 0.03881956126735706,
      "acc_norm": 0.2949640287769784,
      "acc_norm_stderr": 0.03881956126735706
    },
    "Cmmlu-economics": {
      "acc": 0.25157232704402516,
      "acc_stderr": 0.034520558111649044,
      "acc_norm": 0.25157232704402516,
      "acc_norm_stderr": 0.034520558111649044
    },
    "Cmmlu-education": {
      "acc": 0.2147239263803681,
      "acc_stderr": 0.03226219377286772,
      "acc_norm": 0.2147239263803681,
      "acc_norm_stderr": 0.03226219377286772
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.2558139534883721,
      "acc_stderr": 0.03336605189761063,
      "acc_norm": 0.2558139534883721,
      "acc_norm_stderr": 0.03336605189761063
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.24603174603174602,
      "acc_stderr": 0.02718536971605992,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.02718536971605992
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.03191178226713547,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.03191178226713547
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.23109243697478993,
      "acc_stderr": 0.02738140692786896,
      "acc_norm": 0.23109243697478993,
      "acc_norm_stderr": 0.02738140692786896
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.2565217391304348,
      "acc_stderr": 0.028858814315305646,
      "acc_norm": 0.2565217391304348,
      "acc_norm_stderr": 0.028858814315305646
    },
    "Cmmlu-ethnology": {
      "acc": 0.24444444444444444,
      "acc_stderr": 0.037125378336148665,
      "acc_norm": 0.24444444444444444,
      "acc_norm_stderr": 0.037125378336148665
    },
    "Cmmlu-food_science": {
      "acc": 0.26573426573426573,
      "acc_stderr": 0.037068604626235575,
      "acc_norm": 0.26573426573426573,
      "acc_norm_stderr": 0.037068604626235575
    },
    "Cmmlu-genetics": {
      "acc": 0.25,
      "acc_stderr": 0.032732683535398856,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.032732683535398856
    },
    "Cmmlu-global_facts": {
      "acc": 0.22818791946308725,
      "acc_stderr": 0.03449619964127218,
      "acc_norm": 0.22818791946308725,
      "acc_norm_stderr": 0.03449619964127218
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.24260355029585798,
      "acc_stderr": 0.033071627503231775,
      "acc_norm": 0.24260355029585798,
      "acc_norm_stderr": 0.033071627503231775
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.21212121212121213,
      "acc_stderr": 0.0357179155646827,
      "acc_norm": 0.21212121212121213,
      "acc_norm_stderr": 0.0357179155646827
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2288135593220339,
      "acc_stderr": 0.0388353872453885,
      "acc_norm": 0.2288135593220339,
      "acc_norm_stderr": 0.0388353872453885
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.03363591048272823,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.03363591048272823
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2636363636363636,
      "acc_stderr": 0.04220224692971987,
      "acc_norm": 0.2636363636363636,
      "acc_norm_stderr": 0.04220224692971987
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.25874125874125875,
      "acc_stderr": 0.03675137438900237,
      "acc_norm": 0.25874125874125875,
      "acc_norm_stderr": 0.03675137438900237
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.04134913018303317,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.04134913018303317
    },
    "Cmmlu-international_law": {
      "acc": 0.25405405405405407,
      "acc_stderr": 0.032092816451453864,
      "acc_norm": 0.25405405405405407,
      "acc_norm_stderr": 0.032092816451453864
    },
    "Cmmlu-journalism": {
      "acc": 0.23255813953488372,
      "acc_stderr": 0.03230654083203451,
      "acc_norm": 0.23255813953488372,
      "acc_norm_stderr": 0.03230654083203451
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.25304136253041365,
      "acc_stderr": 0.021470991853398288,
      "acc_norm": 0.25304136253041365,
      "acc_norm_stderr": 0.021470991853398288
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.3037383177570093,
      "acc_stderr": 0.03150984286811784,
      "acc_norm": 0.3037383177570093,
      "acc_norm_stderr": 0.03150984286811784
    },
    "Cmmlu-logical": {
      "acc": 0.25203252032520324,
      "acc_stderr": 0.03930879526823992,
      "acc_norm": 0.25203252032520324,
      "acc_norm_stderr": 0.03930879526823992
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2540983606557377,
      "acc_stderr": 0.03957756102798663,
      "acc_norm": 0.2540983606557377,
      "acc_norm_stderr": 0.03957756102798663
    },
    "Cmmlu-management": {
      "acc": 0.2761904761904762,
      "acc_stderr": 0.030927395843275772,
      "acc_norm": 0.2761904761904762,
      "acc_norm_stderr": 0.030927395843275772
    },
    "Cmmlu-marketing": {
      "acc": 0.20555555555555555,
      "acc_stderr": 0.030204375458665847,
      "acc_norm": 0.20555555555555555,
      "acc_norm_stderr": 0.030204375458665847
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.20634920634920634,
      "acc_stderr": 0.029514620555468683,
      "acc_norm": 0.20634920634920634,
      "acc_norm_stderr": 0.029514620555468683
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.041678081808441514,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.041678081808441514
    },
    "Cmmlu-nutrition": {
      "acc": 0.2482758620689655,
      "acc_stderr": 0.03600105692727773,
      "acc_norm": 0.2482758620689655,
      "acc_norm_stderr": 0.03600105692727773
    },
    "Cmmlu-philosophy": {
      "acc": 0.19047619047619047,
      "acc_stderr": 0.03850512095536384,
      "acc_norm": 0.19047619047619047,
      "acc_norm_stderr": 0.03850512095536384
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.26857142857142857,
      "acc_stderr": 0.033600151915923894,
      "acc_norm": 0.26857142857142857,
      "acc_norm_stderr": 0.033600151915923894
    },
    "Cmmlu-professional_law": {
      "acc": 0.3127962085308057,
      "acc_stderr": 0.031993655655275954,
      "acc_norm": 0.3127962085308057,
      "acc_norm_stderr": 0.031993655655275954
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2473404255319149,
      "acc_stderr": 0.02228082221281225,
      "acc_norm": 0.2473404255319149,
      "acc_norm_stderr": 0.02228082221281225
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.23706896551724138,
      "acc_stderr": 0.02798169400862498,
      "acc_norm": 0.23706896551724138,
      "acc_norm_stderr": 0.02798169400862498
    },
    "Cmmlu-public_relations": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.03398079939585583,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.03398079939585583
    },
    "Cmmlu-security_study": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.03853254836552003,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.03853254836552003
    },
    "Cmmlu-sociology": {
      "acc": 0.23893805309734514,
      "acc_stderr": 0.028428988326033675,
      "acc_norm": 0.23893805309734514,
      "acc_norm_stderr": 0.028428988326033675
    },
    "Cmmlu-sports_science": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.033175059300091805,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.033175059300091805
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.23243243243243245,
      "acc_stderr": 0.031138505170794653,
      "acc_norm": 0.23243243243243245,
      "acc_norm_stderr": 0.031138505170794653
    },
    "Cmmlu-virology": {
      "acc": 0.24260355029585798,
      "acc_stderr": 0.03307162750323177,
      "acc_norm": 0.24260355029585798,
      "acc_norm_stderr": 0.03307162750323177
    },
    "Cmmlu-world_history": {
      "acc": 0.2732919254658385,
      "acc_stderr": 0.0352316839773709,
      "acc_norm": 0.2732919254658385,
      "acc_norm_stderr": 0.0352316839773709
    },
    "Cmmlu-world_religions": {
      "acc": 0.26875,
      "acc_stderr": 0.035156741348767645,
      "acc_norm": 0.26875,
      "acc_norm_stderr": 0.035156741348767645
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/open_llama_7b,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:1",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:28:21.065148",
    "model_name": "open_llama"
  }
}
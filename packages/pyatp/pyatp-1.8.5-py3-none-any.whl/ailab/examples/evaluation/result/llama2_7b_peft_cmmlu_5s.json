{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.3136094674556213,
      "acc_stderr": 0.03579526516456226,
      "acc_norm": 0.3136094674556213,
      "acc_norm_stderr": 0.03579526516456226
    },
    "Cmmlu-anatomy": {
      "acc": 0.25,
      "acc_stderr": 0.03571428571428571,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03571428571428571
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.21951219512195122,
      "acc_stderr": 0.03242041613395386,
      "acc_norm": 0.21951219512195122,
      "acc_norm_stderr": 0.03242041613395386
    },
    "Cmmlu-arts": {
      "acc": 0.29375,
      "acc_stderr": 0.03612181848191273,
      "acc_norm": 0.29375,
      "acc_norm_stderr": 0.03612181848191273
    },
    "Cmmlu-astronomy": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.036810508691615486,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.036810508691615486
    },
    "Cmmlu-business_ethics": {
      "acc": 0.3875598086124402,
      "acc_stderr": 0.033780769688873835,
      "acc_norm": 0.3875598086124402,
      "acc_norm_stderr": 0.033780769688873835
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.25625,
      "acc_stderr": 0.0346215784586514,
      "acc_norm": 0.25625,
      "acc_norm_stderr": 0.0346215784586514
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.3893129770992366,
      "acc_stderr": 0.04276486542814591,
      "acc_norm": 0.3893129770992366,
      "acc_norm_stderr": 0.04276486542814591
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.3382352941176471,
      "acc_stderr": 0.04071874442606893,
      "acc_norm": 0.3382352941176471,
      "acc_norm_stderr": 0.04071874442606893
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.3644859813084112,
      "acc_stderr": 0.046746602211107734,
      "acc_norm": 0.3644859813084112,
      "acc_norm_stderr": 0.046746602211107734
    },
    "Cmmlu-chinese_history": {
      "acc": 0.32507739938080493,
      "acc_stderr": 0.026103121097542564,
      "acc_norm": 0.32507739938080493,
      "acc_norm_stderr": 0.026103121097542564
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.3137254901960784,
      "acc_stderr": 0.03256685484460388,
      "acc_norm": 0.3137254901960784,
      "acc_norm_stderr": 0.03256685484460388
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.3407821229050279,
      "acc_stderr": 0.035525720039779315,
      "acc_norm": 0.3407821229050279,
      "acc_norm_stderr": 0.035525720039779315
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.2911392405063291,
      "acc_stderr": 0.029571601065753378,
      "acc_norm": 0.2911392405063291,
      "acc_norm_stderr": 0.029571601065753378
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.19811320754716982,
      "acc_stderr": 0.0388972228831855,
      "acc_norm": 0.19811320754716982,
      "acc_norm_stderr": 0.0388972228831855
    },
    "Cmmlu-college_education": {
      "acc": 0.38317757009345793,
      "acc_stderr": 0.04722013080771233,
      "acc_norm": 0.38317757009345793,
      "acc_norm_stderr": 0.04722013080771233
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.42452830188679247,
      "acc_stderr": 0.048235930372434704,
      "acc_norm": 0.42452830188679247,
      "acc_norm_stderr": 0.048235930372434704
    },
    "Cmmlu-college_law": {
      "acc": 0.25925925925925924,
      "acc_stderr": 0.042365112580946315,
      "acc_norm": 0.25925925925925924,
      "acc_norm_stderr": 0.042365112580946315
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2857142857142857,
      "acc_stderr": 0.04429811949614585,
      "acc_norm": 0.2857142857142857,
      "acc_norm_stderr": 0.04429811949614585
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3113207547169811,
      "acc_stderr": 0.0451874553177075,
      "acc_norm": 0.3113207547169811,
      "acc_norm_stderr": 0.0451874553177075
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2893772893772894,
      "acc_stderr": 0.027495860234525278,
      "acc_norm": 0.2893772893772894,
      "acc_norm_stderr": 0.027495860234525278
    },
    "Cmmlu-computer_science": {
      "acc": 0.35294117647058826,
      "acc_stderr": 0.03354092437591518,
      "acc_norm": 0.35294117647058826,
      "acc_norm_stderr": 0.03354092437591518
    },
    "Cmmlu-computer_security": {
      "acc": 0.3216374269005848,
      "acc_stderr": 0.03582529442573122,
      "acc_norm": 0.3216374269005848,
      "acc_norm_stderr": 0.03582529442573122
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.32653061224489793,
      "acc_stderr": 0.03881007243853121,
      "acc_norm": 0.32653061224489793,
      "acc_norm_stderr": 0.03881007243853121
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2805755395683453,
      "acc_stderr": 0.03824529014900686,
      "acc_norm": 0.2805755395683453,
      "acc_norm_stderr": 0.03824529014900686
    },
    "Cmmlu-economics": {
      "acc": 0.29559748427672955,
      "acc_stderr": 0.036302143777231344,
      "acc_norm": 0.29559748427672955,
      "acc_norm_stderr": 0.036302143777231344
    },
    "Cmmlu-education": {
      "acc": 0.3619631901840491,
      "acc_stderr": 0.037757007291414416,
      "acc_norm": 0.3619631901840491,
      "acc_norm_stderr": 0.037757007291414416
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.32558139534883723,
      "acc_stderr": 0.03583410038767278,
      "acc_norm": 0.32558139534883723,
      "acc_norm_stderr": 0.03583410038767278
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.25793650793650796,
      "acc_stderr": 0.027614684139414543,
      "acc_norm": 0.25793650793650796,
      "acc_norm_stderr": 0.027614684139414543
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.32323232323232326,
      "acc_stderr": 0.03332299921070645,
      "acc_norm": 0.32323232323232326,
      "acc_norm_stderr": 0.03332299921070645
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.41596638655462187,
      "acc_stderr": 0.03201650100739615,
      "acc_norm": 0.41596638655462187,
      "acc_norm_stderr": 0.03201650100739615
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.27391304347826084,
      "acc_stderr": 0.029470189815005897,
      "acc_norm": 0.27391304347826084,
      "acc_norm_stderr": 0.029470189815005897
    },
    "Cmmlu-ethnology": {
      "acc": 0.362962962962963,
      "acc_stderr": 0.04153948404742401,
      "acc_norm": 0.362962962962963,
      "acc_norm_stderr": 0.04153948404742401
    },
    "Cmmlu-food_science": {
      "acc": 0.32167832167832167,
      "acc_stderr": 0.03919986517659165,
      "acc_norm": 0.32167832167832167,
      "acc_norm_stderr": 0.03919986517659165
    },
    "Cmmlu-genetics": {
      "acc": 0.24431818181818182,
      "acc_stderr": 0.03248092256353737,
      "acc_norm": 0.24431818181818182,
      "acc_norm_stderr": 0.03248092256353737
    },
    "Cmmlu-global_facts": {
      "acc": 0.2953020134228188,
      "acc_stderr": 0.03749763364527049,
      "acc_norm": 0.2953020134228188,
      "acc_norm_stderr": 0.03749763364527049
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.25443786982248523,
      "acc_stderr": 0.033603007963315265,
      "acc_norm": 0.25443786982248523,
      "acc_norm_stderr": 0.033603007963315265
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.038911438636713876,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.038911438636713876
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3050847457627119,
      "acc_stderr": 0.04256799926288002,
      "acc_norm": 0.3050847457627119,
      "acc_norm_stderr": 0.04256799926288002
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.23780487804878048,
      "acc_stderr": 0.033346454086653377,
      "acc_norm": 0.23780487804878048,
      "acc_norm_stderr": 0.033346454086653377
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.3,
      "acc_stderr": 0.04389311454644287,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.04389311454644287
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.2867132867132867,
      "acc_stderr": 0.03795000212801782,
      "acc_norm": 0.2867132867132867,
      "acc_norm_stderr": 0.03795000212801782
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04285714285714281,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04285714285714281
    },
    "Cmmlu-international_law": {
      "acc": 0.34054054054054056,
      "acc_stderr": 0.03493570809271873,
      "acc_norm": 0.34054054054054056,
      "acc_norm_stderr": 0.03493570809271873
    },
    "Cmmlu-journalism": {
      "acc": 0.3372093023255814,
      "acc_stderr": 0.036152631988716336,
      "acc_norm": 0.3372093023255814,
      "acc_norm_stderr": 0.036152631988716336
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.340632603406326,
      "acc_stderr": 0.023405337774718562,
      "acc_norm": 0.340632603406326,
      "acc_norm_stderr": 0.023405337774718562
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.5654205607476636,
      "acc_stderr": 0.033964919089940517,
      "acc_norm": 0.5654205607476636,
      "acc_norm_stderr": 0.033964919089940517
    },
    "Cmmlu-logical": {
      "acc": 0.37398373983739835,
      "acc_stderr": 0.04380657018753398,
      "acc_norm": 0.37398373983739835,
      "acc_norm_stderr": 0.04380657018753398
    },
    "Cmmlu-machine_learning": {
      "acc": 0.23770491803278687,
      "acc_stderr": 0.038697949843811565,
      "acc_norm": 0.23770491803278687,
      "acc_norm_stderr": 0.038697949843811565
    },
    "Cmmlu-management": {
      "acc": 0.32857142857142857,
      "acc_stderr": 0.032489397968768416,
      "acc_norm": 0.32857142857142857,
      "acc_norm_stderr": 0.032489397968768416
    },
    "Cmmlu-marketing": {
      "acc": 0.34444444444444444,
      "acc_stderr": 0.03551712696743982,
      "acc_norm": 0.34444444444444444,
      "acc_norm_stderr": 0.03551712696743982
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4126984126984127,
      "acc_stderr": 0.03590608560215488,
      "acc_norm": 0.4126984126984127,
      "acc_norm_stderr": 0.03590608560215488
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3275862068965517,
      "acc_stderr": 0.04376552980994348,
      "acc_norm": 0.3275862068965517,
      "acc_norm_stderr": 0.04376552980994348
    },
    "Cmmlu-nutrition": {
      "acc": 0.2689655172413793,
      "acc_stderr": 0.036951833116502325,
      "acc_norm": 0.2689655172413793,
      "acc_norm_stderr": 0.036951833116502325
    },
    "Cmmlu-philosophy": {
      "acc": 0.4,
      "acc_stderr": 0.04803844614152614,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04803844614152614
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.26857142857142857,
      "acc_stderr": 0.033600151915923894,
      "acc_norm": 0.26857142857142857,
      "acc_norm_stderr": 0.033600151915923894
    },
    "Cmmlu-professional_law": {
      "acc": 0.3222748815165877,
      "acc_stderr": 0.032250048524146424,
      "acc_norm": 0.3222748815165877,
      "acc_norm_stderr": 0.032250048524146424
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2526595744680851,
      "acc_stderr": 0.02243941258278639,
      "acc_norm": 0.2526595744680851,
      "acc_norm_stderr": 0.02243941258278639
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.31896551724137934,
      "acc_stderr": 0.030665526709401484,
      "acc_norm": 0.31896551724137934,
      "acc_norm_stderr": 0.030665526709401484
    },
    "Cmmlu-public_relations": {
      "acc": 0.3793103448275862,
      "acc_stderr": 0.036890245521675503,
      "acc_norm": 0.3793103448275862,
      "acc_norm_stderr": 0.036890245521675503
    },
    "Cmmlu-security_study": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04072314811876837,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04072314811876837
    },
    "Cmmlu-sociology": {
      "acc": 0.27876106194690264,
      "acc_stderr": 0.029892647352308933,
      "acc_norm": 0.27876106194690264,
      "acc_norm_stderr": 0.029892647352308933
    },
    "Cmmlu-sports_science": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.03713158067481913,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.03713158067481913
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2756756756756757,
      "acc_stderr": 0.03294252220324153,
      "acc_norm": 0.2756756756756757,
      "acc_norm_stderr": 0.03294252220324153
    },
    "Cmmlu-virology": {
      "acc": 0.35502958579881655,
      "acc_stderr": 0.036918795945769134,
      "acc_norm": 0.35502958579881655,
      "acc_norm_stderr": 0.036918795945769134
    },
    "Cmmlu-world_history": {
      "acc": 0.3416149068322981,
      "acc_stderr": 0.03749284617282494,
      "acc_norm": 0.3416149068322981,
      "acc_norm_stderr": 0.03749284617282494
    },
    "Cmmlu-world_religions": {
      "acc": 0.40625,
      "acc_stderr": 0.03894932504400619,
      "acc_norm": 0.40625,
      "acc_norm_stderr": 0.03894932504400619
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/llama2-7b-hf,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_llama2_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:26:45.818870",
    "model_name": "llama2_7b"
  }
}
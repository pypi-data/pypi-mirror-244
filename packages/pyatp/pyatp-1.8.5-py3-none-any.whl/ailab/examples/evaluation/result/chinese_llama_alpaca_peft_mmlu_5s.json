{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.039725528847851375
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3815789473684211,
      "acc_stderr": 0.03953173377749194,
      "acc_norm": 0.3815789473684211,
      "acc_norm_stderr": 0.03953173377749194
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939098,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939098
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.41132075471698115,
      "acc_stderr": 0.030285009259009798,
      "acc_norm": 0.41132075471698115,
      "acc_norm_stderr": 0.030285009259009798
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3125,
      "acc_stderr": 0.038760854559127644,
      "acc_norm": 0.3125,
      "acc_norm_stderr": 0.038760854559127644
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.29,
      "acc_stderr": 0.045604802157206845,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.045604802157206845
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.26,
      "acc_stderr": 0.04408440022768078,
      "acc_norm": 0.26,
      "acc_norm_stderr": 0.04408440022768078
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.30057803468208094,
      "acc_stderr": 0.03496101481191181,
      "acc_norm": 0.30057803468208094,
      "acc_norm_stderr": 0.03496101481191181
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.043898699568087785,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.043898699568087785
    },
    "hendrycksTest-computer_security": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.28936170212765955,
      "acc_stderr": 0.029644006577009618,
      "acc_norm": 0.28936170212765955,
      "acc_norm_stderr": 0.029644006577009618
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2719298245614035,
      "acc_stderr": 0.041857744240220554,
      "acc_norm": 0.2719298245614035,
      "acc_norm_stderr": 0.041857744240220554
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.31724137931034485,
      "acc_stderr": 0.038783523721386215,
      "acc_norm": 0.31724137931034485,
      "acc_norm_stderr": 0.038783523721386215
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.023068188848261107,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.023068188848261107
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.037184890068181146,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.037184890068181146
    },
    "hendrycksTest-global_facts": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3580645161290323,
      "acc_stderr": 0.027273890594300642,
      "acc_norm": 0.3580645161290323,
      "acc_norm_stderr": 0.027273890594300642
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2660098522167488,
      "acc_stderr": 0.031089826002937523,
      "acc_norm": 0.2660098522167488,
      "acc_norm_stderr": 0.031089826002937523
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.037563357751878974,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.037563357751878974
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.4393939393939394,
      "acc_stderr": 0.035360859475294805,
      "acc_norm": 0.4393939393939394,
      "acc_norm_stderr": 0.035360859475294805
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.45595854922279794,
      "acc_stderr": 0.035944137112724366,
      "acc_norm": 0.45595854922279794,
      "acc_norm_stderr": 0.035944137112724366
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.32051282051282054,
      "acc_stderr": 0.02366129639396428,
      "acc_norm": 0.32051282051282054,
      "acc_norm_stderr": 0.02366129639396428
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.026067159222275794,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.026067159222275794
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.31932773109243695,
      "acc_stderr": 0.030283995525884396,
      "acc_norm": 0.31932773109243695,
      "acc_norm_stderr": 0.030283995525884396
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.304635761589404,
      "acc_stderr": 0.03757949922943343,
      "acc_norm": 0.304635761589404,
      "acc_norm_stderr": 0.03757949922943343
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.43302752293577984,
      "acc_stderr": 0.021244146569074345,
      "acc_norm": 0.43302752293577984,
      "acc_norm_stderr": 0.021244146569074345
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.4305555555555556,
      "acc_stderr": 0.03376922151252335,
      "acc_norm": 0.4305555555555556,
      "acc_norm_stderr": 0.03376922151252335
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.034107853389047184,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.034107853389047184
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3670886075949367,
      "acc_stderr": 0.03137624072561619,
      "acc_norm": 0.3670886075949367,
      "acc_norm_stderr": 0.03137624072561619
    },
    "hendrycksTest-human_aging": {
      "acc": 0.2645739910313901,
      "acc_stderr": 0.02960510321703833,
      "acc_norm": 0.2645739910313901,
      "acc_norm_stderr": 0.02960510321703833
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.37404580152671757,
      "acc_stderr": 0.04243869242230524,
      "acc_norm": 0.37404580152671757,
      "acc_norm_stderr": 0.04243869242230524
    },
    "hendrycksTest-international_law": {
      "acc": 0.30578512396694213,
      "acc_stderr": 0.04205953933884124,
      "acc_norm": 0.30578512396694213,
      "acc_norm_stderr": 0.04205953933884124
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.3611111111111111,
      "acc_stderr": 0.04643454608906275,
      "acc_norm": 0.3611111111111111,
      "acc_norm_stderr": 0.04643454608906275
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.294478527607362,
      "acc_stderr": 0.03581165790474082,
      "acc_norm": 0.294478527607362,
      "acc_norm_stderr": 0.03581165790474082
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.23214285714285715,
      "acc_stderr": 0.04007341809755807,
      "acc_norm": 0.23214285714285715,
      "acc_norm_stderr": 0.04007341809755807
    },
    "hendrycksTest-management": {
      "acc": 0.3300970873786408,
      "acc_stderr": 0.0465614711001235,
      "acc_norm": 0.3300970873786408,
      "acc_norm_stderr": 0.0465614711001235
    },
    "hendrycksTest-marketing": {
      "acc": 0.5128205128205128,
      "acc_stderr": 0.0327453193884235,
      "acc_norm": 0.5128205128205128,
      "acc_norm_stderr": 0.0327453193884235
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.35,
      "acc_stderr": 0.04793724854411019,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.04793724854411019
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.42911877394636017,
      "acc_stderr": 0.017699388483126792,
      "acc_norm": 0.42911877394636017,
      "acc_norm_stderr": 0.017699388483126792
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.36127167630057805,
      "acc_stderr": 0.025862201852277895,
      "acc_norm": 0.36127167630057805,
      "acc_norm_stderr": 0.025862201852277895
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24692737430167597,
      "acc_stderr": 0.014422292204808835,
      "acc_norm": 0.24692737430167597,
      "acc_norm_stderr": 0.014422292204808835
    },
    "hendrycksTest-nutrition": {
      "acc": 0.38562091503267976,
      "acc_stderr": 0.02787074527829032,
      "acc_norm": 0.38562091503267976,
      "acc_norm_stderr": 0.02787074527829032
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3504823151125402,
      "acc_stderr": 0.027098652621301754,
      "acc_norm": 0.3504823151125402,
      "acc_norm_stderr": 0.027098652621301754
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.02712511551316686,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.02712511551316686
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.28368794326241137,
      "acc_stderr": 0.02689170942834396,
      "acc_norm": 0.28368794326241137,
      "acc_norm_stderr": 0.02689170942834396
    },
    "hendrycksTest-professional_law": {
      "acc": 0.30247718383311606,
      "acc_stderr": 0.011731524234165699,
      "acc_norm": 0.30247718383311606,
      "acc_norm_stderr": 0.011731524234165699
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.3786764705882353,
      "acc_stderr": 0.02946513363977613,
      "acc_norm": 0.3786764705882353,
      "acc_norm_stderr": 0.02946513363977613
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.31209150326797386,
      "acc_stderr": 0.01874501120127766,
      "acc_norm": 0.31209150326797386,
      "acc_norm_stderr": 0.01874501120127766
    },
    "hendrycksTest-public_relations": {
      "acc": 0.33636363636363636,
      "acc_stderr": 0.04525393596302506,
      "acc_norm": 0.33636363636363636,
      "acc_norm_stderr": 0.04525393596302506
    },
    "hendrycksTest-security_studies": {
      "acc": 0.44081632653061226,
      "acc_stderr": 0.03178419114175363,
      "acc_norm": 0.44081632653061226,
      "acc_norm_stderr": 0.03178419114175363
    },
    "hendrycksTest-sociology": {
      "acc": 0.42786069651741293,
      "acc_stderr": 0.03498541988407795,
      "acc_norm": 0.42786069651741293,
      "acc_norm_stderr": 0.03498541988407795
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621503,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621503
    },
    "hendrycksTest-virology": {
      "acc": 0.28313253012048195,
      "acc_stderr": 0.03507295431370518,
      "acc_norm": 0.28313253012048195,
      "acc_norm_stderr": 0.03507295431370518
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4619883040935672,
      "acc_stderr": 0.03823727092882307,
      "acc_norm": 0.4619883040935672,
      "acc_norm_stderr": 0.03823727092882307
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/home/sdk_models/llama-7b-hf',load_in_8bit=True,dtype='float16',tokenizer='/data1/cgzhang6/tokenizer/chinese_llama_alpaca_tokenizer',use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_chinese_llama_alpaca_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "3:42:19.080479",
    "model_name": "chinese_llama_alpaca"
  }
}
{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.27218934911242604,
      "acc_stderr": 0.03433919627548536,
      "acc_norm": 0.27218934911242604,
      "acc_norm_stderr": 0.03433919627548536
    },
    "Cmmlu-anatomy": {
      "acc": 0.22972972972972974,
      "acc_stderr": 0.03469536825407606,
      "acc_norm": 0.22972972972972974,
      "acc_norm_stderr": 0.03469536825407606
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2621951219512195,
      "acc_stderr": 0.034450002891734596,
      "acc_norm": 0.2621951219512195,
      "acc_norm_stderr": 0.034450002891734596
    },
    "Cmmlu-arts": {
      "acc": 0.26875,
      "acc_stderr": 0.035156741348767645,
      "acc_norm": 0.26875,
      "acc_norm_stderr": 0.035156741348767645
    },
    "Cmmlu-astronomy": {
      "acc": 0.24848484848484848,
      "acc_stderr": 0.03374402644139404,
      "acc_norm": 0.24848484848484848,
      "acc_norm_stderr": 0.03374402644139404
    },
    "Cmmlu-business_ethics": {
      "acc": 0.24401913875598086,
      "acc_stderr": 0.029780753228706103,
      "acc_norm": 0.24401913875598086,
      "acc_norm_stderr": 0.029780753228706103
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.26875,
      "acc_stderr": 0.03515674134876764,
      "acc_norm": 0.26875,
      "acc_norm_stderr": 0.03515674134876764
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.26717557251908397,
      "acc_stderr": 0.03880848301082396,
      "acc_norm": 0.26717557251908397,
      "acc_norm_stderr": 0.03880848301082396
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.25,
      "acc_stderr": 0.037267799624996496,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.037267799624996496
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.24299065420560748,
      "acc_stderr": 0.041657429989652724,
      "acc_norm": 0.24299065420560748,
      "acc_norm_stderr": 0.041657429989652724
    },
    "Cmmlu-chinese_history": {
      "acc": 0.21671826625386997,
      "acc_stderr": 0.022960366833113467,
      "acc_norm": 0.21671826625386997,
      "acc_norm_stderr": 0.022960366833113467
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.029771775228145638,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.029771775228145638
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.24022346368715083,
      "acc_stderr": 0.032021424638044936,
      "acc_norm": 0.24022346368715083,
      "acc_norm_stderr": 0.032021424638044936
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.2109704641350211,
      "acc_stderr": 0.02655837250266192,
      "acc_norm": 0.2109704641350211,
      "acc_norm_stderr": 0.02655837250266192
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.044801270921106716,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.044801270921106716
    },
    "Cmmlu-college_education": {
      "acc": 0.19626168224299065,
      "acc_stderr": 0.038576441428227824,
      "acc_norm": 0.19626168224299065,
      "acc_norm_stderr": 0.038576441428227824
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.20754716981132076,
      "acc_stderr": 0.039577692383779325,
      "acc_norm": 0.20754716981132076,
      "acc_norm_stderr": 0.039577692383779325
    },
    "Cmmlu-college_law": {
      "acc": 0.28703703703703703,
      "acc_stderr": 0.043733130409147614,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.043733130409147614
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.20952380952380953,
      "acc_stderr": 0.03990657150993187,
      "acc_norm": 0.20952380952380953,
      "acc_norm_stderr": 0.03990657150993187
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.2830188679245283,
      "acc_stderr": 0.043960933774393765,
      "acc_norm": 0.2830188679245283,
      "acc_norm_stderr": 0.043960933774393765
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2600732600732601,
      "acc_stderr": 0.02659853762760146,
      "acc_norm": 0.2600732600732601,
      "acc_norm_stderr": 0.02659853762760146
    },
    "Cmmlu-computer_science": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.03058759135160424,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.03058759135160424
    },
    "Cmmlu-computer_security": {
      "acc": 0.23976608187134502,
      "acc_stderr": 0.03274485211946956,
      "acc_norm": 0.23976608187134502,
      "acc_norm_stderr": 0.03274485211946956
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2585034013605442,
      "acc_stderr": 0.03623358323071023,
      "acc_norm": 0.2585034013605442,
      "acc_norm_stderr": 0.03623358323071023
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.2589928057553957,
      "acc_stderr": 0.037291986581642324,
      "acc_norm": 0.2589928057553957,
      "acc_norm_stderr": 0.037291986581642324
    },
    "Cmmlu-economics": {
      "acc": 0.27044025157232704,
      "acc_stderr": 0.03533764101912229,
      "acc_norm": 0.27044025157232704,
      "acc_norm_stderr": 0.03533764101912229
    },
    "Cmmlu-education": {
      "acc": 0.25766871165644173,
      "acc_stderr": 0.03436150827846917,
      "acc_norm": 0.25766871165644173,
      "acc_norm_stderr": 0.03436150827846917
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.23255813953488372,
      "acc_stderr": 0.032306540832034505,
      "acc_norm": 0.23255813953488372,
      "acc_norm_stderr": 0.032306540832034505
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.026241257787125122,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.026241257787125122
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.26262626262626265,
      "acc_stderr": 0.031353050095330834,
      "acc_norm": 0.26262626262626265,
      "acc_norm_stderr": 0.031353050095330834
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.22268907563025211,
      "acc_stderr": 0.027025433498882392,
      "acc_norm": 0.22268907563025211,
      "acc_norm_stderr": 0.027025433498882392
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.26956521739130435,
      "acc_stderr": 0.029322764228949527,
      "acc_norm": 0.26956521739130435,
      "acc_norm_stderr": 0.029322764228949527
    },
    "Cmmlu-ethnology": {
      "acc": 0.2518518518518518,
      "acc_stderr": 0.03749850709174022,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.03749850709174022
    },
    "Cmmlu-food_science": {
      "acc": 0.2517482517482518,
      "acc_stderr": 0.036421927837417066,
      "acc_norm": 0.2517482517482518,
      "acc_norm_stderr": 0.036421927837417066
    },
    "Cmmlu-genetics": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.03167872965623496,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.03167872965623496
    },
    "Cmmlu-global_facts": {
      "acc": 0.2483221476510067,
      "acc_stderr": 0.03551344041697432,
      "acc_norm": 0.2483221476510067,
      "acc_norm_stderr": 0.03551344041697432
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.2485207100591716,
      "acc_stderr": 0.033341501981019636,
      "acc_norm": 0.2485207100591716,
      "acc_norm_stderr": 0.033341501981019636
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.25757575757575757,
      "acc_stderr": 0.03820699814849796,
      "acc_norm": 0.25757575757575757,
      "acc_norm_stderr": 0.03820699814849796
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2796610169491525,
      "acc_stderr": 0.04149459161011112,
      "acc_norm": 0.2796610169491525,
      "acc_norm_stderr": 0.04149459161011112
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2073170731707317,
      "acc_stderr": 0.03175217536073676,
      "acc_norm": 0.2073170731707317,
      "acc_norm_stderr": 0.03175217536073676
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.04265792110940588,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.04265792110940588
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.3076923076923077,
      "acc_stderr": 0.03873144730600104,
      "acc_norm": 0.3076923076923077,
      "acc_norm_stderr": 0.03873144730600104
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.03809523809523809,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.03809523809523809
    },
    "Cmmlu-international_law": {
      "acc": 0.2702702702702703,
      "acc_stderr": 0.03273943999002355,
      "acc_norm": 0.2702702702702703,
      "acc_norm_stderr": 0.03273943999002355
    },
    "Cmmlu-journalism": {
      "acc": 0.29651162790697677,
      "acc_stderr": 0.03492619473255953,
      "acc_norm": 0.29651162790697677,
      "acc_norm_stderr": 0.03492619473255953
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.25304136253041365,
      "acc_stderr": 0.021470991853398295,
      "acc_norm": 0.25304136253041365,
      "acc_norm_stderr": 0.021470991853398295
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.2570093457943925,
      "acc_stderr": 0.029941691533244642,
      "acc_norm": 0.2570093457943925,
      "acc_norm_stderr": 0.029941691533244642
    },
    "Cmmlu-logical": {
      "acc": 0.25203252032520324,
      "acc_stderr": 0.039308795268239924,
      "acc_norm": 0.25203252032520324,
      "acc_norm_stderr": 0.039308795268239924
    },
    "Cmmlu-machine_learning": {
      "acc": 0.22950819672131148,
      "acc_stderr": 0.038228778951954236,
      "acc_norm": 0.22950819672131148,
      "acc_norm_stderr": 0.038228778951954236
    },
    "Cmmlu-management": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.030412684459928757,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.030412684459928757
    },
    "Cmmlu-marketing": {
      "acc": 0.2388888888888889,
      "acc_stderr": 0.03187098535605761,
      "acc_norm": 0.2388888888888889,
      "acc_norm_stderr": 0.03187098535605761
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.23809523809523808,
      "acc_stderr": 0.031063241573973475,
      "acc_norm": 0.23809523809523808,
      "acc_norm_stderr": 0.031063241573973475
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.25,
      "acc_stderr": 0.04037864265436242,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04037864265436242
    },
    "Cmmlu-nutrition": {
      "acc": 0.2413793103448276,
      "acc_stderr": 0.03565998174135302,
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.03565998174135302
    },
    "Cmmlu-philosophy": {
      "acc": 0.22857142857142856,
      "acc_stderr": 0.041175810978451015,
      "acc_norm": 0.22857142857142856,
      "acc_norm_stderr": 0.041175810978451015
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.25142857142857145,
      "acc_stderr": 0.032888897342098204,
      "acc_norm": 0.25142857142857145,
      "acc_norm_stderr": 0.032888897342098204
    },
    "Cmmlu-professional_law": {
      "acc": 0.2843601895734597,
      "acc_stderr": 0.031129489323148667,
      "acc_norm": 0.2843601895734597,
      "acc_norm_stderr": 0.031129489323148667
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2632978723404255,
      "acc_stderr": 0.022743327388426438,
      "acc_norm": 0.2632978723404255,
      "acc_norm_stderr": 0.022743327388426438
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.25,
      "acc_stderr": 0.028490144114909487,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.028490144114909487
    },
    "Cmmlu-public_relations": {
      "acc": 0.2988505747126437,
      "acc_stderr": 0.034802407456637846,
      "acc_norm": 0.2988505747126437,
      "acc_norm_stderr": 0.034802407456637846
    },
    "Cmmlu-security_study": {
      "acc": 0.2518518518518518,
      "acc_stderr": 0.03749850709174022,
      "acc_norm": 0.2518518518518518,
      "acc_norm_stderr": 0.03749850709174022
    },
    "Cmmlu-sociology": {
      "acc": 0.25663716814159293,
      "acc_stderr": 0.02911849599823729,
      "acc_norm": 0.25663716814159293,
      "acc_norm_stderr": 0.02911849599823729
    },
    "Cmmlu-sports_science": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03477691162163659,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03477691162163659
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2594594594594595,
      "acc_stderr": 0.032314709966177586,
      "acc_norm": 0.2594594594594595,
      "acc_norm_stderr": 0.032314709966177586
    },
    "Cmmlu-virology": {
      "acc": 0.23668639053254437,
      "acc_stderr": 0.032793177922689494,
      "acc_norm": 0.23668639053254437,
      "acc_norm_stderr": 0.032793177922689494
    },
    "Cmmlu-world_history": {
      "acc": 0.2484472049689441,
      "acc_stderr": 0.03416149068322981,
      "acc_norm": 0.2484472049689441,
      "acc_norm_stderr": 0.03416149068322981
    },
    "Cmmlu-world_religions": {
      "acc": 0.2625,
      "acc_stderr": 0.0348937065201876,
      "acc_norm": 0.2625,
      "acc_norm_stderr": 0.0348937065201876
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/open_llama_7b,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_open_llama_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "2:13:17.798412",
    "model_name": "open_llama"
  }
}
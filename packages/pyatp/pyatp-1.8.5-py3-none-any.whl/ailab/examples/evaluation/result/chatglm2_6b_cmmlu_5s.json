{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.42011834319526625,
      "acc_stderr": 0.03808034433196808,
      "acc_norm": 0.42011834319526625,
      "acc_norm_stderr": 0.03808034433196808
    },
    "Cmmlu-anatomy": {
      "acc": 0.36486486486486486,
      "acc_stderr": 0.039704563322785984,
      "acc_norm": 0.36486486486486486,
      "acc_norm_stderr": 0.039704563322785984
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.29878048780487804,
      "acc_stderr": 0.03585166336909661,
      "acc_norm": 0.29878048780487804,
      "acc_norm_stderr": 0.03585166336909661
    },
    "Cmmlu-arts": {
      "acc": 0.675,
      "acc_stderr": 0.03714454174077365,
      "acc_norm": 0.675,
      "acc_norm_stderr": 0.03714454174077365
    },
    "Cmmlu-astronomy": {
      "acc": 0.2909090909090909,
      "acc_stderr": 0.03546563019624336,
      "acc_norm": 0.2909090909090909,
      "acc_norm_stderr": 0.03546563019624336
    },
    "Cmmlu-business_ethics": {
      "acc": 0.46411483253588515,
      "acc_stderr": 0.03457935791797729,
      "acc_norm": 0.46411483253588515,
      "acc_norm_stderr": 0.03457935791797729
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.5,
      "acc_stderr": 0.03965257928590721,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.03965257928590721
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5954198473282443,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.5954198473282443,
      "acc_norm_stderr": 0.043046937953806645
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.41911764705882354,
      "acc_stderr": 0.04246637405992851,
      "acc_norm": 0.41911764705882354,
      "acc_norm_stderr": 0.04246637405992851
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.5700934579439252,
      "acc_stderr": 0.04808472349429953,
      "acc_norm": 0.5700934579439252,
      "acc_norm_stderr": 0.04808472349429953
    },
    "Cmmlu-chinese_history": {
      "acc": 0.6501547987616099,
      "acc_stderr": 0.026577762175614854,
      "acc_norm": 0.6501547987616099,
      "acc_norm_stderr": 0.026577762175614854
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.4019607843137255,
      "acc_stderr": 0.034411900234824655,
      "acc_norm": 0.4019607843137255,
      "acc_norm_stderr": 0.034411900234824655
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.5865921787709497,
      "acc_stderr": 0.03691029168738378,
      "acc_norm": 0.5865921787709497,
      "acc_norm_stderr": 0.03691029168738378
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.4050632911392405,
      "acc_stderr": 0.031955147413706725,
      "acc_norm": 0.4050632911392405,
      "acc_norm_stderr": 0.031955147413706725
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.29245283018867924,
      "acc_stderr": 0.044392639061996274,
      "acc_norm": 0.29245283018867924,
      "acc_norm_stderr": 0.044392639061996274
    },
    "Cmmlu-college_education": {
      "acc": 0.6822429906542056,
      "acc_stderr": 0.0452235007738203,
      "acc_norm": 0.6822429906542056,
      "acc_norm_stderr": 0.0452235007738203
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.39622641509433965,
      "acc_stderr": 0.047732492983673595,
      "acc_norm": 0.39622641509433965,
      "acc_norm_stderr": 0.047732492983673595
    },
    "Cmmlu-college_law": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199984,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199984
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.29523809523809524,
      "acc_stderr": 0.044729159560441434,
      "acc_norm": 0.29523809523809524,
      "acc_norm_stderr": 0.044729159560441434
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.44339622641509435,
      "acc_stderr": 0.048481318229754794,
      "acc_norm": 0.44339622641509435,
      "acc_norm_stderr": 0.048481318229754794
    },
    "Cmmlu-college_medicine": {
      "acc": 0.4358974358974359,
      "acc_stderr": 0.030066767691175843,
      "acc_norm": 0.4358974358974359,
      "acc_norm_stderr": 0.030066767691175843
    },
    "Cmmlu-computer_science": {
      "acc": 0.45588235294117646,
      "acc_stderr": 0.034956245220154725,
      "acc_norm": 0.45588235294117646,
      "acc_norm_stderr": 0.034956245220154725
    },
    "Cmmlu-computer_security": {
      "acc": 0.5263157894736842,
      "acc_stderr": 0.038295098689947266,
      "acc_norm": 0.5263157894736842,
      "acc_norm_stderr": 0.038295098689947266
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.6598639455782312,
      "acc_stderr": 0.0392082182208768,
      "acc_norm": 0.6598639455782312,
      "acc_norm_stderr": 0.0392082182208768
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.39568345323741005,
      "acc_stderr": 0.04162618828625744,
      "acc_norm": 0.39568345323741005,
      "acc_norm_stderr": 0.04162618828625744
    },
    "Cmmlu-economics": {
      "acc": 0.44654088050314467,
      "acc_stderr": 0.03954985017675704,
      "acc_norm": 0.44654088050314467,
      "acc_norm_stderr": 0.03954985017675704
    },
    "Cmmlu-education": {
      "acc": 0.6319018404907976,
      "acc_stderr": 0.03789213935838396,
      "acc_norm": 0.6319018404907976,
      "acc_norm_stderr": 0.03789213935838396
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3953488372093023,
      "acc_stderr": 0.03738906664833521,
      "acc_norm": 0.3953488372093023,
      "acc_norm_stderr": 0.03738906664833521
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.46825396825396826,
      "acc_stderr": 0.03149604347936578,
      "acc_norm": 0.46825396825396826,
      "acc_norm_stderr": 0.03149604347936578
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.5050505050505051,
      "acc_stderr": 0.035621707606254015,
      "acc_norm": 0.5050505050505051,
      "acc_norm_stderr": 0.035621707606254015
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.6260504201680672,
      "acc_stderr": 0.031429466378837076,
      "acc_norm": 0.6260504201680672,
      "acc_norm_stderr": 0.031429466378837076
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.3565217391304348,
      "acc_stderr": 0.031651347692206504,
      "acc_norm": 0.3565217391304348,
      "acc_norm_stderr": 0.031651347692206504
    },
    "Cmmlu-ethnology": {
      "acc": 0.37777777777777777,
      "acc_stderr": 0.04188307537595853,
      "acc_norm": 0.37777777777777777,
      "acc_norm_stderr": 0.04188307537595853
    },
    "Cmmlu-food_science": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04178532361608381,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04178532361608381
    },
    "Cmmlu-genetics": {
      "acc": 0.42045454545454547,
      "acc_stderr": 0.037315069392646734,
      "acc_norm": 0.42045454545454547,
      "acc_norm_stderr": 0.037315069392646734
    },
    "Cmmlu-global_facts": {
      "acc": 0.4899328859060403,
      "acc_stderr": 0.041091415327375716,
      "acc_norm": 0.4899328859060403,
      "acc_norm_stderr": 0.041091415327375716
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.6804733727810651,
      "acc_stderr": 0.03597530251676528,
      "acc_norm": 0.6804733727810651,
      "acc_norm_stderr": 0.03597530251676528
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.5606060606060606,
      "acc_stderr": 0.04336309556090911,
      "acc_norm": 0.5606060606060606,
      "acc_norm_stderr": 0.04336309556090911
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.5338983050847458,
      "acc_stderr": 0.04611866011948887,
      "acc_norm": 0.5338983050847458,
      "acc_norm_stderr": 0.04611866011948887
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.3231707317073171,
      "acc_stderr": 0.036632096446028446,
      "acc_norm": 0.3231707317073171,
      "acc_norm_stderr": 0.036632096446028446
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.45454545454545453,
      "acc_stderr": 0.04769300568972743,
      "acc_norm": 0.45454545454545453,
      "acc_norm_stderr": 0.04769300568972743
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.5314685314685315,
      "acc_stderr": 0.041875883974458974,
      "acc_norm": 0.5314685314685315,
      "acc_norm_stderr": 0.041875883974458974
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.0442626668137991,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.0442626668137991
    },
    "Cmmlu-international_law": {
      "acc": 0.34594594594594597,
      "acc_stderr": 0.03506727605846201,
      "acc_norm": 0.34594594594594597,
      "acc_norm_stderr": 0.03506727605846201
    },
    "Cmmlu-journalism": {
      "acc": 0.47093023255813954,
      "acc_stderr": 0.0381712782490057,
      "acc_norm": 0.47093023255813954,
      "acc_norm_stderr": 0.0381712782490057
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.5036496350364964,
      "acc_stderr": 0.024692582087670466,
      "acc_norm": 0.5036496350364964,
      "acc_norm_stderr": 0.024692582087670466
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.8364485981308412,
      "acc_stderr": 0.02534293808681739,
      "acc_norm": 0.8364485981308412,
      "acc_norm_stderr": 0.02534293808681739
    },
    "Cmmlu-logical": {
      "acc": 0.35772357723577236,
      "acc_stderr": 0.04339651526440302,
      "acc_norm": 0.35772357723577236,
      "acc_norm_stderr": 0.04339651526440302
    },
    "Cmmlu-machine_learning": {
      "acc": 0.4262295081967213,
      "acc_stderr": 0.04495708831296081,
      "acc_norm": 0.4262295081967213,
      "acc_norm_stderr": 0.04495708831296081
    },
    "Cmmlu-management": {
      "acc": 0.5904761904761905,
      "acc_stderr": 0.03401477718256437,
      "acc_norm": 0.5904761904761905,
      "acc_norm_stderr": 0.03401477718256437
    },
    "Cmmlu-marketing": {
      "acc": 0.5611111111111111,
      "acc_stderr": 0.0370915696198558,
      "acc_norm": 0.5611111111111111,
      "acc_norm_stderr": 0.0370915696198558
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.5767195767195767,
      "acc_stderr": 0.03603441813251289,
      "acc_norm": 0.5767195767195767,
      "acc_norm_stderr": 0.03603441813251289
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.3706896551724138,
      "acc_stderr": 0.0450390009465778,
      "acc_norm": 0.3706896551724138,
      "acc_norm_stderr": 0.0450390009465778
    },
    "Cmmlu-nutrition": {
      "acc": 0.47586206896551725,
      "acc_stderr": 0.041618085035015295,
      "acc_norm": 0.47586206896551725,
      "acc_norm_stderr": 0.041618085035015295
    },
    "Cmmlu-philosophy": {
      "acc": 0.6095238095238096,
      "acc_stderr": 0.047838322981141455,
      "acc_norm": 0.6095238095238096,
      "acc_norm_stderr": 0.047838322981141455
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.5485714285714286,
      "acc_stderr": 0.037725628985298354,
      "acc_norm": 0.5485714285714286,
      "acc_norm_stderr": 0.037725628985298354
    },
    "Cmmlu-professional_law": {
      "acc": 0.38388625592417064,
      "acc_stderr": 0.033560010105331634,
      "acc_norm": 0.38388625592417064,
      "acc_norm_stderr": 0.033560010105331634
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.3537234042553192,
      "acc_stderr": 0.02469024949447835,
      "acc_norm": 0.3537234042553192,
      "acc_norm_stderr": 0.02469024949447835
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.5560344827586207,
      "acc_stderr": 0.03269034414952844,
      "acc_norm": 0.5560344827586207,
      "acc_norm_stderr": 0.03269034414952844
    },
    "Cmmlu-public_relations": {
      "acc": 0.5229885057471264,
      "acc_stderr": 0.03797409587134035,
      "acc_norm": 0.5229885057471264,
      "acc_norm_stderr": 0.03797409587134035
    },
    "Cmmlu-security_study": {
      "acc": 0.6148148148148148,
      "acc_stderr": 0.042039210401562783,
      "acc_norm": 0.6148148148148148,
      "acc_norm_stderr": 0.042039210401562783
    },
    "Cmmlu-sociology": {
      "acc": 0.4823008849557522,
      "acc_stderr": 0.0333124428756083,
      "acc_norm": 0.4823008849557522,
      "acc_norm_stderr": 0.0333124428756083
    },
    "Cmmlu-sports_science": {
      "acc": 0.48484848484848486,
      "acc_stderr": 0.03902551007374448,
      "acc_norm": 0.48484848484848486,
      "acc_norm_stderr": 0.03902551007374448
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.4864864864864865,
      "acc_stderr": 0.03684702401944814,
      "acc_norm": 0.4864864864864865,
      "acc_norm_stderr": 0.03684702401944814
    },
    "Cmmlu-virology": {
      "acc": 0.47337278106508873,
      "acc_stderr": 0.03852109743620031,
      "acc_norm": 0.47337278106508873,
      "acc_norm_stderr": 0.03852109743620031
    },
    "Cmmlu-world_history": {
      "acc": 0.5838509316770186,
      "acc_stderr": 0.03896865898200244,
      "acc_norm": 0.5838509316770186,
      "acc_norm_stderr": 0.03896865898200244
    },
    "Cmmlu-world_religions": {
      "acc": 0.53125,
      "acc_stderr": 0.039575057062617526,
      "acc_norm": 0.53125,
      "acc_norm_stderr": 0.039575057062617526
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained=/home/sdk_models/chatglm2_6b,add_special_tokens=True,dtype='float16',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "0:40:17.320176",
    "model_name": "chatglm2_6b"
  }
}
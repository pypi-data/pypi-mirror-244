{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.21893491124260356,
      "acc_stderr": 0.031904098844912326,
      "acc_norm": 0.21893491124260356,
      "acc_norm_stderr": 0.031904098844912326
    },
    "Cmmlu-anatomy": {
      "acc": 0.2905405405405405,
      "acc_stderr": 0.03744626397928733,
      "acc_norm": 0.2905405405405405,
      "acc_norm_stderr": 0.03744626397928733
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2804878048780488,
      "acc_stderr": 0.03518700228801578,
      "acc_norm": 0.2804878048780488,
      "acc_norm_stderr": 0.03518700228801578
    },
    "Cmmlu-arts": {
      "acc": 0.26875,
      "acc_stderr": 0.035156741348767645,
      "acc_norm": 0.26875,
      "acc_norm_stderr": 0.035156741348767645
    },
    "Cmmlu-astronomy": {
      "acc": 0.23030303030303031,
      "acc_stderr": 0.03287666758603489,
      "acc_norm": 0.23030303030303031,
      "acc_norm_stderr": 0.03287666758603489
    },
    "Cmmlu-business_ethics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03088028274939802,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03088028274939802
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.23125,
      "acc_stderr": 0.03343758265727744,
      "acc_norm": 0.23125,
      "acc_norm_stderr": 0.03343758265727744
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.2595419847328244,
      "acc_stderr": 0.03844876139785271,
      "acc_norm": 0.2595419847328244,
      "acc_norm_stderr": 0.03844876139785271
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.037970424962817856,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.037970424962817856
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.3177570093457944,
      "acc_stderr": 0.04522350077382029,
      "acc_norm": 0.3177570093457944,
      "acc_norm_stderr": 0.04522350077382029
    },
    "Cmmlu-chinese_history": {
      "acc": 0.25696594427244585,
      "acc_stderr": 0.02435085467633009,
      "acc_norm": 0.25696594427244585,
      "acc_norm_stderr": 0.02435085467633009
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.030190282453501954,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.030190282453501954
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.2737430167597765,
      "acc_stderr": 0.03342001835130119,
      "acc_norm": 0.2737430167597765,
      "acc_norm_stderr": 0.03342001835130119
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.25316455696202533,
      "acc_stderr": 0.028304657943035282,
      "acc_norm": 0.25316455696202533,
      "acc_norm_stderr": 0.028304657943035282
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.14150943396226415,
      "acc_stderr": 0.03401463467418859,
      "acc_norm": 0.14150943396226415,
      "acc_norm_stderr": 0.03401463467418859
    },
    "Cmmlu-college_education": {
      "acc": 0.27102803738317754,
      "acc_stderr": 0.04317273776566668,
      "acc_norm": 0.27102803738317754,
      "acc_norm_stderr": 0.04317273776566668
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.2358490566037736,
      "acc_stderr": 0.04142972007800375,
      "acc_norm": 0.2358490566037736,
      "acc_norm_stderr": 0.04142972007800375
    },
    "Cmmlu-college_law": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.04489931073591311,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.04489931073591311
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.2,
      "acc_stderr": 0.03922322702763677,
      "acc_norm": 0.2,
      "acc_norm_stderr": 0.03922322702763677
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.044801270921106716,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.044801270921106716
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2564102564102564,
      "acc_stderr": 0.0264758517066997,
      "acc_norm": 0.2564102564102564,
      "acc_norm_stderr": 0.0264758517066997
    },
    "Cmmlu-computer_science": {
      "acc": 0.23529411764705882,
      "acc_stderr": 0.029771775228145638,
      "acc_norm": 0.23529411764705882,
      "acc_norm_stderr": 0.029771775228145638
    },
    "Cmmlu-computer_security": {
      "acc": 0.26900584795321636,
      "acc_stderr": 0.034010526201040885,
      "acc_norm": 0.26900584795321636,
      "acc_norm_stderr": 0.034010526201040885
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.2585034013605442,
      "acc_stderr": 0.03623358323071023,
      "acc_norm": 0.2585034013605442,
      "acc_norm_stderr": 0.03623358323071023
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.23741007194244604,
      "acc_stderr": 0.036220593237998276,
      "acc_norm": 0.23741007194244604,
      "acc_norm_stderr": 0.036220593237998276
    },
    "Cmmlu-economics": {
      "acc": 0.25157232704402516,
      "acc_stderr": 0.034520558111649044,
      "acc_norm": 0.25157232704402516,
      "acc_norm_stderr": 0.034520558111649044
    },
    "Cmmlu-education": {
      "acc": 0.22699386503067484,
      "acc_stderr": 0.03291099578615768,
      "acc_norm": 0.22699386503067484,
      "acc_norm_stderr": 0.03291099578615768
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.2558139534883721,
      "acc_stderr": 0.033366051897610625,
      "acc_norm": 0.2558139534883721,
      "acc_norm_stderr": 0.033366051897610625
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.25396825396825395,
      "acc_stderr": 0.02747460833869743,
      "acc_norm": 0.25396825396825395,
      "acc_norm_stderr": 0.02747460833869743
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.25252525252525254,
      "acc_stderr": 0.030954055470365897,
      "acc_norm": 0.25252525252525254,
      "acc_norm_stderr": 0.030954055470365897
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.226890756302521,
      "acc_stderr": 0.02720537153827948,
      "acc_norm": 0.226890756302521,
      "acc_norm_stderr": 0.02720537153827948
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.26521739130434785,
      "acc_stderr": 0.02917176407847258,
      "acc_norm": 0.26521739130434785,
      "acc_norm_stderr": 0.02917176407847258
    },
    "Cmmlu-ethnology": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.038201699145179055,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.038201699145179055
    },
    "Cmmlu-food_science": {
      "acc": 0.27972027972027974,
      "acc_stderr": 0.037667638895398516,
      "acc_norm": 0.27972027972027974,
      "acc_norm_stderr": 0.037667638895398516
    },
    "Cmmlu-genetics": {
      "acc": 0.25,
      "acc_stderr": 0.032732683535398856,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.032732683535398856
    },
    "Cmmlu-global_facts": {
      "acc": 0.26174496644295303,
      "acc_stderr": 0.036133623910754545,
      "acc_norm": 0.26174496644295303,
      "acc_norm_stderr": 0.036133623910754545
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.23076923076923078,
      "acc_stderr": 0.03250593287417369,
      "acc_norm": 0.23076923076923078,
      "acc_norm_stderr": 0.03250593287417369
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.23484848484848486,
      "acc_stderr": 0.03703667194552484,
      "acc_norm": 0.23484848484848486,
      "acc_norm_stderr": 0.03703667194552484
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.2627118644067797,
      "acc_stderr": 0.04068792432070351,
      "acc_norm": 0.2627118644067797,
      "acc_norm_stderr": 0.04068792432070351
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.25,
      "acc_stderr": 0.03391617237346009,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03391617237346009
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.22727272727272727,
      "acc_stderr": 0.04013964554072775,
      "acc_norm": 0.22727272727272727,
      "acc_norm_stderr": 0.04013964554072775
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.26573426573426573,
      "acc_stderr": 0.03706860462623558,
      "acc_norm": 0.26573426573426573,
      "acc_norm_stderr": 0.03706860462623558
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.03932537680392871,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.03932537680392871
    },
    "Cmmlu-international_law": {
      "acc": 0.25405405405405407,
      "acc_stderr": 0.032092816451453864,
      "acc_norm": 0.25405405405405407,
      "acc_norm_stderr": 0.032092816451453864
    },
    "Cmmlu-journalism": {
      "acc": 0.21511627906976744,
      "acc_stderr": 0.03142253684735939,
      "acc_norm": 0.21511627906976744,
      "acc_norm_stderr": 0.03142253684735939
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.25060827250608275,
      "acc_stderr": 0.021402288814095338,
      "acc_norm": 0.25060827250608275,
      "acc_norm_stderr": 0.021402288814095338
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.2850467289719626,
      "acc_stderr": 0.030931932789218724,
      "acc_norm": 0.2850467289719626,
      "acc_norm_stderr": 0.030931932789218724
    },
    "Cmmlu-logical": {
      "acc": 0.25203252032520324,
      "acc_stderr": 0.039308795268239924,
      "acc_norm": 0.25203252032520324,
      "acc_norm_stderr": 0.039308795268239924
    },
    "Cmmlu-machine_learning": {
      "acc": 0.2540983606557377,
      "acc_stderr": 0.03957756102798663,
      "acc_norm": 0.2540983606557377,
      "acc_norm_stderr": 0.03957756102798663
    },
    "Cmmlu-management": {
      "acc": 0.2571428571428571,
      "acc_stderr": 0.030231990420749873,
      "acc_norm": 0.2571428571428571,
      "acc_norm_stderr": 0.030231990420749873
    },
    "Cmmlu-marketing": {
      "acc": 0.25,
      "acc_stderr": 0.032364888900157734,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.032364888900157734
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.25396825396825395,
      "acc_stderr": 0.031746031746031744,
      "acc_norm": 0.25396825396825395,
      "acc_norm_stderr": 0.031746031746031744
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.041678081808441514,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.041678081808441514
    },
    "Cmmlu-nutrition": {
      "acc": 0.2620689655172414,
      "acc_stderr": 0.03664666337225256,
      "acc_norm": 0.2620689655172414,
      "acc_norm_stderr": 0.03664666337225256
    },
    "Cmmlu-philosophy": {
      "acc": 0.21904761904761905,
      "acc_stderr": 0.040556911537178254,
      "acc_norm": 0.21904761904761905,
      "acc_norm_stderr": 0.040556911537178254
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.25142857142857145,
      "acc_stderr": 0.032888897342098204,
      "acc_norm": 0.25142857142857145,
      "acc_norm_stderr": 0.032888897342098204
    },
    "Cmmlu-professional_law": {
      "acc": 0.23696682464454977,
      "acc_stderr": 0.02934308944866773,
      "acc_norm": 0.23696682464454977,
      "acc_norm_stderr": 0.02934308944866773
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.023099237430720333,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.023099237430720333
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.22844827586206898,
      "acc_stderr": 0.027622976595715287,
      "acc_norm": 0.22844827586206898,
      "acc_norm_stderr": 0.027622976595715287
    },
    "Cmmlu-public_relations": {
      "acc": 0.23563218390804597,
      "acc_stderr": 0.03226602373932446,
      "acc_norm": 0.23563218390804597,
      "acc_norm_stderr": 0.03226602373932446
    },
    "Cmmlu-security_study": {
      "acc": 0.24444444444444444,
      "acc_stderr": 0.037125378336148665,
      "acc_norm": 0.24444444444444444,
      "acc_norm_stderr": 0.037125378336148665
    },
    "Cmmlu-sociology": {
      "acc": 0.2345132743362832,
      "acc_stderr": 0.028246281839590662,
      "acc_norm": 0.2345132743362832,
      "acc_norm_stderr": 0.028246281839590662
    },
    "Cmmlu-sports_science": {
      "acc": 0.24242424242424243,
      "acc_stderr": 0.03346409881055953,
      "acc_norm": 0.24242424242424243,
      "acc_norm_stderr": 0.03346409881055953
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2594594594594595,
      "acc_stderr": 0.03231470996617758,
      "acc_norm": 0.2594594594594595,
      "acc_norm_stderr": 0.03231470996617758
    },
    "Cmmlu-virology": {
      "acc": 0.21893491124260356,
      "acc_stderr": 0.03190409884491231,
      "acc_norm": 0.21893491124260356,
      "acc_norm_stderr": 0.03190409884491231
    },
    "Cmmlu-world_history": {
      "acc": 0.2670807453416149,
      "acc_stderr": 0.03497754822823695,
      "acc_norm": 0.2670807453416149,
      "acc_norm_stderr": 0.03497754822823695
    },
    "Cmmlu-world_religions": {
      "acc": 0.26875,
      "acc_stderr": 0.035156741348767645,
      "acc_norm": 0.26875,
      "acc_norm_stderr": 0.035156741348767645
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/falcon_7b,trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:12:02.219398",
    "model_name": "falcon_7b"
  }
}
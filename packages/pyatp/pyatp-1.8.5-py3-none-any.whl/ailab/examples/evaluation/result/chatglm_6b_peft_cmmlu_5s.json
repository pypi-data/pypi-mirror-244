{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.26627218934911245,
      "acc_stderr": 0.03410167836676975,
      "acc_norm": 0.26627218934911245,
      "acc_norm_stderr": 0.03410167836676975
    },
    "Cmmlu-anatomy": {
      "acc": 0.23648648648648649,
      "acc_stderr": 0.03504716241250436,
      "acc_norm": 0.23648648648648649,
      "acc_norm_stderr": 0.03504716241250436
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.24390243902439024,
      "acc_stderr": 0.033635910482728223,
      "acc_norm": 0.24390243902439024,
      "acc_norm_stderr": 0.033635910482728223
    },
    "Cmmlu-arts": {
      "acc": 0.3375,
      "acc_stderr": 0.037499999999999964,
      "acc_norm": 0.3375,
      "acc_norm_stderr": 0.037499999999999964
    },
    "Cmmlu-astronomy": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.03453131801885417,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.03453131801885417
    },
    "Cmmlu-business_ethics": {
      "acc": 0.3444976076555024,
      "acc_stderr": 0.03294948099678349,
      "acc_norm": 0.3444976076555024,
      "acc_norm_stderr": 0.03294948099678349
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.28125,
      "acc_stderr": 0.03565632932250201,
      "acc_norm": 0.28125,
      "acc_norm_stderr": 0.03565632932250201
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.33587786259541985,
      "acc_stderr": 0.041423137719966634,
      "acc_norm": 0.33587786259541985,
      "acc_norm_stderr": 0.041423137719966634
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.3235294117647059,
      "acc_stderr": 0.040263772107873116,
      "acc_norm": 0.3235294117647059,
      "acc_norm_stderr": 0.040263772107873116
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.37383177570093457,
      "acc_stderr": 0.04699273118994851,
      "acc_norm": 0.37383177570093457,
      "acc_norm_stderr": 0.04699273118994851
    },
    "Cmmlu-chinese_history": {
      "acc": 0.43962848297213625,
      "acc_stderr": 0.027660052586805192,
      "acc_norm": 0.43962848297213625,
      "acc_norm_stderr": 0.027660052586805192
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.2647058823529412,
      "acc_stderr": 0.030964517926923413,
      "acc_norm": 0.2647058823529412,
      "acc_norm_stderr": 0.030964517926923413
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.37988826815642457,
      "acc_stderr": 0.0363791806643084,
      "acc_norm": 0.37988826815642457,
      "acc_norm_stderr": 0.0363791806643084
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.2616033755274262,
      "acc_stderr": 0.028609516716994934,
      "acc_norm": 0.2616033755274262,
      "acc_norm_stderr": 0.028609516716994934
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.22641509433962265,
      "acc_stderr": 0.04084247315337099,
      "acc_norm": 0.22641509433962265,
      "acc_norm_stderr": 0.04084247315337099
    },
    "Cmmlu-college_education": {
      "acc": 0.2616822429906542,
      "acc_stderr": 0.0426929191572811,
      "acc_norm": 0.2616822429906542,
      "acc_norm_stderr": 0.0426929191572811
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.3584905660377358,
      "acc_stderr": 0.04679998780012862,
      "acc_norm": 0.3584905660377358,
      "acc_norm_stderr": 0.04679998780012862
    },
    "Cmmlu-college_law": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.04489931073591312,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.04489931073591312
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04622501635210239,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04622501635210239
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.36792452830188677,
      "acc_stderr": 0.04706187110761455,
      "acc_norm": 0.36792452830188677,
      "acc_norm_stderr": 0.04706187110761455
    },
    "Cmmlu-college_medicine": {
      "acc": 0.2783882783882784,
      "acc_stderr": 0.02717645531875414,
      "acc_norm": 0.2783882783882784,
      "acc_norm_stderr": 0.02717645531875414
    },
    "Cmmlu-computer_science": {
      "acc": 0.3480392156862745,
      "acc_stderr": 0.03343311240488418,
      "acc_norm": 0.3480392156862745,
      "acc_norm_stderr": 0.03343311240488418
    },
    "Cmmlu-computer_security": {
      "acc": 0.2982456140350877,
      "acc_stderr": 0.03508771929824563,
      "acc_norm": 0.2982456140350877,
      "acc_norm_stderr": 0.03508771929824563
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.30612244897959184,
      "acc_stderr": 0.038142800826175174,
      "acc_norm": 0.30612244897959184,
      "acc_norm_stderr": 0.038142800826175174
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.30935251798561153,
      "acc_stderr": 0.03934735112547112,
      "acc_norm": 0.30935251798561153,
      "acc_norm_stderr": 0.03934735112547112
    },
    "Cmmlu-economics": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.03750293003086744,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.03750293003086744
    },
    "Cmmlu-education": {
      "acc": 0.3312883435582822,
      "acc_stderr": 0.03697983910025588,
      "acc_norm": 0.3312883435582822,
      "acc_norm_stderr": 0.03697983910025588
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.3023255813953488,
      "acc_stderr": 0.03512091263428369,
      "acc_norm": 0.3023255813953488,
      "acc_norm_stderr": 0.03512091263428369
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.27380952380952384,
      "acc_stderr": 0.028145741115683846,
      "acc_norm": 0.27380952380952384,
      "acc_norm_stderr": 0.028145741115683846
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.35353535353535354,
      "acc_stderr": 0.03406086723547153,
      "acc_norm": 0.35353535353535354,
      "acc_norm_stderr": 0.03406086723547153
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.38235294117647056,
      "acc_stderr": 0.031566630992154156,
      "acc_norm": 0.38235294117647056,
      "acc_norm_stderr": 0.031566630992154156
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.27391304347826084,
      "acc_stderr": 0.0294701898150059,
      "acc_norm": 0.27391304347826084,
      "acc_norm_stderr": 0.0294701898150059
    },
    "Cmmlu-ethnology": {
      "acc": 0.24444444444444444,
      "acc_stderr": 0.037125378336148665,
      "acc_norm": 0.24444444444444444,
      "acc_norm_stderr": 0.037125378336148665
    },
    "Cmmlu-food_science": {
      "acc": 0.32867132867132864,
      "acc_stderr": 0.03941888501263192,
      "acc_norm": 0.32867132867132864,
      "acc_norm_stderr": 0.03941888501263192
    },
    "Cmmlu-genetics": {
      "acc": 0.26136363636363635,
      "acc_stderr": 0.03321382551635591,
      "acc_norm": 0.26136363636363635,
      "acc_norm_stderr": 0.03321382551635591
    },
    "Cmmlu-global_facts": {
      "acc": 0.2550335570469799,
      "acc_stderr": 0.035829121651111746,
      "acc_norm": 0.2550335570469799,
      "acc_norm_stderr": 0.035829121651111746
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.24260355029585798,
      "acc_stderr": 0.033071627503231775,
      "acc_norm": 0.24260355029585798,
      "acc_norm_stderr": 0.033071627503231775
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.20454545454545456,
      "acc_stderr": 0.03524251981380331,
      "acc_norm": 0.20454545454545456,
      "acc_norm_stderr": 0.03524251981380331
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.288135593220339,
      "acc_stderr": 0.04187011593049808,
      "acc_norm": 0.288135593220339,
      "acc_norm_stderr": 0.04187011593049808
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.2621951219512195,
      "acc_stderr": 0.0344500028917346,
      "acc_norm": 0.2621951219512195,
      "acc_norm_stderr": 0.0344500028917346
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.040693063197213754,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.040693063197213754
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.2727272727272727,
      "acc_stderr": 0.03737392962695624,
      "acc_norm": 0.2727272727272727,
      "acc_norm_stderr": 0.03737392962695624
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3412698412698413,
      "acc_stderr": 0.042407993275749234,
      "acc_norm": 0.3412698412698413,
      "acc_norm_stderr": 0.042407993275749234
    },
    "Cmmlu-international_law": {
      "acc": 0.3081081081081081,
      "acc_stderr": 0.03403782277834384,
      "acc_norm": 0.3081081081081081,
      "acc_norm_stderr": 0.03403782277834384
    },
    "Cmmlu-journalism": {
      "acc": 0.3430232558139535,
      "acc_stderr": 0.036302683175748356,
      "acc_norm": 0.3430232558139535,
      "acc_norm_stderr": 0.036302683175748356
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.31873479318734793,
      "acc_stderr": 0.023013406739421847,
      "acc_norm": 0.31873479318734793,
      "acc_norm_stderr": 0.023013406739421847
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.4485981308411215,
      "acc_stderr": 0.03407791733638944,
      "acc_norm": 0.4485981308411215,
      "acc_norm_stderr": 0.03407791733638944
    },
    "Cmmlu-logical": {
      "acc": 0.3252032520325203,
      "acc_stderr": 0.04241153733573297,
      "acc_norm": 0.3252032520325203,
      "acc_norm_stderr": 0.04241153733573297
    },
    "Cmmlu-machine_learning": {
      "acc": 0.26229508196721313,
      "acc_stderr": 0.039989293189265945,
      "acc_norm": 0.26229508196721313,
      "acc_norm_stderr": 0.039989293189265945
    },
    "Cmmlu-management": {
      "acc": 0.28095238095238095,
      "acc_stderr": 0.031090094469344617,
      "acc_norm": 0.28095238095238095,
      "acc_norm_stderr": 0.031090094469344617
    },
    "Cmmlu-marketing": {
      "acc": 0.35555555555555557,
      "acc_stderr": 0.03577832139648919,
      "acc_norm": 0.35555555555555557,
      "acc_norm_stderr": 0.03577832139648919
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.2804232804232804,
      "acc_stderr": 0.03276171742795849,
      "acc_norm": 0.2804232804232804,
      "acc_norm_stderr": 0.03276171742795849
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.28448275862068967,
      "acc_stderr": 0.04207160755584022,
      "acc_norm": 0.28448275862068967,
      "acc_norm_stderr": 0.04207160755584022
    },
    "Cmmlu-nutrition": {
      "acc": 0.30344827586206896,
      "acc_stderr": 0.038312260488503336,
      "acc_norm": 0.30344827586206896,
      "acc_norm_stderr": 0.038312260488503336
    },
    "Cmmlu-philosophy": {
      "acc": 0.3142857142857143,
      "acc_stderr": 0.045521571818039494,
      "acc_norm": 0.3142857142857143,
      "acc_norm_stderr": 0.045521571818039494
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.4514285714285714,
      "acc_stderr": 0.03772562898529836,
      "acc_norm": 0.4514285714285714,
      "acc_norm_stderr": 0.03772562898529836
    },
    "Cmmlu-professional_law": {
      "acc": 0.26540284360189575,
      "acc_stderr": 0.030469670650846666,
      "acc_norm": 0.26540284360189575,
      "acc_norm_stderr": 0.030469670650846666
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.23670212765957446,
      "acc_stderr": 0.021949896304751575,
      "acc_norm": 0.23670212765957446,
      "acc_norm_stderr": 0.021949896304751575
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.34051724137931033,
      "acc_stderr": 0.031179222859254792,
      "acc_norm": 0.34051724137931033,
      "acc_norm_stderr": 0.031179222859254792
    },
    "Cmmlu-public_relations": {
      "acc": 0.3103448275862069,
      "acc_stderr": 0.0351734690130024,
      "acc_norm": 0.3103448275862069,
      "acc_norm_stderr": 0.0351734690130024
    },
    "Cmmlu-security_study": {
      "acc": 0.3111111111111111,
      "acc_stderr": 0.03999262876617723,
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.03999262876617723
    },
    "Cmmlu-sociology": {
      "acc": 0.3141592920353982,
      "acc_stderr": 0.030945344741493037,
      "acc_norm": 0.3141592920353982,
      "acc_norm_stderr": 0.030945344741493037
    },
    "Cmmlu-sports_science": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.037563357751878954,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.037563357751878954
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.2810810810810811,
      "acc_stderr": 0.03313956873549873,
      "acc_norm": 0.2810810810810811,
      "acc_norm_stderr": 0.03313956873549873
    },
    "Cmmlu-virology": {
      "acc": 0.28402366863905326,
      "acc_stderr": 0.03479140427262331,
      "acc_norm": 0.28402366863905326,
      "acc_norm_stderr": 0.03479140427262331
    },
    "Cmmlu-world_history": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.0391230398217976,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.0391230398217976
    },
    "Cmmlu-world_religions": {
      "acc": 0.36875,
      "acc_stderr": 0.03826204233503226,
      "acc_norm": 0.36875,
      "acc_norm_stderr": 0.03826204233503226
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-chatglm",
    "model_args": "pretrained=/home/sdk_models/chatglm-6b,add_special_tokens=True,dtype='float16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_chatglm_6b_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "1:03:48.267551",
    "model_name": "chatglm_6b"
  }
}
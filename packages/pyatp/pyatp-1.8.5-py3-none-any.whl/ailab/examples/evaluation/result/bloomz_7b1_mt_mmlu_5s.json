{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.21,
      "acc_stderr": 0.04093601807403326,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.04093601807403326
    },
    "hendrycksTest-anatomy": {
      "acc": 0.2814814814814815,
      "acc_stderr": 0.03885004245800254,
      "acc_norm": 0.2814814814814815,
      "acc_norm_stderr": 0.03885004245800254
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3815789473684211,
      "acc_stderr": 0.03953173377749194,
      "acc_norm": 0.3815789473684211,
      "acc_norm_stderr": 0.03953173377749194
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621504,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621504
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.4641509433962264,
      "acc_stderr": 0.030693675018458006,
      "acc_norm": 0.4641509433962264,
      "acc_norm_stderr": 0.030693675018458006
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3958333333333333,
      "acc_stderr": 0.04089465449325582,
      "acc_norm": 0.3958333333333333,
      "acc_norm_stderr": 0.04089465449325582
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.4,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3988439306358382,
      "acc_stderr": 0.037336266553835096,
      "acc_norm": 0.3988439306358382,
      "acc_norm_stderr": 0.037336266553835096
    },
    "hendrycksTest-college_physics": {
      "acc": 0.43137254901960786,
      "acc_stderr": 0.04928099597287534,
      "acc_norm": 0.43137254901960786,
      "acc_norm_stderr": 0.04928099597287534
    },
    "hendrycksTest-computer_security": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3574468085106383,
      "acc_stderr": 0.03132941789476425,
      "acc_norm": 0.3574468085106383,
      "acc_norm_stderr": 0.03132941789476425
    },
    "hendrycksTest-econometrics": {
      "acc": 0.23684210526315788,
      "acc_stderr": 0.03999423879281336,
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.03999423879281336
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.35172413793103446,
      "acc_stderr": 0.039792366374974096,
      "acc_norm": 0.35172413793103446,
      "acc_norm_stderr": 0.039792366374974096
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.26455026455026454,
      "acc_stderr": 0.0227174678977086,
      "acc_norm": 0.26455026455026454,
      "acc_norm_stderr": 0.0227174678977086
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.35714285714285715,
      "acc_stderr": 0.04285714285714281,
      "acc_norm": 0.35714285714285715,
      "acc_norm_stderr": 0.04285714285714281
    },
    "hendrycksTest-global_facts": {
      "acc": 0.17,
      "acc_stderr": 0.0377525168068637,
      "acc_norm": 0.17,
      "acc_norm_stderr": 0.0377525168068637
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3774193548387097,
      "acc_stderr": 0.027575960723278243,
      "acc_norm": 0.3774193548387097,
      "acc_norm_stderr": 0.027575960723278243
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3251231527093596,
      "acc_stderr": 0.032957975663112704,
      "acc_norm": 0.3251231527093596,
      "acc_norm_stderr": 0.032957975663112704
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.0368105086916155,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.0368105086916155
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.4797979797979798,
      "acc_stderr": 0.03559443565563919,
      "acc_norm": 0.4797979797979798,
      "acc_norm_stderr": 0.03559443565563919
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.47150259067357514,
      "acc_stderr": 0.03602573571288442,
      "acc_norm": 0.47150259067357514,
      "acc_norm_stderr": 0.03602573571288442
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.4,
      "acc_stderr": 0.024838811988033165,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.024838811988033165
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.28888888888888886,
      "acc_stderr": 0.027634907264178544,
      "acc_norm": 0.28888888888888886,
      "acc_norm_stderr": 0.027634907264178544
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4327731092436975,
      "acc_stderr": 0.03218358107742613,
      "acc_norm": 0.4327731092436975,
      "acc_norm_stderr": 0.03218358107742613
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.33112582781456956,
      "acc_stderr": 0.038425817186598696,
      "acc_norm": 0.33112582781456956,
      "acc_norm_stderr": 0.038425817186598696
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5247706422018349,
      "acc_stderr": 0.021410999753635914,
      "acc_norm": 0.5247706422018349,
      "acc_norm_stderr": 0.021410999753635914
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.46296296296296297,
      "acc_stderr": 0.03400603625538271,
      "acc_norm": 0.46296296296296297,
      "acc_norm_stderr": 0.03400603625538271
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.29901960784313725,
      "acc_stderr": 0.03213325717373616,
      "acc_norm": 0.29901960784313725,
      "acc_norm_stderr": 0.03213325717373616
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.510548523206751,
      "acc_stderr": 0.032539983791662855,
      "acc_norm": 0.510548523206751,
      "acc_norm_stderr": 0.032539983791662855
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3901345291479821,
      "acc_stderr": 0.03273766725459157,
      "acc_norm": 0.3901345291479821,
      "acc_norm_stderr": 0.03273766725459157
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.40458015267175573,
      "acc_stderr": 0.043046937953806645,
      "acc_norm": 0.40458015267175573,
      "acc_norm_stderr": 0.043046937953806645
    },
    "hendrycksTest-international_law": {
      "acc": 0.3884297520661157,
      "acc_stderr": 0.044492703500683815,
      "acc_norm": 0.3884297520661157,
      "acc_norm_stderr": 0.044492703500683815
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.04803752235190192,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.04803752235190192
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.36809815950920244,
      "acc_stderr": 0.03789213935838396,
      "acc_norm": 0.36809815950920244,
      "acc_norm_stderr": 0.03789213935838396
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.26785714285714285,
      "acc_stderr": 0.0420327729146776,
      "acc_norm": 0.26785714285714285,
      "acc_norm_stderr": 0.0420327729146776
    },
    "hendrycksTest-management": {
      "acc": 0.5048543689320388,
      "acc_stderr": 0.049505043821289195,
      "acc_norm": 0.5048543689320388,
      "acc_norm_stderr": 0.049505043821289195
    },
    "hendrycksTest-marketing": {
      "acc": 0.49145299145299143,
      "acc_stderr": 0.032751303000970296,
      "acc_norm": 0.49145299145299143,
      "acc_norm_stderr": 0.032751303000970296
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.45338441890166026,
      "acc_stderr": 0.017802087135850297,
      "acc_norm": 0.45338441890166026,
      "acc_norm_stderr": 0.017802087135850297
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.32947976878612717,
      "acc_stderr": 0.02530525813187971,
      "acc_norm": 0.32947976878612717,
      "acc_norm_stderr": 0.02530525813187971
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2782122905027933,
      "acc_stderr": 0.014987325439963577,
      "acc_norm": 0.2782122905027933,
      "acc_norm_stderr": 0.014987325439963577
    },
    "hendrycksTest-nutrition": {
      "acc": 0.3627450980392157,
      "acc_stderr": 0.02753007844711031,
      "acc_norm": 0.3627450980392157,
      "acc_norm_stderr": 0.02753007844711031
    },
    "hendrycksTest-philosophy": {
      "acc": 0.36977491961414793,
      "acc_stderr": 0.027417996705630995,
      "acc_norm": 0.36977491961414793,
      "acc_norm_stderr": 0.027417996705630995
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3734567901234568,
      "acc_stderr": 0.026915003011380154,
      "acc_norm": 0.3734567901234568,
      "acc_norm_stderr": 0.026915003011380154
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.26595744680851063,
      "acc_stderr": 0.026358065698880585,
      "acc_norm": 0.26595744680851063,
      "acc_norm_stderr": 0.026358065698880585
    },
    "hendrycksTest-professional_law": {
      "acc": 0.29139504563233376,
      "acc_stderr": 0.011605720214257598,
      "acc_norm": 0.29139504563233376,
      "acc_norm_stderr": 0.011605720214257598
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.47058823529411764,
      "acc_stderr": 0.03032024326500413,
      "acc_norm": 0.47058823529411764,
      "acc_norm_stderr": 0.03032024326500413
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3839869281045752,
      "acc_stderr": 0.01967580813528152,
      "acc_norm": 0.3839869281045752,
      "acc_norm_stderr": 0.01967580813528152
    },
    "hendrycksTest-public_relations": {
      "acc": 0.4909090909090909,
      "acc_stderr": 0.04788339768702861,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.04788339768702861
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3795918367346939,
      "acc_stderr": 0.03106721126287248,
      "acc_norm": 0.3795918367346939,
      "acc_norm_stderr": 0.03106721126287248
    },
    "hendrycksTest-sociology": {
      "acc": 0.4577114427860697,
      "acc_stderr": 0.035228658640995975,
      "acc_norm": 0.4577114427860697,
      "acc_norm_stderr": 0.035228658640995975
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.42,
      "acc_stderr": 0.049604496374885836,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.049604496374885836
    },
    "hendrycksTest-virology": {
      "acc": 0.37349397590361444,
      "acc_stderr": 0.037658451171688624,
      "acc_norm": 0.37349397590361444,
      "acc_norm_stderr": 0.037658451171688624
    },
    "hendrycksTest-world_religions": {
      "acc": 0.34502923976608185,
      "acc_stderr": 0.036459813773888065,
      "acc_norm": 0.34502923976608185,
      "acc_norm_stderr": 0.036459813773888065
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/bloomz_7b',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "15:07:46.707785",
    "model_name": "bloomz_7b1_mt"
  }
}
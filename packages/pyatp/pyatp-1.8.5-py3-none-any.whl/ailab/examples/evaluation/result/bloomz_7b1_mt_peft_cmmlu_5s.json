{
  "results": {
    "Cmmlu-agronomy": {
      "acc": 0.39644970414201186,
      "acc_stderr": 0.03773949997679293,
      "acc_norm": 0.39644970414201186,
      "acc_norm_stderr": 0.03773949997679293
    },
    "Cmmlu-anatomy": {
      "acc": 0.25,
      "acc_stderr": 0.03571428571428571,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03571428571428571
    },
    "Cmmlu-ancient_chinese": {
      "acc": 0.2804878048780488,
      "acc_stderr": 0.03518700228801578,
      "acc_norm": 0.2804878048780488,
      "acc_norm_stderr": 0.03518700228801578
    },
    "Cmmlu-arts": {
      "acc": 0.35,
      "acc_stderr": 0.03782614981812041,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.03782614981812041
    },
    "Cmmlu-astronomy": {
      "acc": 0.23636363636363636,
      "acc_stderr": 0.033175059300091805,
      "acc_norm": 0.23636363636363636,
      "acc_norm_stderr": 0.033175059300091805
    },
    "Cmmlu-business_ethics": {
      "acc": 0.41148325358851673,
      "acc_stderr": 0.034121163182778434,
      "acc_norm": 0.41148325358851673,
      "acc_norm_stderr": 0.034121163182778434
    },
    "Cmmlu-chinese_civil_service_exam": {
      "acc": 0.33125,
      "acc_stderr": 0.03732598513993525,
      "acc_norm": 0.33125,
      "acc_norm_stderr": 0.03732598513993525
    },
    "Cmmlu-chinese_driving_rule": {
      "acc": 0.5572519083969466,
      "acc_stderr": 0.043564472026650695,
      "acc_norm": 0.5572519083969466,
      "acc_norm_stderr": 0.043564472026650695
    },
    "Cmmlu-chinese_food_culture": {
      "acc": 0.40441176470588236,
      "acc_stderr": 0.04223943122454429,
      "acc_norm": 0.40441176470588236,
      "acc_norm_stderr": 0.04223943122454429
    },
    "Cmmlu-chinese_foreign_policy": {
      "acc": 0.40186915887850466,
      "acc_stderr": 0.04761979313593575,
      "acc_norm": 0.40186915887850466,
      "acc_norm_stderr": 0.04761979313593575
    },
    "Cmmlu-chinese_history": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.027426612007068012,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.027426612007068012
    },
    "Cmmlu-chinese_literature": {
      "acc": 0.27941176470588236,
      "acc_stderr": 0.03149328104507955,
      "acc_norm": 0.27941176470588236,
      "acc_norm_stderr": 0.03149328104507955
    },
    "Cmmlu-chinese_teacher_qualification": {
      "acc": 0.46368715083798884,
      "acc_stderr": 0.03737761880538031,
      "acc_norm": 0.46368715083798884,
      "acc_norm_stderr": 0.03737761880538031
    },
    "Cmmlu-clinical_knowledge": {
      "acc": 0.26582278481012656,
      "acc_stderr": 0.028756799629658342,
      "acc_norm": 0.26582278481012656,
      "acc_norm_stderr": 0.028756799629658342
    },
    "Cmmlu-college_actuarial_science": {
      "acc": 0.3018867924528302,
      "acc_stderr": 0.044801270921106716,
      "acc_norm": 0.3018867924528302,
      "acc_norm_stderr": 0.044801270921106716
    },
    "Cmmlu-college_education": {
      "acc": 0.4953271028037383,
      "acc_stderr": 0.04856217217482834,
      "acc_norm": 0.4953271028037383,
      "acc_norm_stderr": 0.04856217217482834
    },
    "Cmmlu-college_engineering_hydrology": {
      "acc": 0.37735849056603776,
      "acc_stderr": 0.04730439022852894,
      "acc_norm": 0.37735849056603776,
      "acc_norm_stderr": 0.04730439022852894
    },
    "Cmmlu-college_law": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.04524596007030049,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.04524596007030049
    },
    "Cmmlu-college_mathematics": {
      "acc": 0.24761904761904763,
      "acc_stderr": 0.04232473532055042,
      "acc_norm": 0.24761904761904763,
      "acc_norm_stderr": 0.04232473532055042
    },
    "Cmmlu-college_medical_statistics": {
      "acc": 0.3113207547169811,
      "acc_stderr": 0.0451874553177075,
      "acc_norm": 0.3113207547169811,
      "acc_norm_stderr": 0.0451874553177075
    },
    "Cmmlu-college_medicine": {
      "acc": 0.31135531135531136,
      "acc_stderr": 0.02807639142262989,
      "acc_norm": 0.31135531135531136,
      "acc_norm_stderr": 0.02807639142262989
    },
    "Cmmlu-computer_science": {
      "acc": 0.39705882352941174,
      "acc_stderr": 0.034341311647191286,
      "acc_norm": 0.39705882352941174,
      "acc_norm_stderr": 0.034341311647191286
    },
    "Cmmlu-computer_security": {
      "acc": 0.3567251461988304,
      "acc_stderr": 0.03674013002860954,
      "acc_norm": 0.3567251461988304,
      "acc_norm_stderr": 0.03674013002860954
    },
    "Cmmlu-conceptual_physics": {
      "acc": 0.3401360544217687,
      "acc_stderr": 0.0392082182208768,
      "acc_norm": 0.3401360544217687,
      "acc_norm_stderr": 0.0392082182208768
    },
    "Cmmlu-construction_project_management": {
      "acc": 0.35251798561151076,
      "acc_stderr": 0.04066913648640819,
      "acc_norm": 0.35251798561151076,
      "acc_norm_stderr": 0.04066913648640819
    },
    "Cmmlu-economics": {
      "acc": 0.36477987421383645,
      "acc_stderr": 0.03829561213441044,
      "acc_norm": 0.36477987421383645,
      "acc_norm_stderr": 0.03829561213441044
    },
    "Cmmlu-education": {
      "acc": 0.49079754601226994,
      "acc_stderr": 0.03927705600787443,
      "acc_norm": 0.49079754601226994,
      "acc_norm_stderr": 0.03927705600787443
    },
    "Cmmlu-electrical_engineering": {
      "acc": 0.37209302325581395,
      "acc_stderr": 0.03696369368553606,
      "acc_norm": 0.37209302325581395,
      "acc_norm_stderr": 0.03696369368553606
    },
    "Cmmlu-elementary_chinese": {
      "acc": 0.2976190476190476,
      "acc_stderr": 0.028858905984721215,
      "acc_norm": 0.2976190476190476,
      "acc_norm_stderr": 0.028858905984721215
    },
    "Cmmlu-elementary_commonsense": {
      "acc": 0.3787878787878788,
      "acc_stderr": 0.03456088731993747,
      "acc_norm": 0.3787878787878788,
      "acc_norm_stderr": 0.03456088731993747
    },
    "Cmmlu-elementary_information_and_technology": {
      "acc": 0.47058823529411764,
      "acc_stderr": 0.032422250271150074,
      "acc_norm": 0.47058823529411764,
      "acc_norm_stderr": 0.032422250271150074
    },
    "Cmmlu-elementary_mathematics": {
      "acc": 0.2565217391304348,
      "acc_stderr": 0.028858814315305646,
      "acc_norm": 0.2565217391304348,
      "acc_norm_stderr": 0.028858814315305646
    },
    "Cmmlu-ethnology": {
      "acc": 0.32592592592592595,
      "acc_stderr": 0.04049122041702506,
      "acc_norm": 0.32592592592592595,
      "acc_norm_stderr": 0.04049122041702506
    },
    "Cmmlu-food_science": {
      "acc": 0.42657342657342656,
      "acc_stderr": 0.041504160517293893,
      "acc_norm": 0.42657342657342656,
      "acc_norm_stderr": 0.041504160517293893
    },
    "Cmmlu-genetics": {
      "acc": 0.32386363636363635,
      "acc_stderr": 0.03537359640062134,
      "acc_norm": 0.32386363636363635,
      "acc_norm_stderr": 0.03537359640062134
    },
    "Cmmlu-global_facts": {
      "acc": 0.35570469798657717,
      "acc_stderr": 0.039351059072328484,
      "acc_norm": 0.35570469798657717,
      "acc_norm_stderr": 0.039351059072328484
    },
    "Cmmlu-high_school_biology": {
      "acc": 0.2603550295857988,
      "acc_stderr": 0.03385633936516737,
      "acc_norm": 0.2603550295857988,
      "acc_norm_stderr": 0.03385633936516737
    },
    "Cmmlu-high_school_chemistry": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.0401526608280194,
      "acc_norm": 0.30303030303030304,
      "acc_norm_stderr": 0.0401526608280194
    },
    "Cmmlu-high_school_geography": {
      "acc": 0.3220338983050847,
      "acc_stderr": 0.043197822302613424,
      "acc_norm": 0.3220338983050847,
      "acc_norm_stderr": 0.043197822302613424
    },
    "Cmmlu-high_school_mathematics": {
      "acc": 0.21951219512195122,
      "acc_stderr": 0.03242041613395385,
      "acc_norm": 0.21951219512195122,
      "acc_norm_stderr": 0.03242041613395385
    },
    "Cmmlu-high_school_physics": {
      "acc": 0.34545454545454546,
      "acc_stderr": 0.04554619617541054,
      "acc_norm": 0.34545454545454546,
      "acc_norm_stderr": 0.04554619617541054
    },
    "Cmmlu-high_school_politics": {
      "acc": 0.3076923076923077,
      "acc_stderr": 0.03873144730600104,
      "acc_norm": 0.3076923076923077,
      "acc_norm_stderr": 0.03873144730600104
    },
    "Cmmlu-human_sexuality": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.04360314860077459,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.04360314860077459
    },
    "Cmmlu-international_law": {
      "acc": 0.33513513513513515,
      "acc_stderr": 0.03479907984892718,
      "acc_norm": 0.33513513513513515,
      "acc_norm_stderr": 0.03479907984892718
    },
    "Cmmlu-journalism": {
      "acc": 0.37790697674418605,
      "acc_stderr": 0.03707849218723278,
      "acc_norm": 0.37790697674418605,
      "acc_norm_stderr": 0.03707849218723278
    },
    "Cmmlu-jurisprudence": {
      "acc": 0.3236009732360097,
      "acc_stderr": 0.02310545196757981,
      "acc_norm": 0.3236009732360097,
      "acc_norm_stderr": 0.02310545196757981
    },
    "Cmmlu-legal_and_moral_basis": {
      "acc": 0.5794392523364486,
      "acc_stderr": 0.03382427699890584,
      "acc_norm": 0.5794392523364486,
      "acc_norm_stderr": 0.03382427699890584
    },
    "Cmmlu-logical": {
      "acc": 0.2926829268292683,
      "acc_stderr": 0.04119323030208567,
      "acc_norm": 0.2926829268292683,
      "acc_norm_stderr": 0.04119323030208567
    },
    "Cmmlu-machine_learning": {
      "acc": 0.3114754098360656,
      "acc_stderr": 0.0420996926731014,
      "acc_norm": 0.3114754098360656,
      "acc_norm_stderr": 0.0420996926731014
    },
    "Cmmlu-management": {
      "acc": 0.3904761904761905,
      "acc_stderr": 0.03374578018258081,
      "acc_norm": 0.3904761904761905,
      "acc_norm_stderr": 0.03374578018258081
    },
    "Cmmlu-marketing": {
      "acc": 0.4444444444444444,
      "acc_stderr": 0.03714034835915976,
      "acc_norm": 0.4444444444444444,
      "acc_norm_stderr": 0.03714034835915976
    },
    "Cmmlu-marxist_theory": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.035835514581251636,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.035835514581251636
    },
    "Cmmlu-modern_chinese": {
      "acc": 0.2672413793103448,
      "acc_stderr": 0.041265147363241016,
      "acc_norm": 0.2672413793103448,
      "acc_norm_stderr": 0.041265147363241016
    },
    "Cmmlu-nutrition": {
      "acc": 0.35172413793103446,
      "acc_stderr": 0.0397923663749741,
      "acc_norm": 0.35172413793103446,
      "acc_norm_stderr": 0.0397923663749741
    },
    "Cmmlu-philosophy": {
      "acc": 0.42857142857142855,
      "acc_stderr": 0.048526158606197,
      "acc_norm": 0.42857142857142855,
      "acc_norm_stderr": 0.048526158606197
    },
    "Cmmlu-professional_accounting": {
      "acc": 0.37142857142857144,
      "acc_stderr": 0.03663028925010777,
      "acc_norm": 0.37142857142857144,
      "acc_norm_stderr": 0.03663028925010777
    },
    "Cmmlu-professional_law": {
      "acc": 0.3412322274881517,
      "acc_stderr": 0.032717608075019854,
      "acc_norm": 0.3412322274881517,
      "acc_norm_stderr": 0.032717608075019854
    },
    "Cmmlu-professional_medicine": {
      "acc": 0.23138297872340424,
      "acc_stderr": 0.021777351897815815,
      "acc_norm": 0.23138297872340424,
      "acc_norm_stderr": 0.021777351897815815
    },
    "Cmmlu-professional_psychology": {
      "acc": 0.3793103448275862,
      "acc_stderr": 0.031924831026639656,
      "acc_norm": 0.3793103448275862,
      "acc_norm_stderr": 0.031924831026639656
    },
    "Cmmlu-public_relations": {
      "acc": 0.3448275862068966,
      "acc_stderr": 0.03613730415279119,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.03613730415279119
    },
    "Cmmlu-security_study": {
      "acc": 0.4,
      "acc_stderr": 0.042320736951515885,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.042320736951515885
    },
    "Cmmlu-sociology": {
      "acc": 0.336283185840708,
      "acc_stderr": 0.03149580605318968,
      "acc_norm": 0.336283185840708,
      "acc_norm_stderr": 0.03149580605318968
    },
    "Cmmlu-sports_science": {
      "acc": 0.3878787878787879,
      "acc_stderr": 0.0380491365397101,
      "acc_norm": 0.3878787878787879,
      "acc_norm_stderr": 0.0380491365397101
    },
    "Cmmlu-traditional_chinese_medicine": {
      "acc": 0.3567567567567568,
      "acc_stderr": 0.035315455206482514,
      "acc_norm": 0.3567567567567568,
      "acc_norm_stderr": 0.035315455206482514
    },
    "Cmmlu-virology": {
      "acc": 0.34911242603550297,
      "acc_stderr": 0.036777398275939434,
      "acc_norm": 0.34911242603550297,
      "acc_norm_stderr": 0.036777398275939434
    },
    "Cmmlu-world_history": {
      "acc": 0.40993788819875776,
      "acc_stderr": 0.03888193796754317,
      "acc_norm": 0.40993788819875776,
      "acc_norm_stderr": 0.03888193796754317
    },
    "Cmmlu-world_religions": {
      "acc": 0.43125,
      "acc_stderr": 0.03927594984018919,
      "acc_norm": 0.43125,
      "acc_norm_stderr": 0.03927594984018919
    }
  },
  "versions": {
    "Cmmlu-agronomy": 1,
    "Cmmlu-anatomy": 1,
    "Cmmlu-ancient_chinese": 1,
    "Cmmlu-arts": 1,
    "Cmmlu-astronomy": 1,
    "Cmmlu-business_ethics": 1,
    "Cmmlu-chinese_civil_service_exam": 1,
    "Cmmlu-chinese_driving_rule": 1,
    "Cmmlu-chinese_food_culture": 1,
    "Cmmlu-chinese_foreign_policy": 1,
    "Cmmlu-chinese_history": 1,
    "Cmmlu-chinese_literature": 1,
    "Cmmlu-chinese_teacher_qualification": 1,
    "Cmmlu-clinical_knowledge": 1,
    "Cmmlu-college_actuarial_science": 1,
    "Cmmlu-college_education": 1,
    "Cmmlu-college_engineering_hydrology": 1,
    "Cmmlu-college_law": 1,
    "Cmmlu-college_mathematics": 1,
    "Cmmlu-college_medical_statistics": 1,
    "Cmmlu-college_medicine": 1,
    "Cmmlu-computer_science": 1,
    "Cmmlu-computer_security": 1,
    "Cmmlu-conceptual_physics": 1,
    "Cmmlu-construction_project_management": 1,
    "Cmmlu-economics": 1,
    "Cmmlu-education": 1,
    "Cmmlu-electrical_engineering": 1,
    "Cmmlu-elementary_chinese": 1,
    "Cmmlu-elementary_commonsense": 1,
    "Cmmlu-elementary_information_and_technology": 1,
    "Cmmlu-elementary_mathematics": 1,
    "Cmmlu-ethnology": 1,
    "Cmmlu-food_science": 1,
    "Cmmlu-genetics": 1,
    "Cmmlu-global_facts": 1,
    "Cmmlu-high_school_biology": 1,
    "Cmmlu-high_school_chemistry": 1,
    "Cmmlu-high_school_geography": 1,
    "Cmmlu-high_school_mathematics": 1,
    "Cmmlu-high_school_physics": 1,
    "Cmmlu-high_school_politics": 1,
    "Cmmlu-human_sexuality": 1,
    "Cmmlu-international_law": 1,
    "Cmmlu-journalism": 1,
    "Cmmlu-jurisprudence": 1,
    "Cmmlu-legal_and_moral_basis": 1,
    "Cmmlu-logical": 1,
    "Cmmlu-machine_learning": 1,
    "Cmmlu-management": 1,
    "Cmmlu-marketing": 1,
    "Cmmlu-marxist_theory": 1,
    "Cmmlu-modern_chinese": 1,
    "Cmmlu-nutrition": 1,
    "Cmmlu-philosophy": 1,
    "Cmmlu-professional_accounting": 1,
    "Cmmlu-professional_law": 1,
    "Cmmlu-professional_medicine": 1,
    "Cmmlu-professional_psychology": 1,
    "Cmmlu-public_relations": 1,
    "Cmmlu-security_study": 1,
    "Cmmlu-sociology": 1,
    "Cmmlu-sports_science": 1,
    "Cmmlu-traditional_chinese_medicine": 1,
    "Cmmlu-virology": 1,
    "Cmmlu-world_history": 1,
    "Cmmlu-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/bloomz_7b,trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_bloomz_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:2",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "5:56:23.426941",
    "model_name": "bloomz_7b1_mt"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.22,
      "acc_stderr": 0.04163331998932269,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3111111111111111,
      "acc_stderr": 0.039992628766177214,
      "acc_norm": 0.3111111111111111,
      "acc_norm_stderr": 0.039992628766177214
    },
    "hendrycksTest-astronomy": {
      "acc": 0.28289473684210525,
      "acc_stderr": 0.03665349695640767,
      "acc_norm": 0.28289473684210525,
      "acc_norm_stderr": 0.03665349695640767
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3849056603773585,
      "acc_stderr": 0.029946498567699945,
      "acc_norm": 0.3849056603773585,
      "acc_norm_stderr": 0.029946498567699945
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4236111111111111,
      "acc_stderr": 0.041321250197233685,
      "acc_norm": 0.4236111111111111,
      "acc_norm_stderr": 0.041321250197233685
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.35260115606936415,
      "acc_stderr": 0.03643037168958548,
      "acc_norm": 0.35260115606936415,
      "acc_norm_stderr": 0.03643037168958548
    },
    "hendrycksTest-college_physics": {
      "acc": 0.3137254901960784,
      "acc_stderr": 0.04617034827006719,
      "acc_norm": 0.3137254901960784,
      "acc_norm_stderr": 0.04617034827006719
    },
    "hendrycksTest-computer_security": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.39574468085106385,
      "acc_stderr": 0.031967586978353627,
      "acc_norm": 0.39574468085106385,
      "acc_norm_stderr": 0.031967586978353627
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2719298245614035,
      "acc_stderr": 0.04185774424022056,
      "acc_norm": 0.2719298245614035,
      "acc_norm_stderr": 0.04185774424022056
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4068965517241379,
      "acc_stderr": 0.040937939812662374,
      "acc_norm": 0.4068965517241379,
      "acc_norm_stderr": 0.040937939812662374
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.291005291005291,
      "acc_stderr": 0.023393826500484865,
      "acc_norm": 0.291005291005291,
      "acc_norm_stderr": 0.023393826500484865
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.04360314860077459,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.04360314860077459
    },
    "hendrycksTest-global_facts": {
      "acc": 0.22,
      "acc_stderr": 0.041633319989322695,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.041633319989322695
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.3548387096774194,
      "acc_stderr": 0.02721888977330876,
      "acc_norm": 0.3548387096774194,
      "acc_norm_stderr": 0.02721888977330876
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3448275862068966,
      "acc_stderr": 0.03344283744280458,
      "acc_norm": 0.3448275862068966,
      "acc_norm_stderr": 0.03344283744280458
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.30303030303030304,
      "acc_stderr": 0.035886248000917075,
      "acc_norm": 0.30303030303030304,
      "acc_norm_stderr": 0.035886248000917075
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.5050505050505051,
      "acc_stderr": 0.035621707606254015,
      "acc_norm": 0.5050505050505051,
      "acc_norm_stderr": 0.035621707606254015
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.44559585492227977,
      "acc_stderr": 0.0358701498607566,
      "acc_norm": 0.44559585492227977,
      "acc_norm_stderr": 0.0358701498607566
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.35384615384615387,
      "acc_stderr": 0.024243783994062167,
      "acc_norm": 0.35384615384615387,
      "acc_norm_stderr": 0.024243783994062167
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085626,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085626
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.4117647058823529,
      "acc_stderr": 0.031968769891957786,
      "acc_norm": 0.4117647058823529,
      "acc_norm_stderr": 0.031968769891957786
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.037345356767871984,
      "acc_norm": 0.2980132450331126,
      "acc_norm_stderr": 0.037345356767871984
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.5082568807339449,
      "acc_stderr": 0.021434399918214327,
      "acc_norm": 0.5082568807339449,
      "acc_norm_stderr": 0.021434399918214327
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.42592592592592593,
      "acc_stderr": 0.033723432716530624,
      "acc_norm": 0.42592592592592593,
      "acc_norm_stderr": 0.033723432716530624
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.03132179803083291,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.03132179803083291
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4936708860759494,
      "acc_stderr": 0.03254462010767859,
      "acc_norm": 0.4936708860759494,
      "acc_norm_stderr": 0.03254462010767859
    },
    "hendrycksTest-human_aging": {
      "acc": 0.40358744394618834,
      "acc_stderr": 0.03292802819330314,
      "acc_norm": 0.40358744394618834,
      "acc_norm_stderr": 0.03292802819330314
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3969465648854962,
      "acc_stderr": 0.04291135671009224,
      "acc_norm": 0.3969465648854962,
      "acc_norm_stderr": 0.04291135671009224
    },
    "hendrycksTest-international_law": {
      "acc": 0.34710743801652894,
      "acc_stderr": 0.04345724570292535,
      "acc_norm": 0.34710743801652894,
      "acc_norm_stderr": 0.04345724570292535
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.37962962962962965,
      "acc_stderr": 0.04691521224077742,
      "acc_norm": 0.37962962962962965,
      "acc_norm_stderr": 0.04691521224077742
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.36809815950920244,
      "acc_stderr": 0.03789213935838396,
      "acc_norm": 0.36809815950920244,
      "acc_norm_stderr": 0.03789213935838396
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.2767857142857143,
      "acc_stderr": 0.042466243366976235,
      "acc_norm": 0.2767857142857143,
      "acc_norm_stderr": 0.042466243366976235
    },
    "hendrycksTest-management": {
      "acc": 0.3883495145631068,
      "acc_stderr": 0.0482572933735639,
      "acc_norm": 0.3883495145631068,
      "acc_norm_stderr": 0.0482572933735639
    },
    "hendrycksTest-marketing": {
      "acc": 0.46153846153846156,
      "acc_stderr": 0.03265903381186195,
      "acc_norm": 0.46153846153846156,
      "acc_norm_stderr": 0.03265903381186195
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001974,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001974
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.42911877394636017,
      "acc_stderr": 0.017699388483126792,
      "acc_norm": 0.42911877394636017,
      "acc_norm_stderr": 0.017699388483126792
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3468208092485549,
      "acc_stderr": 0.025624723994030457,
      "acc_norm": 0.3468208092485549,
      "acc_norm_stderr": 0.025624723994030457
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.23798882681564246,
      "acc_stderr": 0.014242630070574915,
      "acc_norm": 0.23798882681564246,
      "acc_norm_stderr": 0.014242630070574915
    },
    "hendrycksTest-nutrition": {
      "acc": 0.39215686274509803,
      "acc_stderr": 0.027956046165424516,
      "acc_norm": 0.39215686274509803,
      "acc_norm_stderr": 0.027956046165424516
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3183279742765273,
      "acc_stderr": 0.02645722506781103,
      "acc_norm": 0.3183279742765273,
      "acc_norm_stderr": 0.02645722506781103
    },
    "hendrycksTest-prehistory": {
      "acc": 0.32098765432098764,
      "acc_stderr": 0.02597656601086274,
      "acc_norm": 0.32098765432098764,
      "acc_norm_stderr": 0.02597656601086274
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2978723404255319,
      "acc_stderr": 0.02728160834446941,
      "acc_norm": 0.2978723404255319,
      "acc_norm_stderr": 0.02728160834446941
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2692307692307692,
      "acc_stderr": 0.011328734403140315,
      "acc_norm": 0.2692307692307692,
      "acc_norm_stderr": 0.011328734403140315
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.40808823529411764,
      "acc_stderr": 0.029855261393483927,
      "acc_norm": 0.40808823529411764,
      "acc_norm_stderr": 0.029855261393483927
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3480392156862745,
      "acc_stderr": 0.019270998708223974,
      "acc_norm": 0.3480392156862745,
      "acc_norm_stderr": 0.019270998708223974
    },
    "hendrycksTest-public_relations": {
      "acc": 0.4909090909090909,
      "acc_stderr": 0.04788339768702861,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.04788339768702861
    },
    "hendrycksTest-security_studies": {
      "acc": 0.34285714285714286,
      "acc_stderr": 0.030387262919547728,
      "acc_norm": 0.34285714285714286,
      "acc_norm_stderr": 0.030387262919547728
    },
    "hendrycksTest-sociology": {
      "acc": 0.3482587064676617,
      "acc_stderr": 0.033687874661154596,
      "acc_norm": 0.3482587064676617,
      "acc_norm_stderr": 0.033687874661154596
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.43,
      "acc_stderr": 0.049756985195624284,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.049756985195624284
    },
    "hendrycksTest-virology": {
      "acc": 0.35542168674698793,
      "acc_stderr": 0.03726214354322415,
      "acc_norm": 0.35542168674698793,
      "acc_norm_stderr": 0.03726214354322415
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3216374269005848,
      "acc_stderr": 0.03582529442573122,
      "acc_norm": 0.3216374269005848,
      "acc_norm_stderr": 0.03582529442573122
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/bloomz_7b',trust_remote_code=True,use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_bloomz_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "15:14:42.180944",
    "model_name": "bloomz_7b1_mt"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.24,
      "acc_stderr": 0.04292346959909281,
      "acc_norm": 0.24,
      "acc_norm_stderr": 0.04292346959909281
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3037037037037037,
      "acc_stderr": 0.039725528847851375,
      "acc_norm": 0.3037037037037037,
      "acc_norm_stderr": 0.039725528847851375
    },
    "hendrycksTest-astronomy": {
      "acc": 0.25,
      "acc_stderr": 0.03523807393012047,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.03523807393012047
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.35094339622641507,
      "acc_stderr": 0.02937364625323469,
      "acc_norm": 0.35094339622641507,
      "acc_norm_stderr": 0.02937364625323469
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3680555555555556,
      "acc_stderr": 0.040329990539607195,
      "acc_norm": 0.3680555555555556,
      "acc_norm_stderr": 0.040329990539607195
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720683,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.34104046242774566,
      "acc_stderr": 0.036146654241808254,
      "acc_norm": 0.34104046242774566,
      "acc_norm_stderr": 0.036146654241808254
    },
    "hendrycksTest-college_physics": {
      "acc": 0.24509803921568626,
      "acc_stderr": 0.04280105837364395,
      "acc_norm": 0.24509803921568626,
      "acc_norm_stderr": 0.04280105837364395
    },
    "hendrycksTest-computer_security": {
      "acc": 0.4,
      "acc_stderr": 0.04923659639173309,
      "acc_norm": 0.4,
      "acc_norm_stderr": 0.04923659639173309
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3872340425531915,
      "acc_stderr": 0.03184389265339525,
      "acc_norm": 0.3872340425531915,
      "acc_norm_stderr": 0.03184389265339525
    },
    "hendrycksTest-econometrics": {
      "acc": 0.23684210526315788,
      "acc_stderr": 0.039994238792813365,
      "acc_norm": 0.23684210526315788,
      "acc_norm_stderr": 0.039994238792813365
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3586206896551724,
      "acc_stderr": 0.03996629574876719,
      "acc_norm": 0.3586206896551724,
      "acc_norm_stderr": 0.03996629574876719
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.291005291005291,
      "acc_stderr": 0.023393826500484865,
      "acc_norm": 0.291005291005291,
      "acc_norm_stderr": 0.023393826500484865
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.24603174603174602,
      "acc_stderr": 0.03852273364924315,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.03852273364924315
    },
    "hendrycksTest-global_facts": {
      "acc": 0.21,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.21,
      "acc_norm_stderr": 0.040936018074033256
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.29354838709677417,
      "acc_stderr": 0.025906087021319295,
      "acc_norm": 0.29354838709677417,
      "acc_norm_stderr": 0.025906087021319295
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3251231527093596,
      "acc_stderr": 0.03295797566311271,
      "acc_norm": 0.3251231527093596,
      "acc_norm_stderr": 0.03295797566311271
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.36,
      "acc_stderr": 0.048241815132442176,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.048241815132442176
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.2545454545454545,
      "acc_stderr": 0.03401506715249039,
      "acc_norm": 0.2545454545454545,
      "acc_norm_stderr": 0.03401506715249039
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.40404040404040403,
      "acc_stderr": 0.03496130972056128,
      "acc_norm": 0.40404040404040403,
      "acc_norm_stderr": 0.03496130972056128
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.31088082901554404,
      "acc_stderr": 0.03340361906276585,
      "acc_norm": 0.31088082901554404,
      "acc_norm_stderr": 0.03340361906276585
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.36153846153846153,
      "acc_stderr": 0.024359581465396976,
      "acc_norm": 0.36153846153846153,
      "acc_norm_stderr": 0.024359581465396976
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.26666666666666666,
      "acc_stderr": 0.026962424325073838,
      "acc_norm": 0.26666666666666666,
      "acc_norm_stderr": 0.026962424325073838
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.36554621848739494,
      "acc_stderr": 0.03128217706368461,
      "acc_norm": 0.36554621848739494,
      "acc_norm_stderr": 0.03128217706368461
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2781456953642384,
      "acc_stderr": 0.03658603262763743,
      "acc_norm": 0.2781456953642384,
      "acc_norm_stderr": 0.03658603262763743
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.3944954128440367,
      "acc_stderr": 0.020954642108587496,
      "acc_norm": 0.3944954128440367,
      "acc_norm_stderr": 0.020954642108587496
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.28703703703703703,
      "acc_stderr": 0.030851992993257013,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.030851992993257013
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.28431372549019607,
      "acc_stderr": 0.03166009679399814,
      "acc_norm": 0.28431372549019607,
      "acc_norm_stderr": 0.03166009679399814
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.3670886075949367,
      "acc_stderr": 0.03137624072561619,
      "acc_norm": 0.3670886075949367,
      "acc_norm_stderr": 0.03137624072561619
    },
    "hendrycksTest-human_aging": {
      "acc": 0.45739910313901344,
      "acc_stderr": 0.033435777055830646,
      "acc_norm": 0.45739910313901344,
      "acc_norm_stderr": 0.033435777055830646
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.45038167938931295,
      "acc_stderr": 0.04363643698524779,
      "acc_norm": 0.45038167938931295,
      "acc_norm_stderr": 0.04363643698524779
    },
    "hendrycksTest-international_law": {
      "acc": 0.30578512396694213,
      "acc_stderr": 0.04205953933884125,
      "acc_norm": 0.30578512396694213,
      "acc_norm_stderr": 0.04205953933884125
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199985,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199985
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.36809815950920244,
      "acc_stderr": 0.03789213935838396,
      "acc_norm": 0.36809815950920244,
      "acc_norm_stderr": 0.03789213935838396
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.36607142857142855,
      "acc_stderr": 0.0457237235873743,
      "acc_norm": 0.36607142857142855,
      "acc_norm_stderr": 0.0457237235873743
    },
    "hendrycksTest-management": {
      "acc": 0.32038834951456313,
      "acc_stderr": 0.0462028408228004,
      "acc_norm": 0.32038834951456313,
      "acc_norm_stderr": 0.0462028408228004
    },
    "hendrycksTest-marketing": {
      "acc": 0.5256410256410257,
      "acc_stderr": 0.03271298896811159,
      "acc_norm": 0.5256410256410257,
      "acc_norm_stderr": 0.03271298896811159
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.46,
      "acc_stderr": 0.05009082659620333,
      "acc_norm": 0.46,
      "acc_norm_stderr": 0.05009082659620333
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.41890166028097064,
      "acc_stderr": 0.01764320505237718,
      "acc_norm": 0.41890166028097064,
      "acc_norm_stderr": 0.01764320505237718
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3988439306358382,
      "acc_stderr": 0.026362437574546545,
      "acc_norm": 0.3988439306358382,
      "acc_norm_stderr": 0.026362437574546545
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.24134078212290502,
      "acc_stderr": 0.014310999547961445,
      "acc_norm": 0.24134078212290502,
      "acc_norm_stderr": 0.014310999547961445
    },
    "hendrycksTest-nutrition": {
      "acc": 0.3006535947712418,
      "acc_stderr": 0.026256053835718968,
      "acc_norm": 0.3006535947712418,
      "acc_norm_stderr": 0.026256053835718968
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3183279742765273,
      "acc_stderr": 0.026457225067811032,
      "acc_norm": 0.3183279742765273,
      "acc_norm_stderr": 0.026457225067811032
    },
    "hendrycksTest-prehistory": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.026041766202717163,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.026041766202717163
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.28368794326241137,
      "acc_stderr": 0.026891709428343957,
      "acc_norm": 0.28368794326241137,
      "acc_norm_stderr": 0.026891709428343957
    },
    "hendrycksTest-professional_law": {
      "acc": 0.24445893089960888,
      "acc_stderr": 0.010976425013113904,
      "acc_norm": 0.24445893089960888,
      "acc_norm_stderr": 0.010976425013113904
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.41911764705882354,
      "acc_stderr": 0.029972807170464622,
      "acc_norm": 0.41911764705882354,
      "acc_norm_stderr": 0.029972807170464622
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.32189542483660133,
      "acc_stderr": 0.018901015322093085,
      "acc_norm": 0.32189542483660133,
      "acc_norm_stderr": 0.018901015322093085
    },
    "hendrycksTest-public_relations": {
      "acc": 0.4909090909090909,
      "acc_stderr": 0.04788339768702861,
      "acc_norm": 0.4909090909090909,
      "acc_norm_stderr": 0.04788339768702861
    },
    "hendrycksTest-security_studies": {
      "acc": 0.24081632653061225,
      "acc_stderr": 0.027372942201788163,
      "acc_norm": 0.24081632653061225,
      "acc_norm_stderr": 0.027372942201788163
    },
    "hendrycksTest-sociology": {
      "acc": 0.3582089552238806,
      "acc_stderr": 0.03390393042268814,
      "acc_norm": 0.3582089552238806,
      "acc_norm_stderr": 0.03390393042268814
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.44,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-virology": {
      "acc": 0.35542168674698793,
      "acc_stderr": 0.03726214354322415,
      "acc_norm": 0.35542168674698793,
      "acc_norm_stderr": 0.03726214354322415
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3742690058479532,
      "acc_stderr": 0.03711601185389481,
      "acc_norm": 0.3742690058479532,
      "acc_norm_stderr": 0.03711601185389481
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/belle_7b_2m,dtype='bfloat16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_belle_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:7",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "4:20:00.004943",
    "model_name": "belle_7b_2m"
  }
}
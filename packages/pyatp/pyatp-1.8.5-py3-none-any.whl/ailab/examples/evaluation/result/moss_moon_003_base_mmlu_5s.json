{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.31,
      "acc_stderr": 0.046482319871173156,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.046482319871173156
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04072314811876837,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04072314811876837
    },
    "hendrycksTest-astronomy": {
      "acc": 0.3157894736842105,
      "acc_stderr": 0.0378272898086547,
      "acc_norm": 0.3157894736842105,
      "acc_norm_stderr": 0.0378272898086547
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3320754716981132,
      "acc_stderr": 0.02898545565233439,
      "acc_norm": 0.3320754716981132,
      "acc_norm_stderr": 0.02898545565233439
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3263888888888889,
      "acc_stderr": 0.03921067198982266,
      "acc_norm": 0.3263888888888889,
      "acc_norm_stderr": 0.03921067198982266
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816507,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816507
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.26011560693641617,
      "acc_stderr": 0.03345036916788992,
      "acc_norm": 0.26011560693641617,
      "acc_norm_stderr": 0.03345036916788992
    },
    "hendrycksTest-college_physics": {
      "acc": 0.19607843137254902,
      "acc_stderr": 0.03950581861179961,
      "acc_norm": 0.19607843137254902,
      "acc_norm_stderr": 0.03950581861179961
    },
    "hendrycksTest-computer_security": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542128,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542128
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3276595744680851,
      "acc_stderr": 0.030683020843231004,
      "acc_norm": 0.3276595744680851,
      "acc_norm_stderr": 0.030683020843231004
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.04142439719489362,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.04142439719489362
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.3310344827586207,
      "acc_stderr": 0.03921545312467122,
      "acc_norm": 0.3310344827586207,
      "acc_norm_stderr": 0.03921545312467122
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.2671957671957672,
      "acc_stderr": 0.02278967314577656,
      "acc_norm": 0.2671957671957672,
      "acc_norm_stderr": 0.02278967314577656
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2619047619047619,
      "acc_stderr": 0.03932537680392871,
      "acc_norm": 0.2619047619047619,
      "acc_norm_stderr": 0.03932537680392871
    },
    "hendrycksTest-global_facts": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.27419354838709675,
      "acc_stderr": 0.0253781399708852,
      "acc_norm": 0.27419354838709675,
      "acc_norm_stderr": 0.0253781399708852
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.2413793103448276,
      "acc_stderr": 0.030108330718011625,
      "acc_norm": 0.2413793103448276,
      "acc_norm_stderr": 0.030108330718011625
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.04512608598542127,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.04512608598542127
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.28484848484848485,
      "acc_stderr": 0.035243908445117836,
      "acc_norm": 0.28484848484848485,
      "acc_norm_stderr": 0.035243908445117836
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3484848484848485,
      "acc_stderr": 0.033948539651564025,
      "acc_norm": 0.3484848484848485,
      "acc_norm_stderr": 0.033948539651564025
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.3316062176165803,
      "acc_stderr": 0.03397636541089116,
      "acc_norm": 0.3316062176165803,
      "acc_norm_stderr": 0.03397636541089116
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.2717948717948718,
      "acc_stderr": 0.022556551010132354,
      "acc_norm": 0.2717948717948718,
      "acc_norm_stderr": 0.022556551010132354
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.02730914058823018,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.02730914058823018
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.31092436974789917,
      "acc_stderr": 0.03006676158297793,
      "acc_norm": 0.31092436974789917,
      "acc_norm_stderr": 0.03006676158297793
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.271523178807947,
      "acc_stderr": 0.036313298039696545,
      "acc_norm": 0.271523178807947,
      "acc_norm_stderr": 0.036313298039696545
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.28440366972477066,
      "acc_stderr": 0.019342036587702588,
      "acc_norm": 0.28440366972477066,
      "acc_norm_stderr": 0.019342036587702588
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2361111111111111,
      "acc_stderr": 0.02896370257079103,
      "acc_norm": 0.2361111111111111,
      "acc_norm_stderr": 0.02896370257079103
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.28921568627450983,
      "acc_stderr": 0.031822318676475544,
      "acc_norm": 0.28921568627450983,
      "acc_norm_stderr": 0.031822318676475544
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.2911392405063291,
      "acc_stderr": 0.02957160106575337,
      "acc_norm": 0.2911392405063291,
      "acc_norm_stderr": 0.02957160106575337
    },
    "hendrycksTest-human_aging": {
      "acc": 0.35874439461883406,
      "acc_stderr": 0.032190792004199956,
      "acc_norm": 0.35874439461883406,
      "acc_norm_stderr": 0.032190792004199956
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3435114503816794,
      "acc_stderr": 0.041649760719448786,
      "acc_norm": 0.3435114503816794,
      "acc_norm_stderr": 0.041649760719448786
    },
    "hendrycksTest-international_law": {
      "acc": 0.4049586776859504,
      "acc_stderr": 0.044811377559424694,
      "acc_norm": 0.4049586776859504,
      "acc_norm_stderr": 0.044811377559424694
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.37962962962962965,
      "acc_stderr": 0.04691521224077742,
      "acc_norm": 0.37962962962962965,
      "acc_norm_stderr": 0.04691521224077742
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.3006134969325153,
      "acc_stderr": 0.0360251131880677,
      "acc_norm": 0.3006134969325153,
      "acc_norm_stderr": 0.0360251131880677
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.3482142857142857,
      "acc_stderr": 0.045218299028335865,
      "acc_norm": 0.3482142857142857,
      "acc_norm_stderr": 0.045218299028335865
    },
    "hendrycksTest-management": {
      "acc": 0.3300970873786408,
      "acc_stderr": 0.04656147110012352,
      "acc_norm": 0.3300970873786408,
      "acc_norm_stderr": 0.04656147110012352
    },
    "hendrycksTest-marketing": {
      "acc": 0.3717948717948718,
      "acc_stderr": 0.031660988918880785,
      "acc_norm": 0.3717948717948718,
      "acc_norm_stderr": 0.031660988918880785
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.30268199233716475,
      "acc_stderr": 0.016428781581749367,
      "acc_norm": 0.30268199233716475,
      "acc_norm_stderr": 0.016428781581749367
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.30346820809248554,
      "acc_stderr": 0.024752411960917212,
      "acc_norm": 0.30346820809248554,
      "acc_norm_stderr": 0.024752411960917212
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-nutrition": {
      "acc": 0.34967320261437906,
      "acc_stderr": 0.027305308076274695,
      "acc_norm": 0.34967320261437906,
      "acc_norm_stderr": 0.027305308076274695
    },
    "hendrycksTest-philosophy": {
      "acc": 0.33762057877813506,
      "acc_stderr": 0.02685882587948855,
      "acc_norm": 0.33762057877813506,
      "acc_norm_stderr": 0.02685882587948855
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3117283950617284,
      "acc_stderr": 0.02577311116963046,
      "acc_norm": 0.3117283950617284,
      "acc_norm_stderr": 0.02577311116963046
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2765957446808511,
      "acc_stderr": 0.02668456434046099,
      "acc_norm": 0.2765957446808511,
      "acc_norm_stderr": 0.02668456434046099
    },
    "hendrycksTest-professional_law": {
      "acc": 0.3011734028683181,
      "acc_stderr": 0.011717148751648435,
      "acc_norm": 0.3011734028683181,
      "acc_norm_stderr": 0.011717148751648435
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.027678468642144693,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.027678468642144693
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.31862745098039214,
      "acc_stderr": 0.01885008469646871,
      "acc_norm": 0.31862745098039214,
      "acc_norm_stderr": 0.01885008469646871
    },
    "hendrycksTest-public_relations": {
      "acc": 0.3181818181818182,
      "acc_stderr": 0.04461272175910508,
      "acc_norm": 0.3181818181818182,
      "acc_norm_stderr": 0.04461272175910508
    },
    "hendrycksTest-security_studies": {
      "acc": 0.3306122448979592,
      "acc_stderr": 0.030116426296540596,
      "acc_norm": 0.3306122448979592,
      "acc_norm_stderr": 0.030116426296540596
    },
    "hendrycksTest-sociology": {
      "acc": 0.31840796019900497,
      "acc_stderr": 0.03294118479054095,
      "acc_norm": 0.31840796019900497,
      "acc_norm_stderr": 0.03294118479054095
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.36,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.36,
      "acc_norm_stderr": 0.04824181513244218
    },
    "hendrycksTest-virology": {
      "acc": 0.3072289156626506,
      "acc_stderr": 0.03591566797824663,
      "acc_norm": 0.3072289156626506,
      "acc_norm_stderr": 0.03591566797824663
    },
    "hendrycksTest-world_religions": {
      "acc": 0.3684210526315789,
      "acc_stderr": 0.03699658017656878,
      "acc_norm": 0.3684210526315789,
      "acc_norm_stderr": 0.03699658017656878
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/moss-moon-003-base',trust_remote_code=True,use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "5:09:59.718969",
    "model_name": "moss_moon_003_base"
  }
}
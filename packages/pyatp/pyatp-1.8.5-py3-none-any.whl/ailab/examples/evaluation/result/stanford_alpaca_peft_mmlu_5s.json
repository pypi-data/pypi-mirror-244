{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720684,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720684
    },
    "hendrycksTest-anatomy": {
      "acc": 0.3925925925925926,
      "acc_stderr": 0.04218506215368879,
      "acc_norm": 0.3925925925925926,
      "acc_norm_stderr": 0.04218506215368879
    },
    "hendrycksTest-astronomy": {
      "acc": 0.32894736842105265,
      "acc_stderr": 0.03823428969926604,
      "acc_norm": 0.32894736842105265,
      "acc_norm_stderr": 0.03823428969926604
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145632,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.2981132075471698,
      "acc_stderr": 0.02815283794249387,
      "acc_norm": 0.2981132075471698,
      "acc_norm_stderr": 0.02815283794249387
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3402777777777778,
      "acc_stderr": 0.039621355734862175,
      "acc_norm": 0.3402777777777778,
      "acc_norm_stderr": 0.039621355734862175
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145632,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145632
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2774566473988439,
      "acc_stderr": 0.03414014007044036,
      "acc_norm": 0.2774566473988439,
      "acc_norm_stderr": 0.03414014007044036
    },
    "hendrycksTest-college_physics": {
      "acc": 0.2549019607843137,
      "acc_stderr": 0.043364327079931785,
      "acc_norm": 0.2549019607843137,
      "acc_norm_stderr": 0.043364327079931785
    },
    "hendrycksTest-computer_security": {
      "acc": 0.43,
      "acc_stderr": 0.04975698519562427,
      "acc_norm": 0.43,
      "acc_norm_stderr": 0.04975698519562427
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3574468085106383,
      "acc_stderr": 0.03132941789476425,
      "acc_norm": 0.3574468085106383,
      "acc_norm_stderr": 0.03132941789476425
    },
    "hendrycksTest-econometrics": {
      "acc": 0.24561403508771928,
      "acc_stderr": 0.04049339297748142,
      "acc_norm": 0.24561403508771928,
      "acc_norm_stderr": 0.04049339297748142
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.2206896551724138,
      "acc_stderr": 0.034559302019248124,
      "acc_norm": 0.2206896551724138,
      "acc_norm_stderr": 0.034559302019248124
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.24074074074074073,
      "acc_stderr": 0.0220190800122179,
      "acc_norm": 0.24074074074074073,
      "acc_norm_stderr": 0.0220190800122179
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.23015873015873015,
      "acc_stderr": 0.03764950879790606,
      "acc_norm": 0.23015873015873015,
      "acc_norm_stderr": 0.03764950879790606
    },
    "hendrycksTest-global_facts": {
      "acc": 0.3,
      "acc_stderr": 0.046056618647183814,
      "acc_norm": 0.3,
      "acc_norm_stderr": 0.046056618647183814
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.33548387096774196,
      "acc_stderr": 0.02686020644472435,
      "acc_norm": 0.33548387096774196,
      "acc_norm_stderr": 0.02686020644472435
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.27586206896551724,
      "acc_stderr": 0.031447125816782426,
      "acc_norm": 0.27586206896551724,
      "acc_norm_stderr": 0.031447125816782426
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.42424242424242425,
      "acc_stderr": 0.038592681420702615,
      "acc_norm": 0.42424242424242425,
      "acc_norm_stderr": 0.038592681420702615
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.3434343434343434,
      "acc_stderr": 0.03383201223244441,
      "acc_norm": 0.3434343434343434,
      "acc_norm_stderr": 0.03383201223244441
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.40414507772020725,
      "acc_stderr": 0.035415085788840193,
      "acc_norm": 0.40414507772020725,
      "acc_norm_stderr": 0.035415085788840193
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.33076923076923076,
      "acc_stderr": 0.02385479568097114,
      "acc_norm": 0.33076923076923076,
      "acc_norm_stderr": 0.02385479568097114
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.25555555555555554,
      "acc_stderr": 0.026593939101844086,
      "acc_norm": 0.25555555555555554,
      "acc_norm_stderr": 0.026593939101844086
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.31932773109243695,
      "acc_stderr": 0.030283995525884396,
      "acc_norm": 0.31932773109243695,
      "acc_norm_stderr": 0.030283995525884396
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2781456953642384,
      "acc_stderr": 0.03658603262763743,
      "acc_norm": 0.2781456953642384,
      "acc_norm_stderr": 0.03658603262763743
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.42385321100917434,
      "acc_stderr": 0.021187263209087516,
      "acc_norm": 0.42385321100917434,
      "acc_norm_stderr": 0.021187263209087516
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.32407407407407407,
      "acc_stderr": 0.03191923445686186,
      "acc_norm": 0.32407407407407407,
      "acc_norm_stderr": 0.03191923445686186
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.3480392156862745,
      "acc_stderr": 0.03343311240488419,
      "acc_norm": 0.3480392156862745,
      "acc_norm_stderr": 0.03343311240488419
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.4219409282700422,
      "acc_stderr": 0.032148146302403695,
      "acc_norm": 0.4219409282700422,
      "acc_norm_stderr": 0.032148146302403695
    },
    "hendrycksTest-human_aging": {
      "acc": 0.4349775784753363,
      "acc_stderr": 0.033272833702713445,
      "acc_norm": 0.4349775784753363,
      "acc_norm_stderr": 0.033272833702713445
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.3435114503816794,
      "acc_stderr": 0.041649760719448786,
      "acc_norm": 0.3435114503816794,
      "acc_norm_stderr": 0.041649760719448786
    },
    "hendrycksTest-international_law": {
      "acc": 0.5289256198347108,
      "acc_stderr": 0.04556710331269498,
      "acc_norm": 0.5289256198347108,
      "acc_norm_stderr": 0.04556710331269498
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.04750077341199987,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.04750077341199987
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.38650306748466257,
      "acc_stderr": 0.038258255488486076,
      "acc_norm": 0.38650306748466257,
      "acc_norm_stderr": 0.038258255488486076
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.23214285714285715,
      "acc_stderr": 0.04007341809755804,
      "acc_norm": 0.23214285714285715,
      "acc_norm_stderr": 0.04007341809755804
    },
    "hendrycksTest-management": {
      "acc": 0.32038834951456313,
      "acc_stderr": 0.0462028408228004,
      "acc_norm": 0.32038834951456313,
      "acc_norm_stderr": 0.0462028408228004
    },
    "hendrycksTest-marketing": {
      "acc": 0.4658119658119658,
      "acc_stderr": 0.03267942734081228,
      "acc_norm": 0.4658119658119658,
      "acc_norm_stderr": 0.03267942734081228
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.4074074074074074,
      "acc_stderr": 0.017570705239256544,
      "acc_norm": 0.4074074074074074,
      "acc_norm_stderr": 0.017570705239256544
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.38439306358381503,
      "acc_stderr": 0.026189666966272035,
      "acc_norm": 0.38439306358381503,
      "acc_norm_stderr": 0.026189666966272035
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-nutrition": {
      "acc": 0.39869281045751637,
      "acc_stderr": 0.02803609227389177,
      "acc_norm": 0.39869281045751637,
      "acc_norm_stderr": 0.02803609227389177
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3890675241157556,
      "acc_stderr": 0.027690337536485372,
      "acc_norm": 0.3890675241157556,
      "acc_norm_stderr": 0.027690337536485372
    },
    "hendrycksTest-prehistory": {
      "acc": 0.3395061728395062,
      "acc_stderr": 0.026348564412011635,
      "acc_norm": 0.3395061728395062,
      "acc_norm_stderr": 0.026348564412011635
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.2872340425531915,
      "acc_stderr": 0.026992199173064356,
      "acc_norm": 0.2872340425531915,
      "acc_norm_stderr": 0.026992199173064356
    },
    "hendrycksTest-professional_law": {
      "acc": 0.29139504563233376,
      "acc_stderr": 0.0116057202142576,
      "acc_norm": 0.29139504563233376,
      "acc_norm_stderr": 0.0116057202142576
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4375,
      "acc_stderr": 0.030134614954403924,
      "acc_norm": 0.4375,
      "acc_norm_stderr": 0.030134614954403924
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.3627450980392157,
      "acc_stderr": 0.019450768432505514,
      "acc_norm": 0.3627450980392157,
      "acc_norm_stderr": 0.019450768432505514
    },
    "hendrycksTest-public_relations": {
      "acc": 0.39090909090909093,
      "acc_stderr": 0.046737523336702363,
      "acc_norm": 0.39090909090909093,
      "acc_norm_stderr": 0.046737523336702363
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2979591836734694,
      "acc_stderr": 0.029279567411065677,
      "acc_norm": 0.2979591836734694,
      "acc_norm_stderr": 0.029279567411065677
    },
    "hendrycksTest-sociology": {
      "acc": 0.43781094527363185,
      "acc_stderr": 0.0350808011219984,
      "acc_norm": 0.43781094527363185,
      "acc_norm_stderr": 0.0350808011219984
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.44,
      "acc_stderr": 0.04988876515698589,
      "acc_norm": 0.44,
      "acc_norm_stderr": 0.04988876515698589
    },
    "hendrycksTest-virology": {
      "acc": 0.3433734939759036,
      "acc_stderr": 0.036965843170106004,
      "acc_norm": 0.3433734939759036,
      "acc_norm_stderr": 0.036965843170106004
    },
    "hendrycksTest-world_religions": {
      "acc": 0.4502923976608187,
      "acc_stderr": 0.03815827365913235,
      "acc_norm": 0.4502923976608187,
      "acc_norm_stderr": 0.03815827365913235
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/home/sdk_models/llama-7b-hf',load_in_8bit=True,dtype='float16',tokenizer='/data1/cgzhang6/tokenizer/llama-7b-hf_tokenizer',use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_standford_alpaca_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:5",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "3:03:22.638746",
    "model_name": "stanford_alpaca"
  }
}
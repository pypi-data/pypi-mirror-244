{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-anatomy": {
      "acc": 0.5037037037037037,
      "acc_stderr": 0.04319223625811331,
      "acc_norm": 0.5037037037037037,
      "acc_norm_stderr": 0.04319223625811331
    },
    "hendrycksTest-astronomy": {
      "acc": 0.5460526315789473,
      "acc_stderr": 0.04051646342874142,
      "acc_norm": 0.5460526315789473,
      "acc_norm_stderr": 0.04051646342874142
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.57,
      "acc_norm_stderr": 0.04975698519562428
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5811320754716981,
      "acc_stderr": 0.030365050829115208,
      "acc_norm": 0.5811320754716981,
      "acc_norm_stderr": 0.030365050829115208
    },
    "hendrycksTest-college_biology": {
      "acc": 0.5625,
      "acc_stderr": 0.04148415739394154,
      "acc_norm": 0.5625,
      "acc_norm_stderr": 0.04148415739394154
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145633
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.5028901734104047,
      "acc_stderr": 0.038124005659748335,
      "acc_norm": 0.5028901734104047,
      "acc_norm_stderr": 0.038124005659748335
    },
    "hendrycksTest-college_physics": {
      "acc": 0.21568627450980393,
      "acc_stderr": 0.04092563958237654,
      "acc_norm": 0.21568627450980393,
      "acc_norm_stderr": 0.04092563958237654
    },
    "hendrycksTest-computer_security": {
      "acc": 0.67,
      "acc_stderr": 0.04725815626252609,
      "acc_norm": 0.67,
      "acc_norm_stderr": 0.04725815626252609
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.451063829787234,
      "acc_stderr": 0.032529096196131965,
      "acc_norm": 0.451063829787234,
      "acc_norm_stderr": 0.032529096196131965
    },
    "hendrycksTest-econometrics": {
      "acc": 0.37719298245614036,
      "acc_stderr": 0.04559522141958216,
      "acc_norm": 0.37719298245614036,
      "acc_norm_stderr": 0.04559522141958216
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.503448275862069,
      "acc_stderr": 0.041665675771015785,
      "acc_norm": 0.503448275862069,
      "acc_norm_stderr": 0.041665675771015785
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.31216931216931215,
      "acc_stderr": 0.0238652068369726,
      "acc_norm": 0.31216931216931215,
      "acc_norm_stderr": 0.0238652068369726
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.24603174603174602,
      "acc_stderr": 0.03852273364924314,
      "acc_norm": 0.24603174603174602,
      "acc_norm_stderr": 0.03852273364924314
    },
    "hendrycksTest-global_facts": {
      "acc": 0.38,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.38,
      "acc_norm_stderr": 0.04878317312145633
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.6193548387096774,
      "acc_stderr": 0.027621717832907036,
      "acc_norm": 0.6193548387096774,
      "acc_norm_stderr": 0.027621717832907036
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3842364532019704,
      "acc_stderr": 0.0342239856565755,
      "acc_norm": 0.3842364532019704,
      "acc_norm_stderr": 0.0342239856565755
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562427,
      "acc_norm": 0.57,
      "acc_norm_stderr": 0.04975698519562427
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6545454545454545,
      "acc_stderr": 0.03713158067481913,
      "acc_norm": 0.6545454545454545,
      "acc_norm_stderr": 0.03713158067481913
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.6717171717171717,
      "acc_stderr": 0.03345678422756776,
      "acc_norm": 0.6717171717171717,
      "acc_norm_stderr": 0.03345678422756776
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7046632124352331,
      "acc_stderr": 0.03292296639155141,
      "acc_norm": 0.7046632124352331,
      "acc_norm_stderr": 0.03292296639155141
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.5128205128205128,
      "acc_stderr": 0.02534267129380725,
      "acc_norm": 0.5128205128205128,
      "acc_norm_stderr": 0.02534267129380725
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.23703703703703705,
      "acc_stderr": 0.02592887613276611,
      "acc_norm": 0.23703703703703705,
      "acc_norm_stderr": 0.02592887613276611
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.5672268907563025,
      "acc_stderr": 0.03218358107742613,
      "acc_norm": 0.5672268907563025,
      "acc_norm_stderr": 0.03218358107742613
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2980132450331126,
      "acc_stderr": 0.037345356767871984,
      "acc_norm": 0.2980132450331126,
      "acc_norm_stderr": 0.037345356767871984
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.7045871559633028,
      "acc_stderr": 0.019560619182976,
      "acc_norm": 0.7045871559633028,
      "acc_norm_stderr": 0.019560619182976
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3888888888888889,
      "acc_stderr": 0.033247089118091176,
      "acc_norm": 0.3888888888888889,
      "acc_norm_stderr": 0.033247089118091176
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6911764705882353,
      "acc_stderr": 0.03242661719827218,
      "acc_norm": 0.6911764705882353,
      "acc_norm_stderr": 0.03242661719827218
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.7257383966244726,
      "acc_stderr": 0.029041333510598035,
      "acc_norm": 0.7257383966244726,
      "acc_norm_stderr": 0.029041333510598035
    },
    "hendrycksTest-human_aging": {
      "acc": 0.6233183856502242,
      "acc_stderr": 0.032521134899291884,
      "acc_norm": 0.6233183856502242,
      "acc_norm_stderr": 0.032521134899291884
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.6183206106870229,
      "acc_stderr": 0.0426073515764456,
      "acc_norm": 0.6183206106870229,
      "acc_norm_stderr": 0.0426073515764456
    },
    "hendrycksTest-international_law": {
      "acc": 0.6942148760330579,
      "acc_stderr": 0.042059539338841226,
      "acc_norm": 0.6942148760330579,
      "acc_norm_stderr": 0.042059539338841226
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.6759259259259259,
      "acc_stderr": 0.045245960070300476,
      "acc_norm": 0.6759259259259259,
      "acc_norm_stderr": 0.045245960070300476
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.6441717791411042,
      "acc_stderr": 0.03761521380046734,
      "acc_norm": 0.6441717791411042,
      "acc_norm_stderr": 0.03761521380046734
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.39285714285714285,
      "acc_stderr": 0.04635550135609976,
      "acc_norm": 0.39285714285714285,
      "acc_norm_stderr": 0.04635550135609976
    },
    "hendrycksTest-management": {
      "acc": 0.6893203883495146,
      "acc_stderr": 0.045821241601615506,
      "acc_norm": 0.6893203883495146,
      "acc_norm_stderr": 0.045821241601615506
    },
    "hendrycksTest-marketing": {
      "acc": 0.8034188034188035,
      "acc_stderr": 0.02603538609895129,
      "acc_norm": 0.8034188034188035,
      "acc_norm_stderr": 0.02603538609895129
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.61,
      "acc_stderr": 0.04902071300001974,
      "acc_norm": 0.61,
      "acc_norm_stderr": 0.04902071300001974
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.722860791826309,
      "acc_stderr": 0.01600563629412243,
      "acc_norm": 0.722860791826309,
      "acc_norm_stderr": 0.01600563629412243
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5722543352601156,
      "acc_stderr": 0.026636539741116072,
      "acc_norm": 0.5722543352601156,
      "acc_norm_stderr": 0.026636539741116072
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.26256983240223464,
      "acc_stderr": 0.014716824273017763,
      "acc_norm": 0.26256983240223464,
      "acc_norm_stderr": 0.014716824273017763
    },
    "hendrycksTest-nutrition": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.027914055510468,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.027914055510468
    },
    "hendrycksTest-philosophy": {
      "acc": 0.6366559485530546,
      "acc_stderr": 0.027316847674192703,
      "acc_norm": 0.6366559485530546,
      "acc_norm_stderr": 0.027316847674192703
    },
    "hendrycksTest-prehistory": {
      "acc": 0.6481481481481481,
      "acc_stderr": 0.02657148348071997,
      "acc_norm": 0.6481481481481481,
      "acc_norm_stderr": 0.02657148348071997
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.40425531914893614,
      "acc_stderr": 0.029275532159704725,
      "acc_norm": 0.40425531914893614,
      "acc_norm_stderr": 0.029275532159704725
    },
    "hendrycksTest-professional_law": {
      "acc": 0.4061277705345502,
      "acc_stderr": 0.012543154588412937,
      "acc_norm": 0.4061277705345502,
      "acc_norm_stderr": 0.012543154588412937
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.4889705882352941,
      "acc_stderr": 0.030365446477275675,
      "acc_norm": 0.4889705882352941,
      "acc_norm_stderr": 0.030365446477275675
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.5277777777777778,
      "acc_stderr": 0.020196594933541197,
      "acc_norm": 0.5277777777777778,
      "acc_norm_stderr": 0.020196594933541197
    },
    "hendrycksTest-public_relations": {
      "acc": 0.6454545454545455,
      "acc_stderr": 0.04582004841505417,
      "acc_norm": 0.6454545454545455,
      "acc_norm_stderr": 0.04582004841505417
    },
    "hendrycksTest-security_studies": {
      "acc": 0.563265306122449,
      "acc_stderr": 0.031751952375833226,
      "acc_norm": 0.563265306122449,
      "acc_norm_stderr": 0.031751952375833226
    },
    "hendrycksTest-sociology": {
      "acc": 0.7711442786069652,
      "acc_stderr": 0.029705284056772432,
      "acc_norm": 0.7711442786069652,
      "acc_norm_stderr": 0.029705284056772432
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.74,
      "acc_stderr": 0.0440844002276808,
      "acc_norm": 0.74,
      "acc_norm_stderr": 0.0440844002276808
    },
    "hendrycksTest-virology": {
      "acc": 0.4759036144578313,
      "acc_stderr": 0.03887971849597264,
      "acc_norm": 0.4759036144578313,
      "acc_norm_stderr": 0.03887971849597264
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7543859649122807,
      "acc_stderr": 0.03301405946987251,
      "acc_norm": 0.7543859649122807,
      "acc_norm_stderr": 0.03301405946987251
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/xverse_13b,dtype='bfloat16',trust_remote_code=True,use_accelerate=False,peft=/home/finetuned_models/my_xverse_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:3",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "5:58:24.901837",
    "model_name": "xverse_13b"
  }
}
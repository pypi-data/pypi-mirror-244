{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.23,
      "acc_stderr": 0.04229525846816503,
      "acc_norm": 0.23,
      "acc_norm_stderr": 0.04229525846816503
    },
    "hendrycksTest-anatomy": {
      "acc": 0.31851851851851853,
      "acc_stderr": 0.040247784019771096,
      "acc_norm": 0.31851851851851853,
      "acc_norm_stderr": 0.040247784019771096
    },
    "hendrycksTest-astronomy": {
      "acc": 0.21052631578947367,
      "acc_stderr": 0.03317672787533156,
      "acc_norm": 0.21052631578947367,
      "acc_norm_stderr": 0.03317672787533156
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.28,
      "acc_stderr": 0.045126085985421276,
      "acc_norm": 0.28,
      "acc_norm_stderr": 0.045126085985421276
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.3283018867924528,
      "acc_stderr": 0.02890159361241178,
      "acc_norm": 0.3283018867924528,
      "acc_norm_stderr": 0.02890159361241178
    },
    "hendrycksTest-college_biology": {
      "acc": 0.3055555555555556,
      "acc_stderr": 0.03852084696008534,
      "acc_norm": 0.3055555555555556,
      "acc_norm_stderr": 0.03852084696008534
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.27,
      "acc_stderr": 0.04461960433384741,
      "acc_norm": 0.27,
      "acc_norm_stderr": 0.04461960433384741
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.22,
      "acc_stderr": 0.04163331998932269,
      "acc_norm": 0.22,
      "acc_norm_stderr": 0.04163331998932269
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.34,
      "acc_stderr": 0.047609522856952344,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.047609522856952344
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.2658959537572254,
      "acc_stderr": 0.033687629322594316,
      "acc_norm": 0.2658959537572254,
      "acc_norm_stderr": 0.033687629322594316
    },
    "hendrycksTest-college_physics": {
      "acc": 0.20588235294117646,
      "acc_stderr": 0.04023382273617749,
      "acc_norm": 0.20588235294117646,
      "acc_norm_stderr": 0.04023382273617749
    },
    "hendrycksTest-computer_security": {
      "acc": 0.31,
      "acc_stderr": 0.04648231987117316,
      "acc_norm": 0.31,
      "acc_norm_stderr": 0.04648231987117316
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.32340425531914896,
      "acc_stderr": 0.030579442773610337,
      "acc_norm": 0.32340425531914896,
      "acc_norm_stderr": 0.030579442773610337
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.04142439719489364,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.04142439719489364
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.30344827586206896,
      "acc_stderr": 0.038312260488503336,
      "acc_norm": 0.30344827586206896,
      "acc_norm_stderr": 0.038312260488503336
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30952380952380953,
      "acc_stderr": 0.023809523809523864,
      "acc_norm": 0.30952380952380953,
      "acc_norm_stderr": 0.023809523809523864
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.2222222222222222,
      "acc_stderr": 0.03718489006818115,
      "acc_norm": 0.2222222222222222,
      "acc_norm_stderr": 0.03718489006818115
    },
    "hendrycksTest-global_facts": {
      "acc": 0.35,
      "acc_stderr": 0.047937248544110196,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.047937248544110196
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.27419354838709675,
      "acc_stderr": 0.025378139970885203,
      "acc_norm": 0.27419354838709675,
      "acc_norm_stderr": 0.025378139970885203
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.28078817733990147,
      "acc_stderr": 0.031618563353586086,
      "acc_norm": 0.28078817733990147,
      "acc_norm_stderr": 0.031618563353586086
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.35,
      "acc_stderr": 0.0479372485441102,
      "acc_norm": 0.35,
      "acc_norm_stderr": 0.0479372485441102
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.03756335775187896,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.03756335775187896
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.36363636363636365,
      "acc_stderr": 0.034273086529999365,
      "acc_norm": 0.36363636363636365,
      "acc_norm_stderr": 0.034273086529999365
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.32642487046632124,
      "acc_stderr": 0.033840286211432945,
      "acc_norm": 0.32642487046632124,
      "acc_norm_stderr": 0.033840286211432945
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.3282051282051282,
      "acc_stderr": 0.02380763319865727,
      "acc_norm": 0.3282051282051282,
      "acc_norm_stderr": 0.02380763319865727
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.2740740740740741,
      "acc_stderr": 0.027195934804085626,
      "acc_norm": 0.2740740740740741,
      "acc_norm_stderr": 0.027195934804085626
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.33613445378151263,
      "acc_stderr": 0.030684737115135356,
      "acc_norm": 0.33613445378151263,
      "acc_norm_stderr": 0.030684737115135356
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.2913907284768212,
      "acc_stderr": 0.03710185726119996,
      "acc_norm": 0.2913907284768212,
      "acc_norm_stderr": 0.03710185726119996
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.29357798165137616,
      "acc_stderr": 0.019525151122639667,
      "acc_norm": 0.29357798165137616,
      "acc_norm_stderr": 0.019525151122639667
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.2777777777777778,
      "acc_stderr": 0.0305467452649532,
      "acc_norm": 0.2777777777777778,
      "acc_norm_stderr": 0.0305467452649532
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.30392156862745096,
      "acc_stderr": 0.03228210387037892,
      "acc_norm": 0.30392156862745096,
      "acc_norm_stderr": 0.03228210387037892
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.34177215189873417,
      "acc_stderr": 0.030874537537553617,
      "acc_norm": 0.34177215189873417,
      "acc_norm_stderr": 0.030874537537553617
    },
    "hendrycksTest-human_aging": {
      "acc": 0.3901345291479821,
      "acc_stderr": 0.03273766725459157,
      "acc_norm": 0.3901345291479821,
      "acc_norm_stderr": 0.03273766725459157
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.2824427480916031,
      "acc_stderr": 0.03948406125768361,
      "acc_norm": 0.2824427480916031,
      "acc_norm_stderr": 0.03948406125768361
    },
    "hendrycksTest-international_law": {
      "acc": 0.4132231404958678,
      "acc_stderr": 0.04495087843548408,
      "acc_norm": 0.4132231404958678,
      "acc_norm_stderr": 0.04495087843548408
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.28703703703703703,
      "acc_stderr": 0.043733130409147614,
      "acc_norm": 0.28703703703703703,
      "acc_norm_stderr": 0.043733130409147614
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.27607361963190186,
      "acc_stderr": 0.0351238528370505,
      "acc_norm": 0.27607361963190186,
      "acc_norm_stderr": 0.0351238528370505
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.25,
      "acc_stderr": 0.04109974682633932,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04109974682633932
    },
    "hendrycksTest-management": {
      "acc": 0.2621359223300971,
      "acc_stderr": 0.04354631077260597,
      "acc_norm": 0.2621359223300971,
      "acc_norm_stderr": 0.04354631077260597
    },
    "hendrycksTest-marketing": {
      "acc": 0.34615384615384615,
      "acc_stderr": 0.0311669573672359,
      "acc_norm": 0.34615384615384615,
      "acc_norm_stderr": 0.0311669573672359
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.25,
      "acc_stderr": 0.04351941398892446,
      "acc_norm": 0.25,
      "acc_norm_stderr": 0.04351941398892446
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.3065134099616858,
      "acc_stderr": 0.016486952893041515,
      "acc_norm": 0.3065134099616858,
      "acc_norm_stderr": 0.016486952893041515
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.3063583815028902,
      "acc_stderr": 0.024818350129436593,
      "acc_norm": 0.3063583815028902,
      "acc_norm_stderr": 0.024818350129436593
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.2424581005586592,
      "acc_stderr": 0.014333522059217889,
      "acc_norm": 0.2424581005586592,
      "acc_norm_stderr": 0.014333522059217889
    },
    "hendrycksTest-nutrition": {
      "acc": 0.31699346405228757,
      "acc_stderr": 0.026643278474508748,
      "acc_norm": 0.31699346405228757,
      "acc_norm_stderr": 0.026643278474508748
    },
    "hendrycksTest-philosophy": {
      "acc": 0.3086816720257235,
      "acc_stderr": 0.026236965881153262,
      "acc_norm": 0.3086816720257235,
      "acc_norm_stderr": 0.026236965881153262
    },
    "hendrycksTest-prehistory": {
      "acc": 0.2932098765432099,
      "acc_stderr": 0.025329888171900926,
      "acc_norm": 0.2932098765432099,
      "acc_norm_stderr": 0.025329888171900926
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.29432624113475175,
      "acc_stderr": 0.027187127011503803,
      "acc_norm": 0.29432624113475175,
      "acc_norm_stderr": 0.027187127011503803
    },
    "hendrycksTest-professional_law": {
      "acc": 0.2966101694915254,
      "acc_stderr": 0.011665946586082854,
      "acc_norm": 0.2966101694915254,
      "acc_norm_stderr": 0.011665946586082854
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.20955882352941177,
      "acc_stderr": 0.02472311040767705,
      "acc_norm": 0.20955882352941177,
      "acc_norm_stderr": 0.02472311040767705
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.29248366013071897,
      "acc_stderr": 0.018403415710109797,
      "acc_norm": 0.29248366013071897,
      "acc_norm_stderr": 0.018403415710109797
    },
    "hendrycksTest-public_relations": {
      "acc": 0.38181818181818183,
      "acc_stderr": 0.046534298079135075,
      "acc_norm": 0.38181818181818183,
      "acc_norm_stderr": 0.046534298079135075
    },
    "hendrycksTest-security_studies": {
      "acc": 0.2816326530612245,
      "acc_stderr": 0.028795185574291296,
      "acc_norm": 0.2816326530612245,
      "acc_norm_stderr": 0.028795185574291296
    },
    "hendrycksTest-sociology": {
      "acc": 0.31343283582089554,
      "acc_stderr": 0.03280188205348643,
      "acc_norm": 0.31343283582089554,
      "acc_norm_stderr": 0.03280188205348643
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.32,
      "acc_stderr": 0.04688261722621505,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.04688261722621505
    },
    "hendrycksTest-virology": {
      "acc": 0.3253012048192771,
      "acc_stderr": 0.03647168523683226,
      "acc_norm": 0.3253012048192771,
      "acc_norm_stderr": 0.03647168523683226
    },
    "hendrycksTest-world_religions": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.033773102522091945,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.033773102522091945
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained='/data1/cgzhang6/models/moss-moon-003-base',trust_remote_code=True,use_accelerate=False,peft=/data1/cgzhang6/finetuned_models/my_moss_model",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:6",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "5:17:21.813916",
    "model_name": "moss_moon_003_base"
  }
}
{
  "results": {
    "hendrycksTest-abstract_algebra": {
      "acc": 0.37,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.04852365870939099
    },
    "hendrycksTest-anatomy": {
      "acc": 0.5111111111111111,
      "acc_stderr": 0.04318275491977976,
      "acc_norm": 0.5111111111111111,
      "acc_norm_stderr": 0.04318275491977976
    },
    "hendrycksTest-astronomy": {
      "acc": 0.48026315789473684,
      "acc_stderr": 0.04065771002562603,
      "acc_norm": 0.48026315789473684,
      "acc_norm_stderr": 0.04065771002562603
    },
    "hendrycksTest-business_ethics": {
      "acc": 0.53,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "hendrycksTest-clinical_knowledge": {
      "acc": 0.5132075471698113,
      "acc_stderr": 0.030762134874500476,
      "acc_norm": 0.5132075471698113,
      "acc_norm_stderr": 0.030762134874500476
    },
    "hendrycksTest-college_biology": {
      "acc": 0.4861111111111111,
      "acc_stderr": 0.04179596617581,
      "acc_norm": 0.4861111111111111,
      "acc_norm_stderr": 0.04179596617581
    },
    "hendrycksTest-college_chemistry": {
      "acc": 0.33,
      "acc_stderr": 0.047258156262526045,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.047258156262526045
    },
    "hendrycksTest-college_computer_science": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001975,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001975
    },
    "hendrycksTest-college_mathematics": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "hendrycksTest-college_medicine": {
      "acc": 0.3930635838150289,
      "acc_stderr": 0.037242495958177295,
      "acc_norm": 0.3930635838150289,
      "acc_norm_stderr": 0.037242495958177295
    },
    "hendrycksTest-college_physics": {
      "acc": 0.3137254901960784,
      "acc_stderr": 0.04617034827006716,
      "acc_norm": 0.3137254901960784,
      "acc_norm_stderr": 0.04617034827006716
    },
    "hendrycksTest-computer_security": {
      "acc": 0.6,
      "acc_stderr": 0.049236596391733084,
      "acc_norm": 0.6,
      "acc_norm_stderr": 0.049236596391733084
    },
    "hendrycksTest-conceptual_physics": {
      "acc": 0.3191489361702128,
      "acc_stderr": 0.03047297336338005,
      "acc_norm": 0.3191489361702128,
      "acc_norm_stderr": 0.03047297336338005
    },
    "hendrycksTest-econometrics": {
      "acc": 0.2719298245614035,
      "acc_stderr": 0.04185774424022056,
      "acc_norm": 0.2719298245614035,
      "acc_norm_stderr": 0.04185774424022056
    },
    "hendrycksTest-electrical_engineering": {
      "acc": 0.4413793103448276,
      "acc_stderr": 0.04137931034482758,
      "acc_norm": 0.4413793103448276,
      "acc_norm_stderr": 0.04137931034482758
    },
    "hendrycksTest-elementary_mathematics": {
      "acc": 0.30423280423280424,
      "acc_stderr": 0.023695415009463087,
      "acc_norm": 0.30423280423280424,
      "acc_norm_stderr": 0.023695415009463087
    },
    "hendrycksTest-formal_logic": {
      "acc": 0.373015873015873,
      "acc_stderr": 0.04325506042017086,
      "acc_norm": 0.373015873015873,
      "acc_norm_stderr": 0.04325506042017086
    },
    "hendrycksTest-global_facts": {
      "acc": 0.33,
      "acc_stderr": 0.04725815626252604,
      "acc_norm": 0.33,
      "acc_norm_stderr": 0.04725815626252604
    },
    "hendrycksTest-high_school_biology": {
      "acc": 0.5774193548387097,
      "acc_stderr": 0.02810096472427264,
      "acc_norm": 0.5774193548387097,
      "acc_norm_stderr": 0.02810096472427264
    },
    "hendrycksTest-high_school_chemistry": {
      "acc": 0.3399014778325123,
      "acc_stderr": 0.033327690684107895,
      "acc_norm": 0.3399014778325123,
      "acc_norm_stderr": 0.033327690684107895
    },
    "hendrycksTest-high_school_computer_science": {
      "acc": 0.57,
      "acc_stderr": 0.04975698519562428,
      "acc_norm": 0.57,
      "acc_norm_stderr": 0.04975698519562428
    },
    "hendrycksTest-high_school_european_history": {
      "acc": 0.6181818181818182,
      "acc_stderr": 0.03793713171165635,
      "acc_norm": 0.6181818181818182,
      "acc_norm_stderr": 0.03793713171165635
    },
    "hendrycksTest-high_school_geography": {
      "acc": 0.6161616161616161,
      "acc_stderr": 0.03464881675016339,
      "acc_norm": 0.6161616161616161,
      "acc_norm_stderr": 0.03464881675016339
    },
    "hendrycksTest-high_school_government_and_politics": {
      "acc": 0.7046632124352331,
      "acc_stderr": 0.03292296639155141,
      "acc_norm": 0.7046632124352331,
      "acc_norm_stderr": 0.03292296639155141
    },
    "hendrycksTest-high_school_macroeconomics": {
      "acc": 0.45384615384615384,
      "acc_stderr": 0.02524277098712618,
      "acc_norm": 0.45384615384615384,
      "acc_norm_stderr": 0.02524277098712618
    },
    "hendrycksTest-high_school_mathematics": {
      "acc": 0.3074074074074074,
      "acc_stderr": 0.028133252578815635,
      "acc_norm": 0.3074074074074074,
      "acc_norm_stderr": 0.028133252578815635
    },
    "hendrycksTest-high_school_microeconomics": {
      "acc": 0.44537815126050423,
      "acc_stderr": 0.032284106267163895,
      "acc_norm": 0.44537815126050423,
      "acc_norm_stderr": 0.032284106267163895
    },
    "hendrycksTest-high_school_physics": {
      "acc": 0.3509933774834437,
      "acc_stderr": 0.03896981964257375,
      "acc_norm": 0.3509933774834437,
      "acc_norm_stderr": 0.03896981964257375
    },
    "hendrycksTest-high_school_psychology": {
      "acc": 0.6477064220183486,
      "acc_stderr": 0.020480568843998983,
      "acc_norm": 0.6477064220183486,
      "acc_norm_stderr": 0.020480568843998983
    },
    "hendrycksTest-high_school_statistics": {
      "acc": 0.3287037037037037,
      "acc_stderr": 0.032036140846700596,
      "acc_norm": 0.3287037037037037,
      "acc_norm_stderr": 0.032036140846700596
    },
    "hendrycksTest-high_school_us_history": {
      "acc": 0.6862745098039216,
      "acc_stderr": 0.032566854844603886,
      "acc_norm": 0.6862745098039216,
      "acc_norm_stderr": 0.032566854844603886
    },
    "hendrycksTest-high_school_world_history": {
      "acc": 0.6962025316455697,
      "acc_stderr": 0.029936696387138615,
      "acc_norm": 0.6962025316455697,
      "acc_norm_stderr": 0.029936696387138615
    },
    "hendrycksTest-human_aging": {
      "acc": 0.5022421524663677,
      "acc_stderr": 0.03355746535223263,
      "acc_norm": 0.5022421524663677,
      "acc_norm_stderr": 0.03355746535223263
    },
    "hendrycksTest-human_sexuality": {
      "acc": 0.5725190839694656,
      "acc_stderr": 0.043389203057924,
      "acc_norm": 0.5725190839694656,
      "acc_norm_stderr": 0.043389203057924
    },
    "hendrycksTest-international_law": {
      "acc": 0.6528925619834711,
      "acc_stderr": 0.043457245702925335,
      "acc_norm": 0.6528925619834711,
      "acc_norm_stderr": 0.043457245702925335
    },
    "hendrycksTest-jurisprudence": {
      "acc": 0.5555555555555556,
      "acc_stderr": 0.04803752235190193,
      "acc_norm": 0.5555555555555556,
      "acc_norm_stderr": 0.04803752235190193
    },
    "hendrycksTest-logical_fallacies": {
      "acc": 0.5644171779141104,
      "acc_stderr": 0.03895632464138937,
      "acc_norm": 0.5644171779141104,
      "acc_norm_stderr": 0.03895632464138937
    },
    "hendrycksTest-machine_learning": {
      "acc": 0.33035714285714285,
      "acc_stderr": 0.04464285714285714,
      "acc_norm": 0.33035714285714285,
      "acc_norm_stderr": 0.04464285714285714
    },
    "hendrycksTest-management": {
      "acc": 0.6213592233009708,
      "acc_stderr": 0.048026946982589726,
      "acc_norm": 0.6213592233009708,
      "acc_norm_stderr": 0.048026946982589726
    },
    "hendrycksTest-marketing": {
      "acc": 0.7393162393162394,
      "acc_stderr": 0.02876034895652341,
      "acc_norm": 0.7393162393162394,
      "acc_norm_stderr": 0.02876034895652341
    },
    "hendrycksTest-medical_genetics": {
      "acc": 0.58,
      "acc_stderr": 0.04960449637488583,
      "acc_norm": 0.58,
      "acc_norm_stderr": 0.04960449637488583
    },
    "hendrycksTest-miscellaneous": {
      "acc": 0.6998722860791826,
      "acc_stderr": 0.01638924969131743,
      "acc_norm": 0.6998722860791826,
      "acc_norm_stderr": 0.01638924969131743
    },
    "hendrycksTest-moral_disputes": {
      "acc": 0.5317919075144508,
      "acc_stderr": 0.02686462436675665,
      "acc_norm": 0.5317919075144508,
      "acc_norm_stderr": 0.02686462436675665
    },
    "hendrycksTest-moral_scenarios": {
      "acc": 0.264804469273743,
      "acc_stderr": 0.014756906483260666,
      "acc_norm": 0.264804469273743,
      "acc_norm_stderr": 0.014756906483260666
    },
    "hendrycksTest-nutrition": {
      "acc": 0.5130718954248366,
      "acc_stderr": 0.028620130800700246,
      "acc_norm": 0.5130718954248366,
      "acc_norm_stderr": 0.028620130800700246
    },
    "hendrycksTest-philosophy": {
      "acc": 0.5466237942122186,
      "acc_stderr": 0.028274359854894245,
      "acc_norm": 0.5466237942122186,
      "acc_norm_stderr": 0.028274359854894245
    },
    "hendrycksTest-prehistory": {
      "acc": 0.5709876543209876,
      "acc_stderr": 0.027538925613470863,
      "acc_norm": 0.5709876543209876,
      "acc_norm_stderr": 0.027538925613470863
    },
    "hendrycksTest-professional_accounting": {
      "acc": 0.40070921985815605,
      "acc_stderr": 0.02923346574557309,
      "acc_norm": 0.40070921985815605,
      "acc_norm_stderr": 0.02923346574557309
    },
    "hendrycksTest-professional_law": {
      "acc": 0.39374185136897,
      "acc_stderr": 0.012478532272564444,
      "acc_norm": 0.39374185136897,
      "acc_norm_stderr": 0.012478532272564444
    },
    "hendrycksTest-professional_medicine": {
      "acc": 0.48161764705882354,
      "acc_stderr": 0.030352303395351964,
      "acc_norm": 0.48161764705882354,
      "acc_norm_stderr": 0.030352303395351964
    },
    "hendrycksTest-professional_psychology": {
      "acc": 0.47875816993464054,
      "acc_stderr": 0.020209572388600244,
      "acc_norm": 0.47875816993464054,
      "acc_norm_stderr": 0.020209572388600244
    },
    "hendrycksTest-public_relations": {
      "acc": 0.5363636363636364,
      "acc_stderr": 0.047764491623961985,
      "acc_norm": 0.5363636363636364,
      "acc_norm_stderr": 0.047764491623961985
    },
    "hendrycksTest-security_studies": {
      "acc": 0.5183673469387755,
      "acc_stderr": 0.03198761546763127,
      "acc_norm": 0.5183673469387755,
      "acc_norm_stderr": 0.03198761546763127
    },
    "hendrycksTest-sociology": {
      "acc": 0.681592039800995,
      "acc_stderr": 0.032941184790540944,
      "acc_norm": 0.681592039800995,
      "acc_norm_stderr": 0.032941184790540944
    },
    "hendrycksTest-us_foreign_policy": {
      "acc": 0.79,
      "acc_stderr": 0.040936018074033256,
      "acc_norm": 0.79,
      "acc_norm_stderr": 0.040936018074033256
    },
    "hendrycksTest-virology": {
      "acc": 0.45180722891566266,
      "acc_stderr": 0.03874371556587953,
      "acc_norm": 0.45180722891566266,
      "acc_norm_stderr": 0.03874371556587953
    },
    "hendrycksTest-world_religions": {
      "acc": 0.7485380116959064,
      "acc_stderr": 0.033275044238468436,
      "acc_norm": 0.7485380116959064,
      "acc_norm_stderr": 0.033275044238468436
    }
  },
  "versions": {
    "hendrycksTest-abstract_algebra": 1,
    "hendrycksTest-anatomy": 1,
    "hendrycksTest-astronomy": 1,
    "hendrycksTest-business_ethics": 1,
    "hendrycksTest-clinical_knowledge": 1,
    "hendrycksTest-college_biology": 1,
    "hendrycksTest-college_chemistry": 1,
    "hendrycksTest-college_computer_science": 1,
    "hendrycksTest-college_mathematics": 1,
    "hendrycksTest-college_medicine": 1,
    "hendrycksTest-college_physics": 1,
    "hendrycksTest-computer_security": 1,
    "hendrycksTest-conceptual_physics": 1,
    "hendrycksTest-econometrics": 1,
    "hendrycksTest-electrical_engineering": 1,
    "hendrycksTest-elementary_mathematics": 1,
    "hendrycksTest-formal_logic": 1,
    "hendrycksTest-global_facts": 1,
    "hendrycksTest-high_school_biology": 1,
    "hendrycksTest-high_school_chemistry": 1,
    "hendrycksTest-high_school_computer_science": 1,
    "hendrycksTest-high_school_european_history": 1,
    "hendrycksTest-high_school_geography": 1,
    "hendrycksTest-high_school_government_and_politics": 1,
    "hendrycksTest-high_school_macroeconomics": 1,
    "hendrycksTest-high_school_mathematics": 1,
    "hendrycksTest-high_school_microeconomics": 1,
    "hendrycksTest-high_school_physics": 1,
    "hendrycksTest-high_school_psychology": 1,
    "hendrycksTest-high_school_statistics": 1,
    "hendrycksTest-high_school_us_history": 1,
    "hendrycksTest-high_school_world_history": 1,
    "hendrycksTest-human_aging": 1,
    "hendrycksTest-human_sexuality": 1,
    "hendrycksTest-international_law": 1,
    "hendrycksTest-jurisprudence": 1,
    "hendrycksTest-logical_fallacies": 1,
    "hendrycksTest-machine_learning": 1,
    "hendrycksTest-management": 1,
    "hendrycksTest-marketing": 1,
    "hendrycksTest-medical_genetics": 1,
    "hendrycksTest-miscellaneous": 1,
    "hendrycksTest-moral_disputes": 1,
    "hendrycksTest-moral_scenarios": 1,
    "hendrycksTest-nutrition": 1,
    "hendrycksTest-philosophy": 1,
    "hendrycksTest-prehistory": 1,
    "hendrycksTest-professional_accounting": 1,
    "hendrycksTest-professional_law": 1,
    "hendrycksTest-professional_medicine": 1,
    "hendrycksTest-professional_psychology": 1,
    "hendrycksTest-public_relations": 1,
    "hendrycksTest-security_studies": 1,
    "hendrycksTest-sociology": 1,
    "hendrycksTest-us_foreign_policy": 1,
    "hendrycksTest-virology": 1,
    "hendrycksTest-world_religions": 1
  },
  "config": {
    "model": "hf-causal-experimental",
    "model_args": "pretrained=/home/sdk_models/ziya_llama_13b/Ziya-LLaMA-13B,load_in_8bit=True,dtype='float16',use_accelerate=False",
    "num_fewshot": 5,
    "batch_size": 2,
    "batch_sizes": [],
    "device": "cuda:4",
    "no_cache": true,
    "limit": null,
    "bootstrap_iters": 100000,
    "description_dict": {},
    "cost_time": "4:30:54.825750",
    "model_name": "ziya_llama_13b"
  }
}